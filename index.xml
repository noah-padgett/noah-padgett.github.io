<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Padgett: Beyond-STAT on Padgett: Beyond-STAT</title>
    <link>/</link>
    <description>Recent content in Padgett: Beyond-STAT on Padgett: Beyond-STAT</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0500</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Reproducing Lavaan from (almost) Scratch</title>
      <link>/tutorial/2019-10-27-lavaan-cfa-reproduction/</link>
      <pubDate>Sun, 27 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/tutorial/2019-10-27-lavaan-cfa-reproduction/</guid>
      <description>


&lt;p&gt;Purpose:&lt;/p&gt;
&lt;p&gt;To reproduce the maximum likelihood estimates from lavaan.
To try, I will be using the HolzingerSwineford1939 dataset that comes with lavaan.
These data form a three factor model with 9 items.&lt;/p&gt;
&lt;div id=&#34;lavaan-results&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;lavaan Results&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(2)

library(lavaan)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;lavaan&amp;#39; was built under R version 3.6.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## This is lavaan 0.6-5&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan is BETA software! Please report any bugs.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;HS.model &amp;lt;- &amp;#39; visual  =~ x1 + x2 + x3
              textual =~ x4 + x5 + x6
              speed   =~ x7 + x8 + x9 &amp;#39;

fit.la &amp;lt;- cfa(HS.model, data = HolzingerSwineford1939)
summary(fit.la, fit.measures = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-5 ended normally after 35 iterations
## 
##   Estimator                                         ML
##   Optimization method                           NLMINB
##   Number of free parameters                         21
##                                                       
##   Number of observations                           301
##                                                       
## Model Test User Model:
##                                                       
##   Test statistic                                85.306
##   Degrees of freedom                                24
##   P-value (Chi-square)                           0.000
## 
## Model Test Baseline Model:
## 
##   Test statistic                               918.852
##   Degrees of freedom                                36
##   P-value                                        0.000
## 
## User Model versus Baseline Model:
## 
##   Comparative Fit Index (CFI)                    0.931
##   Tucker-Lewis Index (TLI)                       0.896
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -3737.745
##   Loglikelihood unrestricted model (H1)      -3695.092
##                                                       
##   Akaike (AIC)                                7517.490
##   Bayesian (BIC)                              7595.339
##   Sample-size adjusted Bayesian (BIC)         7528.739
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.092
##   90 Percent confidence interval - lower         0.071
##   90 Percent confidence interval - upper         0.114
##   P-value RMSEA &amp;lt;= 0.05                          0.001
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.065
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   visual =~                                           
##     x1                1.000                           
##     x2                0.554    0.100    5.554    0.000
##     x3                0.729    0.109    6.685    0.000
##   textual =~                                          
##     x4                1.000                           
##     x5                1.113    0.065   17.014    0.000
##     x6                0.926    0.055   16.703    0.000
##   speed =~                                            
##     x7                1.000                           
##     x8                1.180    0.165    7.152    0.000
##     x9                1.082    0.151    7.155    0.000
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   visual ~~                                           
##     textual           0.408    0.074    5.552    0.000
##     speed             0.262    0.056    4.660    0.000
##   textual ~~                                          
##     speed             0.173    0.049    3.518    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .x1                0.549    0.114    4.833    0.000
##    .x2                1.134    0.102   11.146    0.000
##    .x3                0.844    0.091    9.317    0.000
##    .x4                0.371    0.048    7.779    0.000
##    .x5                0.446    0.058    7.642    0.000
##    .x6                0.356    0.043    8.277    0.000
##    .x7                0.799    0.081    9.823    0.000
##    .x8                0.488    0.074    6.573    0.000
##    .x9                0.566    0.071    8.003    0.000
##     visual            0.809    0.145    5.564    0.000
##     textual           0.979    0.112    8.737    0.000
##     speed             0.384    0.086    4.451    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Fit value
fit.la@optim$fx&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1417035&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;reproducing-lavaan&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Reproducing lavaan&lt;/h1&gt;
&lt;p&gt;There are four major sections that I will go through&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the fit function set-up&lt;/li&gt;
&lt;li&gt;the data and model prep&lt;/li&gt;
&lt;li&gt;the optimization function and starting value set-up&lt;/li&gt;
&lt;li&gt;estimating the model&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;reproduced-fit-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Reproduced Fit Function&lt;/h2&gt;
&lt;p&gt;Whenever I have read about CFA/SEM, I have been given this kind of vague idea on how estimation works.
The general idea I have read/been talk is that there’s this fit function
&lt;span class=&#34;math display&#34;&gt;\[
F_{ML} = \frac{1}{2}\left[\log\mid\mathbf{\Sigma}(\theta)\mid - \log\mid \mathbf{S}\mid + tr\left(\mathbf{S}\mathbf{\Sigma}^{-1}(\theta)\right) - p\right]
\]&lt;/span&gt;
where,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mid . \mid\)&lt;/span&gt; is the determinant of a matrix;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{S}\)&lt;/span&gt; is the sample covariance matrix;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\Sigma}(\theta)\)&lt;/span&gt; is the model implied covariance matrix;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(tr(.)\)&lt;/span&gt; is the trace of a matrix;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\Sigma}^{-1}(\theta)\)&lt;/span&gt; is the inverse of the model implied covariance matrix; and&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the number of variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A &lt;em&gt;relatively&lt;/em&gt; straightforward implementation of this fit function is shown in the following chunk of code.
The majority of the &lt;em&gt;cfa.fit()&lt;/em&gt; function is based on handling the model, data (X), and iteratively updating parameter estimates (&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;I first computed the sample related values.
For example, the number of variables &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is simply the number of columns in the dataset.
The sample covariance matrix is also straightforward to get.&lt;/p&gt;
&lt;p&gt;Secondly, the model is unpacked from the model list argument.
The &lt;em&gt;model&lt;/em&gt; argument is a list of three separate matrices for the 1) lambda (&lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt;)-factor loading matrix, 2) phi (&lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt;)-factor covariance matrix, and 3) psi (&lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt;)-error (co)variance matrix.&lt;/p&gt;
&lt;p&gt;Third, the individual parameter estimates (&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;) are then unpacked and placed into the corresponding model components.
This was the trickiest part to figure out as this part needs to be dynamically related to the model.
The part that I still am not sure is done generally enough is the unpacking of the factor covariance matrix section.
Well, this whole unpacking could likely be more general, but I will work on this for a later implementation.&lt;/p&gt;
&lt;p&gt;Next, the model implied covariance matrix is estimated.
The model implied covariance matrix is computed using &lt;span class=&#34;math inline&#34;&gt;\(\Sigma(\theta) = \Lambda\Phi\Lambda^{\mathrm{T}} + \Psi\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Finally, the fit function is estimated.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cfa.fit&amp;lt;-function(theta, X, model){
  # Compue sample statistics
  p&amp;lt;-ncol(X)
  S&amp;lt;-cov(X)
  
  # unpack model
  lambda &amp;lt;- model[[1]]
  phi &amp;lt;- model[[2]]
  psi &amp;lt;- model[[3]]
  
  # number factor loadings
  lam.num &amp;lt;- length(which(is.na(lambda)))
  lambda[which(is.na(lambda))] &amp;lt;- theta[1:lam.num]
  # number elements in factor (co)variance matrix
  phi.num &amp;lt;- length(which(is.na(phi)))
  if(phi.num &amp;gt; 0){
    phi[which(is.na(phi))] &amp;lt;- theta[(lam.num+1):(lam.num+phi.num)]
    phi[upper.tri(phi)] &amp;lt;- phi[lower.tri(phi)]
  }
  # number elements in error (co)variance matrix
  psi.num &amp;lt;- length(which(is.na(psi)))
  psi[which(is.na(psi))] &amp;lt;- theta[(lam.num+phi.num+1):(lam.num+phi.num+psi.num)]
  
  # compute model implied (co)variance matrix
  Sigma&amp;lt;-lambda%*%phi%*%(t(lambda)) + psi
  
  # get inverse of Simga
  sigInv &amp;lt;- solve(Sigma)
  # determinates of S &amp;amp; Sigma
  detS &amp;lt;- det(S)
  detSig &amp;lt;- det(Sigma)
  # compute fit function
  fit &amp;lt;- 0.5*(log(detSig) + trace(sigInv%*%S) - log(detS) - p)
  #return fit value 
  return(fit)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-and-model-prep&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Data and Model Prep&lt;/h2&gt;
&lt;p&gt;The data prep itself is relatively easy.
For this example at least, the data prep is simply extracting the 9 variables (&lt;span class=&#34;math inline&#34;&gt;\(x_1-x_9\)&lt;/span&gt;) from the larger dataframe.
The main thing is to make the 9 variables into all numeric variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- HolzingerSwineford1939[, paste0(&amp;#39;x&amp;#39;,1:9)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model prep was a little mode involved.
Here I needed to set up the specification of the model so that two conditions are met.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;I specify the model I want to, and&lt;/li&gt;
&lt;li&gt;The model is identified.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The later of which is the much more technically involved feature.
Identification is a big topic for statistical analysis, especially in the domain of latent variable models where identification can be notoriously difficult.
However, the basic idea is that we need to be able to come up with unique estimates for all parameter values.
In the CFA model here, the identification can be achieved by fixing the factor loading of one item to 1.&lt;/p&gt;
&lt;p&gt;As you will probably see, the specification for the factor covariance matrix (&lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt;) is a little funky between it is left as &lt;em&gt;NA&lt;/em&gt; along the diagonal and lower triangle, but the upper triangle is left as 0.
I figure this specification out through trial and error for being able to get the lower triangle duplicated more easily while still keeping the number of unique elements in this matrix to be estimated correct.
Note however, that in the &lt;em&gt;cfa.fit&lt;/em&gt; function I turn the &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; matrix into the full appropriate matrix with the upper and lower diagonal being equal.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nF&amp;lt;-3 # number of factor
p&amp;lt;-ncol(X) # number of variables

# model specification for factor loading matrix
#   Note: matrix fills column wise
lambdaMod &amp;lt;- matrix(ncol=nF, nrow=p,
                    # x1,x2, x3, x4,x5, x6, x7,x8,x9
                    c(1, NA, NA, 0,  0,  0, 0,  0, 0,   #f1
                      0,  0,  0, 1, NA, NA, 0,  0, 0,   #f2
                      0,  0,  0, 0,  0,  0, 1, NA, NA)) #f3
lambdaMod&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       [,1] [,2] [,3]
##  [1,]    1    0    0
##  [2,]   NA    0    0
##  [3,]   NA    0    0
##  [4,]    0    1    0
##  [5,]    0   NA    0
##  [6,]    0   NA    0
##  [7,]    0    0    1
##  [8,]    0    0   NA
##  [9,]    0    0   NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# factor covariance matrix (lower diagonal + diagonal)
phiMod &amp;lt;- matrix(nrow=nF,ncol=nF)
phiMod[upper.tri(phiMod)] &amp;lt;- 0
phiMod&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3]
## [1,]   NA    0    0
## [2,]   NA   NA    0
## [3,]   NA   NA   NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# error variances
psiMod &amp;lt;- diag(nrow=p)
diag(psiMod)&amp;lt;-NA
psiMod&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
##  [1,]   NA    0    0    0    0    0    0    0    0
##  [2,]    0   NA    0    0    0    0    0    0    0
##  [3,]    0    0   NA    0    0    0    0    0    0
##  [4,]    0    0    0   NA    0    0    0    0    0
##  [5,]    0    0    0    0   NA    0    0    0    0
##  [6,]    0    0    0    0    0   NA    0    0    0
##  [7,]    0    0    0    0    0    0   NA    0    0
##  [8,]    0    0    0    0    0    0    0   NA    0
##  [9,]    0    0    0    0    0    0    0    0   NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# store as list
cfaModel &amp;lt;- list(lambdaMod, phiMod, psiMod)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;optimization-function-and-starting-value-set-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Optimization Function and Starting Value Set-Up&lt;/h2&gt;
&lt;p&gt;The starting values were also a little tricky to get figured out a first…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get length of each model element
lam.num &amp;lt;- sum(is.na(c(lambdaMod))==T)
phi.num &amp;lt;- sum(is.na(c(phiMod))==T)
psi.num &amp;lt;- sum(is.na(c(psiMod))==T)

k&amp;lt;-lam.num+phi.num+psi.num
sv&amp;lt;-numeric(k)
# generate starting values
sv[1:(lam.num)] &amp;lt;- rep(0.25,lam.num)
sv[(lam.num+1):(lam.num+phi.num)]&amp;lt;- runif(phi.num, 0.05, 1)
sv[(lam.num+phi.num+1):(lam.num+phi.num+psi.num)] &amp;lt;-runif(psi.num, 0.05, 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Last thing with respect to the estimation.
We have to the compute the trace of a matrix.
For some reason, I can not find a built in R function that automatically does this…
So, I initialized a little function that does it for me to keep track.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# trace function
trace &amp;lt;- function(A) {
  n &amp;lt;- dim(A)[1] # get dimension of matrix
  tr &amp;lt;- 0 # initialize trace value
  
  # Loop over the diagonal elements of the supplied matrix and add the element to tr
  for (k in 1:n) {
    l &amp;lt;- A[k,k]
    tr &amp;lt;- tr + l
  }
  return(tr[[1]])
}
# or one could do sum(diag(A))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-the-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. Estimating the Model&lt;/h2&gt;
&lt;p&gt;Lavaan estimates models using the fit function described briefly from above along with a numerical methods for optimization.
Now, that is vague on purpose.
The precise methods of numerical approximations for complex functions is a bit out of my range of knowledge at this point, but I have general enough sense of how to use the basics of these methods.
MANY methods exists for numerically solving complex optimization tasks in high dimensions, I will use two separate R tools and show how vastly different the results can be.&lt;/p&gt;
&lt;p&gt;First, I will use the tools lavaan is built on.
Lavaan uses the &lt;em&gt;nlminb&lt;/em&gt;.
I am unsure why this optimization function was selected (the answer may be hidden in the depths of the lavaan help/tutorial pages).
But, the nice thing is that this function is pretty straightforward to use.
The following chunk of code utilize the function in a similar manner that lavaan does I believe.
The &lt;em&gt;control&lt;/em&gt; argument supplies a list of different evaluation criteria, which I have taken from the fitted &lt;em&gt;fit.la&lt;/em&gt; object from above.
When I was testing this code out, I found that changing most of these settings had neglible impact on the results from this simple model.&lt;/p&gt;
&lt;p&gt;Secondly, I extracted the corresponding parameter estimated and unpacked them into the objects &lt;em&gt;lambda&lt;/em&gt;, &lt;em&gt;phi&lt;/em&gt;, and &lt;em&gt;psi&lt;/em&gt;.
This was so that I could show the resulting model pieces.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;- nlminb(sv, cfa.fit, X=X, model=cfaModel,
               control=list(eval.max=20000, iter.max=10000,
                               abs.tol=2.2e-15, rel.tol=1e-10,
                               x.tol=1.5e-8,xf.tol=2.2e-14))

# value of the fit function
fit1$objective&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1417035&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# unpack parameter estimates
fit&amp;lt;-fit1
lambda &amp;lt;- cfaModel[[1]]
phi &amp;lt;- cfaModel[[2]]
psi &amp;lt;- cfaModel[[3]]
# number factor loadings
lam.num &amp;lt;- length(which(is.na(lambda)))
lambda[which(is.na(lambda))] &amp;lt;- fit$par[1:lam.num]
# number elements in factor (co)variance matrix
phi.num &amp;lt;- length(which(is.na(phi)))
if(phi.num &amp;gt; 0){
  phi[which(is.na(phi))] &amp;lt;- fit$par[(lam.num+1):(lam.num+phi.num)]
}
# number elements in error (co)variance matrix
psi.num &amp;lt;- length(which(is.na(psi)))
psi[which(is.na(psi))] &amp;lt;- fit$par[(lam.num+phi.num+1):(lam.num+phi.num+psi.num)]

print(round(lambda,3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        [,1]  [,2]  [,3]
##  [1,] 1.000 0.000 0.000
##  [2,] 0.554 0.000 0.000
##  [3,] 0.729 0.000 0.000
##  [4,] 0.000 1.000 0.000
##  [5,] 0.000 1.113 0.000
##  [6,] 0.000 0.926 0.000
##  [7,] 0.000 0.000 1.000
##  [8,] 0.000 0.000 1.180
##  [9,] 0.000 0.000 1.082&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(round(phi,3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       [,1]  [,2]  [,3]
## [1,] 0.812 0.000 0.000
## [2,] 0.410 0.983 0.000
## [3,] 0.263 0.174 0.385&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(round(diag(psi),3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.551 1.138 0.847 0.372 0.448 0.357 0.802 0.489 0.568&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I also found an alternative optimization function in R called optim(.).
The really nice part about this function is that the Hessian matrix can be more easily computed.
But, the major downside is that optim was not used by lavaan can may yield slightly different results thatn lavaan.
However, I think once the lower-bounds for parameters are placed the functions yeild identical results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#using optim instead of nlminb
# (easier to get hessian matrix)

# BUT, I need to supply lower limits for the variance parameters
lb &amp;lt;- numeric(k)
lb[1:(lam.num)] &amp;lt;- -Inf
# phi
lb.phi&amp;lt;- matrix(nrow=nF,ncol=nF)
diag(lb.phi)&amp;lt;- 0.001
lb.phi[lower.tri(lb.phi)] &amp;lt;- -Inf
lb[(lam.num+1):(lam.num+phi.num)]&amp;lt;- c(lb.phi[lower.tri(lb.phi, diag=T)])
# phi
lb[(lam.num+phi.num+1):(lam.num+phi.num+psi.num)] &amp;lt;- 0.001

fit2 &amp;lt;- optim(sv, cfa.fit, X=X, model=cfaModel,
              method=&amp;#39;L-BFGS-B&amp;#39;, hessian = T, lower=lb,
              control=list(maxit=10000))

# value of the fit function
fit2$value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1417035&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# unpack parameter estimates
fit&amp;lt;-fit2
lambda &amp;lt;- cfaModel[[1]]
phi &amp;lt;- cfaModel[[2]]
psi &amp;lt;- cfaModel[[3]]
# number factor loadings
lam.num &amp;lt;- length(which(is.na(lambda)))
lambda[which(is.na(lambda))] &amp;lt;- fit$par[1:lam.num]
# number elements in factor (co)variance matrix
phi.num &amp;lt;- length(which(is.na(phi)))
if(phi.num &amp;gt; 0){
  phi[which(is.na(phi))] &amp;lt;- fit$par[(lam.num+1):(lam.num+phi.num)]
}
# number elements in error (co)variance matrix
psi.num &amp;lt;- length(which(is.na(psi)))
psi[which(is.na(psi))] &amp;lt;- fit$par[(lam.num+phi.num+1):(lam.num+phi.num+psi.num)]

print(lambda)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            [,1]      [,2]     [,3]
##  [1,] 1.0000000 0.0000000 0.000000
##  [2,] 0.5535270 0.0000000 0.000000
##  [3,] 0.7293599 0.0000000 0.000000
##  [4,] 0.0000000 1.0000000 0.000000
##  [5,] 0.0000000 1.1130382 0.000000
##  [6,] 0.0000000 0.9261204 0.000000
##  [7,] 0.0000000 0.0000000 1.000000
##  [8,] 0.0000000 0.0000000 1.179954
##  [9,] 0.0000000 0.0000000 1.081630&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(phi)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]      [,2]     [,3]
## [1,] 0.8120324 0.0000000 0.000000
## [2,] 0.4096243 0.9828315 0.000000
## [3,] 0.2631030 0.1740852 0.384998&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(diag(psi))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5508865 1.1375943 0.8471442 0.3723983 0.4477594 0.3573981 0.8020822
## [8] 0.4893649 0.5679775&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One thing that I know is lacking in this replication is the computation of the standard errors.
Standard errors are a much more complicated computation.
I know they are related to the 2nd derivative of the likelihood function.
But I have not figured out how to set up this computation myself yet.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Stumbling with RStan</title>
      <link>/tutorial/2019-06-06-rstan-humble/</link>
      <pubDate>Fri, 06 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/tutorial/2019-06-06-rstan-humble/</guid>
      <description>


&lt;p&gt;So I was listing to one of my favorite podcasts (DataFramed) a few months ago and a particularly interest guest was on talking.
The guest was Michael Betancourt.&lt;/p&gt;
&lt;div id=&#34;reading-about-the-really-cool-hmc&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Reading about the REALLY cool HMC&lt;/h1&gt;
&lt;p&gt;HMC is the shorthand for Hamiltonian Monte Carlo.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;trouble-getting-started&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Trouble Getting Started&lt;/h1&gt;
&lt;p&gt;The Getting Started page (‘&lt;a href=&#34;https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started&#34; class=&#34;uri&#34;&gt;https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started&lt;/a&gt;’) is extremely thorough and gives some excellent pointerr.
The page boils down to using the following R commands to get started&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if (file.exists(&amp;quot;.RData&amp;quot;)) file.remove(&amp;quot;.RData&amp;quot;)
install.packages(&amp;quot;rstan&amp;quot;, repos = &amp;quot;https://cloud.r-project.org/&amp;quot;, dependencies = TRUE)


library(rstan)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

schools_dat &amp;lt;- list(J = 8, 
                     y = c(28,  8, -3,  7, -1,  1, 18, 12),
                     sigma = c(15, 10, 16, 11,  9, 11, 10, 18))

fit &amp;lt;- stan(file = &amp;#39;8schools.stan&amp;#39;, data = schools_dat)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Stan file is also incredibly easy to set up by just going up to ‘Add File -&amp;gt; Stan File’.
Super simple…&lt;/p&gt;
&lt;p&gt;Or NOT…&lt;/p&gt;
&lt;p&gt;I kept getting the following error through back at me.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Error in compileCode(f, code, language = language, verbose = verbose) :    Compilation ERROR, function(s)/method(s) not created! sh: g++: command not found make: *** [C:/PROGRA~1/R/R-3.6.0/etc/x64/Makeconf:215: file197870bf1a48.o] Error 127 In addition: Warning message: In system(cmd, intern = !verbose) :   running command &#39;C:/PROGRA~1/R/R-3.6.0/bin/x64/R CMD SHLIB file197870bf1a48.cpp 2&amp;gt; file197870bf1a48.cpp.err.txt&#39; had status 1 Error in sink(type = &#34;output&#34;) : invalid connection&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Super frustrating…&lt;/p&gt;
&lt;p&gt;Anyways, it turns out that after too much time spent Googling and reading stackexchange and GitHub issues that I found how to I was able to fix this annoying startup issue.
There isa file that is created when Stan is being installed that Windows somehow uses to run C++ code from R (I have no clue how this happens…) and there is just a simple setting that I needed to add in order to get Stan to work.
Here is the link to where I found the solution (&lt;a href=&#34;https://github.com/stan-dev/rstan/issues/633&#34; class=&#34;uri&#34;&gt;https://github.com/stan-dev/rstan/issues/633&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Here is how I fixed the issue through R:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Uninstall rstan so that I can reinstall it properly&lt;/li&gt;
&lt;li&gt;Automatically edit the .Makrvars file&lt;/li&gt;
&lt;li&gt;In this file, make sure the following line is the only line contained in the file: ‘CXX14=$(BINPREF)g++ -O2 -march=native -mtune=nativee’&lt;/li&gt;
&lt;li&gt;Install rstan&lt;/li&gt;
&lt;li&gt;Start-Up Stan&lt;/li&gt;
&lt;li&gt;Run Example (takes a minute)&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Step 1
remove.packages(&amp;#39;rstan&amp;#39;)

# Steps 2-3
dotR &amp;lt;- file.path(Sys.getenv(&amp;quot;HOME&amp;quot;), &amp;quot;.R&amp;quot;)
if (!file.exists(dotR)) dir.create(dotR)
M &amp;lt;- file.path(dotR, ifelse(.Platform$OS.type == &amp;quot;windows&amp;quot;, &amp;quot;Makevars.win&amp;quot;, &amp;quot;Makevars&amp;quot;))
if (!file.exists(M)) file.create(M)
cat(&amp;quot;CXX14=$(BINPREF)g++ -O2 -march=native -mtune=nativee&amp;quot;,
    file = M, sep = &amp;quot;\n&amp;quot;, append = TRUE)

# Step 4
if (file.exists(&amp;quot;.RData&amp;quot;)) file.remove(&amp;quot;.RData&amp;quot;)
install.packages(&amp;quot;rstan&amp;quot;, repos = &amp;quot;https://cloud.r-project.org/&amp;quot;, dependencies = TRUE)

# Step 5
library(rstan)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)


# Step 6
schools_dat &amp;lt;- list(J = 8, 
                    y = c(28,  8, -3,  7, -1,  1, 18, 12),
                    sigma = c(15, 10, 16, 11,  9, 11, 10, 18))

fit &amp;lt;- stan(file = &amp;#39;8schools.stan&amp;#39;, data = schools_dat)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note: I need to come back and edit this content…&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>An examination of sensitivity to measurement error of Rasch residual-based fit statistics</title>
      <link>/publication/rasch-fit/</link>
      <pubDate>Sat, 08 Jun 2019 00:00:00 -0500</pubDate>
      
      <guid>/publication/rasch-fit/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Exploration of the vaginal and gut microbiome in African American women by body mass index, class of obesity, and gestational weight gain: A pilot study</title>
      <link>/publication/microbiome-gestational-weight-pilot/</link>
      <pubDate>Sat, 08 Jun 2019 00:00:00 -0500</pubDate>
      
      <guid>/publication/microbiome-gestational-weight-pilot/</guid>
      <description>&lt;p&gt;In this project, I contributed to the statistical analyses and making of figures.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Simulating Data from Regression Models</title>
      <link>/tutorial/2019-04-21-r-reg-sim/</link>
      <pubDate>Sun, 14 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/tutorial/2019-04-21-r-reg-sim/</guid>
      <description>

&lt;p&gt;This file extends the introduction I gave to simulating data from coin flips.
Now, the goal is to show how the basics extend and can easily be used for helpful parts of study design planning.&lt;/p&gt;

&lt;h1 id=&#34;simulating-more-complex-models&#34;&gt;Simulating More Complex Models&lt;/h1&gt;

&lt;p&gt;We can easily extend the basic concepts of simulating from a simple population to simulating data from a more complex data structure.
This is how we can simulate a simple regression model with one outcome (Y) and five predictors (x1, x2, etc.).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(lavaan)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## This is lavaan 0.6-3
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## lavaan is BETA software! Please report any bugs.
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# what we think the relationship are
hypothesized.model &amp;lt;- &#39; 
y ~ .1*x1 + .2*x2 + -.2*x3 + .5*x4
y ~~ 1*y
&#39;

## Simulate 1 dataset
pop.data &amp;lt;- simulateData(hypothesized.model, 
                         sample.nobs=1000, 
                         seed=2 ## Seed is for reproducing results
                         )

## Visualize scatterplot matrix
plot(pop.data)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/tutorial/2019-04-21-r-reg-sim_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Estimate the model we care about
fitted.model &amp;lt;- &#39;
y ~ x1 + x2 + x3 + x4
&#39;
fitted.out &amp;lt;- sem(
  fitted.model, 
  data=pop.data,estimator=&amp;quot;ML&amp;quot;)
## Get summary statistics of results
summary(fitted.out, rsquare=TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## lavaan 0.6-3 ended normally after 10 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                          5
## 
##   Number of observations                          1000
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                       0.000
##   Degrees of freedom                                 0
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   y ~                                                 
##     x1                0.111    0.032    3.402    0.001
##     x2                0.225    0.031    7.196    0.000
##     x3               -0.240    0.032   -7.449    0.000
##     x4                0.542    0.034   15.841    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .y                 1.036    0.046   22.361    0.000
## 
## R-Square:
##                    Estimate
##     y                 0.273
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;simulation-for-power-calculation&#34;&gt;Simulation for Power Calculation&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(simsem)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Warning: package &#39;simsem&#39; was built under R version 3.5.3
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## 
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## #################################################################
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## This is simsem 0.5-14
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## simsem is BETA software! Please report any bugs.
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## simsem was first developed at the University of Kansas Center for
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Research Methods and Data Analysis, under NSF Grant 1053160.
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## #################################################################
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &#39;simsem&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## The following object is masked from &#39;package:lavaan&#39;:
## 
##     inspect
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Simulate 100 datasets of size 100 from the population
act.power &amp;lt;- sim(nRep=100,
                 generate=hypothesized.model,
                 model=fitted.model,
                 n =100,
                 lavaanfun = &amp;quot;sem&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Progress: 1 / 100 
## Progress: 2 / 100 
## Progress: 3 / 100 
## Progress: 4 / 100 
## Progress: 5 / 100 
## Progress: 6 / 100 
## Progress: 7 / 100 
## Progress: 8 / 100 
## Progress: 9 / 100 
## Progress: 10 / 100 
## Progress: 11 / 100 
## Progress: 12 / 100 
## Progress: 13 / 100 
## Progress: 14 / 100 
## Progress: 15 / 100 
## Progress: 16 / 100 
## Progress: 17 / 100 
## Progress: 18 / 100 
## Progress: 19 / 100 
## Progress: 20 / 100 
## Progress: 21 / 100 
## Progress: 22 / 100 
## Progress: 23 / 100 
## Progress: 24 / 100 
## Progress: 25 / 100 
## Progress: 26 / 100 
## Progress: 27 / 100 
## Progress: 28 / 100 
## Progress: 29 / 100 
## Progress: 30 / 100 
## Progress: 31 / 100 
## Progress: 32 / 100 
## Progress: 33 / 100 
## Progress: 34 / 100 
## Progress: 35 / 100 
## Progress: 36 / 100 
## Progress: 37 / 100 
## Progress: 38 / 100 
## Progress: 39 / 100 
## Progress: 40 / 100 
## Progress: 41 / 100 
## Progress: 42 / 100 
## Progress: 43 / 100 
## Progress: 44 / 100 
## Progress: 45 / 100 
## Progress: 46 / 100 
## Progress: 47 / 100 
## Progress: 48 / 100 
## Progress: 49 / 100 
## Progress: 50 / 100 
## Progress: 51 / 100 
## Progress: 52 / 100 
## Progress: 53 / 100 
## Progress: 54 / 100 
## Progress: 55 / 100 
## Progress: 56 / 100 
## Progress: 57 / 100 
## Progress: 58 / 100 
## Progress: 59 / 100 
## Progress: 60 / 100 
## Progress: 61 / 100 
## Progress: 62 / 100 
## Progress: 63 / 100 
## Progress: 64 / 100 
## Progress: 65 / 100 
## Progress: 66 / 100 
## Progress: 67 / 100 
## Progress: 68 / 100 
## Progress: 69 / 100 
## Progress: 70 / 100 
## Progress: 71 / 100 
## Progress: 72 / 100 
## Progress: 73 / 100 
## Progress: 74 / 100 
## Progress: 75 / 100 
## Progress: 76 / 100 
## Progress: 77 / 100 
## Progress: 78 / 100 
## Progress: 79 / 100 
## Progress: 80 / 100 
## Progress: 81 / 100 
## Progress: 82 / 100 
## Progress: 83 / 100 
## Progress: 84 / 100 
## Progress: 85 / 100 
## Progress: 86 / 100 
## Progress: 87 / 100 
## Progress: 88 / 100 
## Progress: 89 / 100 
## Progress: 90 / 100 
## Progress: 91 / 100 
## Progress: 92 / 100 
## Progress: 93 / 100 
## Progress: 94 / 100 
## Progress: 95 / 100 
## Progress: 96 / 100 
## Progress: 97 / 100 
## Progress: 98 / 100 
## Progress: 99 / 100 
## Progress: 100 / 100
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Extract summary information
sim.parms&amp;lt;-summaryParam(act.power,alpha = 0.05,detail=TRUE)
sim.parms
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##      Estimate Average Estimate SD Average SE Power (Not equal 0)
## y~x1        0.1097629  0.09554286 0.10086173                0.20
## y~x2        0.2049147  0.09947488 0.09872815                0.54
## y~x3       -0.1977827  0.10642876 0.09910948                0.50
## y~x4        0.4910222  0.10093184 0.09851506                1.00
## y~~y        0.9383137  0.13987441 0.13269760                1.00
##          Std Est Std Est SD Std Ave SE Average Param Average Bias Coverage
## y~x1  0.09445416 0.08459703 0.08512030           0.1  0.009762922     0.94
## y~x2  0.17723176 0.08270002 0.08374237           0.2  0.004914709     0.95
## y~x3 -0.17213624 0.08947771 0.08402363          -0.2  0.002217263     0.94
## y~x4  0.42868274 0.07843796 0.07464199           0.5 -0.008977780     0.96
## y~~y  0.71830659 0.07635515 0.06891527           1.0 -0.061686276     0.83
##         Rel Bias    Std Bias  Rel SE Bias Not Cover Below Not Cover Above
## y~x1  0.09762922  0.10218369  0.055669976            0.04            0.02
## y~x2  0.02457354  0.04940653 -0.007506752            0.03            0.02
## y~x3 -0.01108631  0.02083331 -0.068771645            0.03            0.03
## y~x4 -0.01795556 -0.08894894 -0.023944657            0.02            0.02
## y~~y -0.06168628 -0.44101188 -0.051308941            0.00            0.17
##      Average CI Width SD CI Width
## y~x1        0.3953707  0.04252852
## y~x2        0.3870072  0.03941553
## y~x3        0.3885020  0.04763438
## y~x4        0.3861719  0.03956116
## y~~y        0.5201650  0.07754099
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;simulation-for-sample-size-determination&#34;&gt;Simulation for Sample Size Determination&lt;/h1&gt;

&lt;p&gt;Now, we want to calculate the power for a range of sample sizes.
By calculating power across a range we are able to identify the sample size needed to reach power of .80 fora particular parameter of interest.
Let&amp;rsquo;s try to answer this question:
&lt;em&gt;What is sample size I need if I want to detect the effect of x2 on y?&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Simulate data for a range of sample sizes 
# seq(100,500,10): pick samples sizes from 100 to 500 going by 10 each time, so we will have 100, 110, 120, 130, ... , 500.
# the rep(..., 100): each sample size is replicated 100 times, so we will effectively run 100 datasets with sample size 110, then 100 datasets with sample size 500, etc. 
act.n &amp;lt;- sim(model=fitted.model, 
             generate=hypothesized.model, 
             n = rep(seq(100,300,10), 100), 
             lavaanfun = &amp;quot;sem&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Progress: 1 / 2100 
## Progress: 2 / 2100 
## Progress: 3 / 2100 
## Progress: 4 / 2100 
## Progress: 5 / 2100 
## Progress: 6 / 2100 
## Progress: 7 / 2100 
## Progress: 8 / 2100 
## Progress: 9 / 2100 
## Progress: 10 / 2100 
## Progress: 11 / 2100 
## Progress: 12 / 2100 
## Progress: 13 / 2100 
## Progress: 14 / 2100 
## Progress: 15 / 2100 
## Progress: 16 / 2100 
## Progress: 17 / 2100 
## Progress: 18 / 2100 
## Progress: 19 / 2100 
## Progress: 20 / 2100 
## Progress: 21 / 2100 
## Progress: 22 / 2100 
## Progress: 23 / 2100 
## Progress: 24 / 2100 
## Progress: 25 / 2100 
## Progress: 26 / 2100 
## Progress: 27 / 2100 
## Progress: 28 / 2100 
## Progress: 29 / 2100 
## Progress: 30 / 2100 
## Progress: 31 / 2100 
## Progress: 32 / 2100 
## Progress: 33 / 2100 
## Progress: 34 / 2100 
## Progress: 35 / 2100 
## Progress: 36 / 2100 
## Progress: 37 / 2100 
## Progress: 38 / 2100 
## Progress: 39 / 2100 
## Progress: 40 / 2100 
## Progress: 41 / 2100 
## Progress: 42 / 2100 
## Progress: 43 / 2100 
## Progress: 44 / 2100 
## Progress: 45 / 2100 
## Progress: 46 / 2100 
## Progress: 47 / 2100 
## Progress: 48 / 2100 
## Progress: 49 / 2100 
## Progress: 50 / 2100 
## Progress: 51 / 2100 
## Progress: 52 / 2100 
## Progress: 53 / 2100 
## Progress: 54 / 2100 
## Progress: 55 / 2100 
## Progress: 56 / 2100 
## Progress: 57 / 2100 
## Progress: 58 / 2100 
## Progress: 59 / 2100 
## Progress: 60 / 2100 
## Progress: 61 / 2100 
## Progress: 62 / 2100 
## Progress: 63 / 2100 
## Progress: 64 / 2100 
## Progress: 65 / 2100 
## Progress: 66 / 2100 
## Progress: 67 / 2100 
## Progress: 68 / 2100 
## Progress: 69 / 2100 
## Progress: 70 / 2100 
## Progress: 71 / 2100 
## Progress: 72 / 2100 
## Progress: 73 / 2100 
## Progress: 74 / 2100 
## Progress: 75 / 2100 
## Progress: 76 / 2100 
## Progress: 77 / 2100 
## Progress: 78 / 2100 
## Progress: 79 / 2100 
## Progress: 80 / 2100 
## Progress: 81 / 2100 
## Progress: 82 / 2100 
## Progress: 83 / 2100 
## Progress: 84 / 2100 
## Progress: 85 / 2100 
## Progress: 86 / 2100 
## Progress: 87 / 2100 
## Progress: 88 / 2100 
## Progress: 89 / 2100 
## Progress: 90 / 2100 
## Progress: 91 / 2100 
## Progress: 92 / 2100 
## Progress: 93 / 2100 
## Progress: 94 / 2100 
## Progress: 95 / 2100 
## Progress: 96 / 2100 
## Progress: 97 / 2100 
## Progress: 98 / 2100 
## Progress: 99 / 2100 
## Progress: 100 / 2100 
## Progress: 101 / 2100 
## Progress: 102 / 2100 
## Progress: 103 / 2100 
## Progress: 104 / 2100 
## Progress: 105 / 2100 
## Progress: 106 / 2100 
## Progress: 107 / 2100 
## Progress: 108 / 2100 
## Progress: 109 / 2100 
## Progress: 110 / 2100 
## Progress: 111 / 2100 
## Progress: 112 / 2100 
## Progress: 113 / 2100 
## Progress: 114 / 2100 
## Progress: 115 / 2100 
## Progress: 116 / 2100 
## Progress: 117 / 2100 
## Progress: 118 / 2100 
## Progress: 119 / 2100 
## Progress: 120 / 2100 
## Progress: 121 / 2100 
## Progress: 122 / 2100 
## Progress: 123 / 2100 
## Progress: 124 / 2100 
## Progress: 125 / 2100 
## Progress: 126 / 2100 
## Progress: 127 / 2100 
## Progress: 128 / 2100 
## Progress: 129 / 2100 
## Progress: 130 / 2100 
## Progress: 131 / 2100 
## Progress: 132 / 2100 
## Progress: 133 / 2100 
## Progress: 134 / 2100 
## Progress: 135 / 2100 
## Progress: 136 / 2100 
## Progress: 137 / 2100 
## Progress: 138 / 2100 
## Progress: 139 / 2100 
## Progress: 140 / 2100 
## Progress: 141 / 2100 
## Progress: 142 / 2100 
## Progress: 143 / 2100 
## Progress: 144 / 2100 
## Progress: 145 / 2100 
## Progress: 146 / 2100 
## Progress: 147 / 2100 
## Progress: 148 / 2100 
## Progress: 149 / 2100 
## Progress: 150 / 2100 
## Progress: 151 / 2100 
## Progress: 152 / 2100 
## Progress: 153 / 2100 
## Progress: 154 / 2100 
## Progress: 155 / 2100 
## Progress: 156 / 2100 
## Progress: 157 / 2100 
## Progress: 158 / 2100 
## Progress: 159 / 2100 
## Progress: 160 / 2100 
## Progress: 161 / 2100 
## Progress: 162 / 2100 
## Progress: 163 / 2100 
## Progress: 164 / 2100 
## Progress: 165 / 2100 
## Progress: 166 / 2100 
## Progress: 167 / 2100 
## Progress: 168 / 2100 
## Progress: 169 / 2100 
## Progress: 170 / 2100 
## Progress: 171 / 2100 
## Progress: 172 / 2100 
## Progress: 173 / 2100 
## Progress: 174 / 2100 
## Progress: 175 / 2100 
## Progress: 176 / 2100 
## Progress: 177 / 2100 
## Progress: 178 / 2100 
## Progress: 179 / 2100 
## Progress: 180 / 2100 
## Progress: 181 / 2100 
## Progress: 182 / 2100 
## Progress: 183 / 2100 
## Progress: 184 / 2100 
## Progress: 185 / 2100 
## Progress: 186 / 2100 
## Progress: 187 / 2100 
## Progress: 188 / 2100 
## Progress: 189 / 2100 
## Progress: 190 / 2100 
## Progress: 191 / 2100 
## Progress: 192 / 2100 
## Progress: 193 / 2100 
## Progress: 194 / 2100 
## Progress: 195 / 2100 
## Progress: 196 / 2100 
## Progress: 197 / 2100 
## Progress: 198 / 2100 
## Progress: 199 / 2100 
## Progress: 200 / 2100 
## Progress: 201 / 2100 
## Progress: 202 / 2100 
## Progress: 203 / 2100 
## Progress: 204 / 2100 
## Progress: 205 / 2100 
## Progress: 206 / 2100 
## Progress: 207 / 2100 
## Progress: 208 / 2100 
## Progress: 209 / 2100 
## Progress: 210 / 2100 
## Progress: 211 / 2100 
## Progress: 212 / 2100 
## Progress: 213 / 2100 
## Progress: 214 / 2100 
## Progress: 215 / 2100 
## Progress: 216 / 2100 
## Progress: 217 / 2100 
## Progress: 218 / 2100 
## Progress: 219 / 2100 
## Progress: 220 / 2100 
## Progress: 221 / 2100 
## Progress: 222 / 2100 
## Progress: 223 / 2100 
## Progress: 224 / 2100 
## Progress: 225 / 2100 
## Progress: 226 / 2100 
## Progress: 227 / 2100 
## Progress: 228 / 2100 
## Progress: 229 / 2100 
## Progress: 230 / 2100 
## Progress: 231 / 2100 
## Progress: 232 / 2100 
## Progress: 233 / 2100 
## Progress: 234 / 2100 
## Progress: 235 / 2100 
## Progress: 236 / 2100 
## Progress: 237 / 2100 
## Progress: 238 / 2100 
## Progress: 239 / 2100 
## Progress: 240 / 2100 
## Progress: 241 / 2100 
## Progress: 242 / 2100 
## Progress: 243 / 2100 
## Progress: 244 / 2100 
## Progress: 245 / 2100 
## Progress: 246 / 2100 
## Progress: 247 / 2100 
## Progress: 248 / 2100 
## Progress: 249 / 2100 
## Progress: 250 / 2100 
## Progress: 251 / 2100 
## Progress: 252 / 2100 
## Progress: 253 / 2100 
## Progress: 254 / 2100 
## Progress: 255 / 2100 
## Progress: 256 / 2100 
## Progress: 257 / 2100 
## Progress: 258 / 2100 
## Progress: 259 / 2100 
## Progress: 260 / 2100 
## Progress: 261 / 2100 
## Progress: 262 / 2100 
## Progress: 263 / 2100 
## Progress: 264 / 2100 
## Progress: 265 / 2100 
## Progress: 266 / 2100 
## Progress: 267 / 2100 
## Progress: 268 / 2100 
## Progress: 269 / 2100 
## Progress: 270 / 2100 
## Progress: 271 / 2100 
## Progress: 272 / 2100 
## Progress: 273 / 2100 
## Progress: 274 / 2100 
## Progress: 275 / 2100 
## Progress: 276 / 2100 
## Progress: 277 / 2100 
## Progress: 278 / 2100 
## Progress: 279 / 2100 
## Progress: 280 / 2100 
## Progress: 281 / 2100 
## Progress: 282 / 2100 
## Progress: 283 / 2100 
## Progress: 284 / 2100 
## Progress: 285 / 2100 
## Progress: 286 / 2100 
## Progress: 287 / 2100 
## Progress: 288 / 2100 
## Progress: 289 / 2100 
## Progress: 290 / 2100 
## Progress: 291 / 2100 
## Progress: 292 / 2100 
## Progress: 293 / 2100 
## Progress: 294 / 2100 
## Progress: 295 / 2100 
## Progress: 296 / 2100 
## Progress: 297 / 2100 
## Progress: 298 / 2100 
## Progress: 299 / 2100 
## Progress: 300 / 2100 
## Progress: 301 / 2100 
## Progress: 302 / 2100 
## Progress: 303 / 2100 
## Progress: 304 / 2100 
## Progress: 305 / 2100 
## Progress: 306 / 2100 
## Progress: 307 / 2100 
## Progress: 308 / 2100 
## Progress: 309 / 2100 
## Progress: 310 / 2100 
## Progress: 311 / 2100 
## Progress: 312 / 2100 
## Progress: 313 / 2100 
## Progress: 314 / 2100 
## Progress: 315 / 2100 
## Progress: 316 / 2100 
## Progress: 317 / 2100 
## Progress: 318 / 2100 
## Progress: 319 / 2100 
## Progress: 320 / 2100 
## Progress: 321 / 2100 
## Progress: 322 / 2100 
## Progress: 323 / 2100 
## Progress: 324 / 2100 
## Progress: 325 / 2100 
## Progress: 326 / 2100 
## Progress: 327 / 2100 
## Progress: 328 / 2100 
## Progress: 329 / 2100 
## Progress: 330 / 2100 
## Progress: 331 / 2100 
## Progress: 332 / 2100 
## Progress: 333 / 2100 
## Progress: 334 / 2100 
## Progress: 335 / 2100 
## Progress: 336 / 2100 
## Progress: 337 / 2100 
## Progress: 338 / 2100 
## Progress: 339 / 2100 
## Progress: 340 / 2100 
## Progress: 341 / 2100 
## Progress: 342 / 2100 
## Progress: 343 / 2100 
## Progress: 344 / 2100 
## Progress: 345 / 2100 
## Progress: 346 / 2100 
## Progress: 347 / 2100 
## Progress: 348 / 2100 
## Progress: 349 / 2100 
## Progress: 350 / 2100 
## Progress: 351 / 2100 
## Progress: 352 / 2100 
## Progress: 353 / 2100 
## Progress: 354 / 2100 
## Progress: 355 / 2100 
## Progress: 356 / 2100 
## Progress: 357 / 2100 
## Progress: 358 / 2100 
## Progress: 359 / 2100 
## Progress: 360 / 2100 
## Progress: 361 / 2100 
## Progress: 362 / 2100 
## Progress: 363 / 2100 
## Progress: 364 / 2100 
## Progress: 365 / 2100 
## Progress: 366 / 2100 
## Progress: 367 / 2100 
## Progress: 368 / 2100 
## Progress: 369 / 2100 
## Progress: 370 / 2100 
## Progress: 371 / 2100 
## Progress: 372 / 2100 
## Progress: 373 / 2100 
## Progress: 374 / 2100 
## Progress: 375 / 2100 
## Progress: 376 / 2100 
## Progress: 377 / 2100 
## Progress: 378 / 2100 
## Progress: 379 / 2100 
## Progress: 380 / 2100 
## Progress: 381 / 2100 
## Progress: 382 / 2100 
## Progress: 383 / 2100 
## Progress: 384 / 2100 
## Progress: 385 / 2100 
## Progress: 386 / 2100 
## Progress: 387 / 2100 
## Progress: 388 / 2100 
## Progress: 389 / 2100 
## Progress: 390 / 2100 
## Progress: 391 / 2100 
## Progress: 392 / 2100 
## Progress: 393 / 2100 
## Progress: 394 / 2100 
## Progress: 395 / 2100 
## Progress: 396 / 2100 
## Progress: 397 / 2100 
## Progress: 398 / 2100 
## Progress: 399 / 2100 
## Progress: 400 / 2100 
## Progress: 401 / 2100 
## Progress: 402 / 2100 
## Progress: 403 / 2100 
## Progress: 404 / 2100 
## Progress: 405 / 2100 
## Progress: 406 / 2100 
## Progress: 407 / 2100 
## Progress: 408 / 2100 
## Progress: 409 / 2100 
## Progress: 410 / 2100 
## Progress: 411 / 2100 
## Progress: 412 / 2100 
## Progress: 413 / 2100 
## Progress: 414 / 2100 
## Progress: 415 / 2100 
## Progress: 416 / 2100 
## Progress: 417 / 2100 
## Progress: 418 / 2100 
## Progress: 419 / 2100 
## Progress: 420 / 2100 
## Progress: 421 / 2100 
## Progress: 422 / 2100 
## Progress: 423 / 2100 
## Progress: 424 / 2100 
## Progress: 425 / 2100 
## Progress: 426 / 2100 
## Progress: 427 / 2100 
## Progress: 428 / 2100 
## Progress: 429 / 2100 
## Progress: 430 / 2100 
## Progress: 431 / 2100 
## Progress: 432 / 2100 
## Progress: 433 / 2100 
## Progress: 434 / 2100 
## Progress: 435 / 2100 
## Progress: 436 / 2100 
## Progress: 437 / 2100 
## Progress: 438 / 2100 
## Progress: 439 / 2100 
## Progress: 440 / 2100 
## Progress: 441 / 2100 
## Progress: 442 / 2100 
## Progress: 443 / 2100 
## Progress: 444 / 2100 
## Progress: 445 / 2100 
## Progress: 446 / 2100 
## Progress: 447 / 2100 
## Progress: 448 / 2100 
## Progress: 449 / 2100 
## Progress: 450 / 2100 
## Progress: 451 / 2100 
## Progress: 452 / 2100 
## Progress: 453 / 2100 
## Progress: 454 / 2100 
## Progress: 455 / 2100 
## Progress: 456 / 2100 
## Progress: 457 / 2100 
## Progress: 458 / 2100 
## Progress: 459 / 2100 
## Progress: 460 / 2100 
## Progress: 461 / 2100 
## Progress: 462 / 2100 
## Progress: 463 / 2100 
## Progress: 464 / 2100 
## Progress: 465 / 2100 
## Progress: 466 / 2100 
## Progress: 467 / 2100 
## Progress: 468 / 2100 
## Progress: 469 / 2100 
## Progress: 470 / 2100 
## Progress: 471 / 2100 
## Progress: 472 / 2100 
## Progress: 473 / 2100 
## Progress: 474 / 2100 
## Progress: 475 / 2100 
## Progress: 476 / 2100 
## Progress: 477 / 2100 
## Progress: 478 / 2100 
## Progress: 479 / 2100 
## Progress: 480 / 2100 
## Progress: 481 / 2100 
## Progress: 482 / 2100 
## Progress: 483 / 2100 
## Progress: 484 / 2100 
## Progress: 485 / 2100 
## Progress: 486 / 2100 
## Progress: 487 / 2100 
## Progress: 488 / 2100 
## Progress: 489 / 2100 
## Progress: 490 / 2100 
## Progress: 491 / 2100 
## Progress: 492 / 2100 
## Progress: 493 / 2100 
## Progress: 494 / 2100 
## Progress: 495 / 2100 
## Progress: 496 / 2100 
## Progress: 497 / 2100 
## Progress: 498 / 2100 
## Progress: 499 / 2100 
## Progress: 500 / 2100 
## Progress: 501 / 2100 
## Progress: 502 / 2100 
## Progress: 503 / 2100 
## Progress: 504 / 2100 
## Progress: 505 / 2100 
## Progress: 506 / 2100 
## Progress: 507 / 2100 
## Progress: 508 / 2100 
## Progress: 509 / 2100 
## Progress: 510 / 2100 
## Progress: 511 / 2100 
## Progress: 512 / 2100 
## Progress: 513 / 2100 
## Progress: 514 / 2100 
## Progress: 515 / 2100 
## Progress: 516 / 2100 
## Progress: 517 / 2100 
## Progress: 518 / 2100 
## Progress: 519 / 2100 
## Progress: 520 / 2100 
## Progress: 521 / 2100 
## Progress: 522 / 2100 
## Progress: 523 / 2100 
## Progress: 524 / 2100 
## Progress: 525 / 2100 
## Progress: 526 / 2100 
## Progress: 527 / 2100 
## Progress: 528 / 2100 
## Progress: 529 / 2100 
## Progress: 530 / 2100 
## Progress: 531 / 2100 
## Progress: 532 / 2100 
## Progress: 533 / 2100 
## Progress: 534 / 2100 
## Progress: 535 / 2100 
## Progress: 536 / 2100 
## Progress: 537 / 2100 
## Progress: 538 / 2100 
## Progress: 539 / 2100 
## Progress: 540 / 2100 
## Progress: 541 / 2100 
## Progress: 542 / 2100 
## Progress: 543 / 2100 
## Progress: 544 / 2100 
## Progress: 545 / 2100 
## Progress: 546 / 2100 
## Progress: 547 / 2100 
## Progress: 548 / 2100 
## Progress: 549 / 2100 
## Progress: 550 / 2100 
## Progress: 551 / 2100 
## Progress: 552 / 2100 
## Progress: 553 / 2100 
## Progress: 554 / 2100 
## Progress: 555 / 2100 
## Progress: 556 / 2100 
## Progress: 557 / 2100 
## Progress: 558 / 2100 
## Progress: 559 / 2100 
## Progress: 560 / 2100 
## Progress: 561 / 2100 
## Progress: 562 / 2100 
## Progress: 563 / 2100 
## Progress: 564 / 2100 
## Progress: 565 / 2100 
## Progress: 566 / 2100 
## Progress: 567 / 2100 
## Progress: 568 / 2100 
## Progress: 569 / 2100 
## Progress: 570 / 2100 
## Progress: 571 / 2100 
## Progress: 572 / 2100 
## Progress: 573 / 2100 
## Progress: 574 / 2100 
## Progress: 575 / 2100 
## Progress: 576 / 2100 
## Progress: 577 / 2100 
## Progress: 578 / 2100 
## Progress: 579 / 2100 
## Progress: 580 / 2100 
## Progress: 581 / 2100 
## Progress: 582 / 2100 
## Progress: 583 / 2100 
## Progress: 584 / 2100 
## Progress: 585 / 2100 
## Progress: 586 / 2100 
## Progress: 587 / 2100 
## Progress: 588 / 2100 
## Progress: 589 / 2100 
## Progress: 590 / 2100 
## Progress: 591 / 2100 
## Progress: 592 / 2100 
## Progress: 593 / 2100 
## Progress: 594 / 2100 
## Progress: 595 / 2100 
## Progress: 596 / 2100 
## Progress: 597 / 2100 
## Progress: 598 / 2100 
## Progress: 599 / 2100 
## Progress: 600 / 2100 
## Progress: 601 / 2100 
## Progress: 602 / 2100 
## Progress: 603 / 2100 
## Progress: 604 / 2100 
## Progress: 605 / 2100 
## Progress: 606 / 2100 
## Progress: 607 / 2100 
## Progress: 608 / 2100 
## Progress: 609 / 2100 
## Progress: 610 / 2100 
## Progress: 611 / 2100 
## Progress: 612 / 2100 
## Progress: 613 / 2100 
## Progress: 614 / 2100 
## Progress: 615 / 2100 
## Progress: 616 / 2100 
## Progress: 617 / 2100 
## Progress: 618 / 2100 
## Progress: 619 / 2100 
## Progress: 620 / 2100 
## Progress: 621 / 2100 
## Progress: 622 / 2100 
## Progress: 623 / 2100 
## Progress: 624 / 2100 
## Progress: 625 / 2100 
## Progress: 626 / 2100 
## Progress: 627 / 2100 
## Progress: 628 / 2100 
## Progress: 629 / 2100 
## Progress: 630 / 2100 
## Progress: 631 / 2100 
## Progress: 632 / 2100 
## Progress: 633 / 2100 
## Progress: 634 / 2100 
## Progress: 635 / 2100 
## Progress: 636 / 2100 
## Progress: 637 / 2100 
## Progress: 638 / 2100 
## Progress: 639 / 2100 
## Progress: 640 / 2100 
## Progress: 641 / 2100 
## Progress: 642 / 2100 
## Progress: 643 / 2100 
## Progress: 644 / 2100 
## Progress: 645 / 2100 
## Progress: 646 / 2100 
## Progress: 647 / 2100 
## Progress: 648 / 2100 
## Progress: 649 / 2100 
## Progress: 650 / 2100 
## Progress: 651 / 2100 
## Progress: 652 / 2100 
## Progress: 653 / 2100 
## Progress: 654 / 2100 
## Progress: 655 / 2100 
## Progress: 656 / 2100 
## Progress: 657 / 2100 
## Progress: 658 / 2100 
## Progress: 659 / 2100 
## Progress: 660 / 2100 
## Progress: 661 / 2100 
## Progress: 662 / 2100 
## Progress: 663 / 2100 
## Progress: 664 / 2100 
## Progress: 665 / 2100 
## Progress: 666 / 2100 
## Progress: 667 / 2100 
## Progress: 668 / 2100 
## Progress: 669 / 2100 
## Progress: 670 / 2100 
## Progress: 671 / 2100 
## Progress: 672 / 2100 
## Progress: 673 / 2100 
## Progress: 674 / 2100 
## Progress: 675 / 2100 
## Progress: 676 / 2100 
## Progress: 677 / 2100 
## Progress: 678 / 2100 
## Progress: 679 / 2100 
## Progress: 680 / 2100 
## Progress: 681 / 2100 
## Progress: 682 / 2100 
## Progress: 683 / 2100 
## Progress: 684 / 2100 
## Progress: 685 / 2100 
## Progress: 686 / 2100 
## Progress: 687 / 2100 
## Progress: 688 / 2100 
## Progress: 689 / 2100 
## Progress: 690 / 2100 
## Progress: 691 / 2100 
## Progress: 692 / 2100 
## Progress: 693 / 2100 
## Progress: 694 / 2100 
## Progress: 695 / 2100 
## Progress: 696 / 2100 
## Progress: 697 / 2100 
## Progress: 698 / 2100 
## Progress: 699 / 2100 
## Progress: 700 / 2100 
## Progress: 701 / 2100 
## Progress: 702 / 2100 
## Progress: 703 / 2100 
## Progress: 704 / 2100 
## Progress: 705 / 2100 
## Progress: 706 / 2100 
## Progress: 707 / 2100 
## Progress: 708 / 2100 
## Progress: 709 / 2100 
## Progress: 710 / 2100 
## Progress: 711 / 2100 
## Progress: 712 / 2100 
## Progress: 713 / 2100 
## Progress: 714 / 2100 
## Progress: 715 / 2100 
## Progress: 716 / 2100 
## Progress: 717 / 2100 
## Progress: 718 / 2100 
## Progress: 719 / 2100 
## Progress: 720 / 2100 
## Progress: 721 / 2100 
## Progress: 722 / 2100 
## Progress: 723 / 2100 
## Progress: 724 / 2100 
## Progress: 725 / 2100 
## Progress: 726 / 2100 
## Progress: 727 / 2100 
## Progress: 728 / 2100 
## Progress: 729 / 2100 
## Progress: 730 / 2100 
## Progress: 731 / 2100 
## Progress: 732 / 2100 
## Progress: 733 / 2100 
## Progress: 734 / 2100 
## Progress: 735 / 2100 
## Progress: 736 / 2100 
## Progress: 737 / 2100 
## Progress: 738 / 2100 
## Progress: 739 / 2100 
## Progress: 740 / 2100 
## Progress: 741 / 2100 
## Progress: 742 / 2100 
## Progress: 743 / 2100 
## Progress: 744 / 2100 
## Progress: 745 / 2100 
## Progress: 746 / 2100 
## Progress: 747 / 2100 
## Progress: 748 / 2100 
## Progress: 749 / 2100 
## Progress: 750 / 2100 
## Progress: 751 / 2100 
## Progress: 752 / 2100 
## Progress: 753 / 2100 
## Progress: 754 / 2100 
## Progress: 755 / 2100 
## Progress: 756 / 2100 
## Progress: 757 / 2100 
## Progress: 758 / 2100 
## Progress: 759 / 2100 
## Progress: 760 / 2100 
## Progress: 761 / 2100 
## Progress: 762 / 2100 
## Progress: 763 / 2100 
## Progress: 764 / 2100 
## Progress: 765 / 2100 
## Progress: 766 / 2100 
## Progress: 767 / 2100 
## Progress: 768 / 2100 
## Progress: 769 / 2100 
## Progress: 770 / 2100 
## Progress: 771 / 2100 
## Progress: 772 / 2100 
## Progress: 773 / 2100 
## Progress: 774 / 2100 
## Progress: 775 / 2100 
## Progress: 776 / 2100 
## Progress: 777 / 2100 
## Progress: 778 / 2100 
## Progress: 779 / 2100 
## Progress: 780 / 2100 
## Progress: 781 / 2100 
## Progress: 782 / 2100 
## Progress: 783 / 2100 
## Progress: 784 / 2100 
## Progress: 785 / 2100 
## Progress: 786 / 2100 
## Progress: 787 / 2100 
## Progress: 788 / 2100 
## Progress: 789 / 2100 
## Progress: 790 / 2100 
## Progress: 791 / 2100 
## Progress: 792 / 2100 
## Progress: 793 / 2100 
## Progress: 794 / 2100 
## Progress: 795 / 2100 
## Progress: 796 / 2100 
## Progress: 797 / 2100 
## Progress: 798 / 2100 
## Progress: 799 / 2100 
## Progress: 800 / 2100 
## Progress: 801 / 2100 
## Progress: 802 / 2100 
## Progress: 803 / 2100 
## Progress: 804 / 2100 
## Progress: 805 / 2100 
## Progress: 806 / 2100 
## Progress: 807 / 2100 
## Progress: 808 / 2100 
## Progress: 809 / 2100 
## Progress: 810 / 2100 
## Progress: 811 / 2100 
## Progress: 812 / 2100 
## Progress: 813 / 2100 
## Progress: 814 / 2100 
## Progress: 815 / 2100 
## Progress: 816 / 2100 
## Progress: 817 / 2100 
## Progress: 818 / 2100 
## Progress: 819 / 2100 
## Progress: 820 / 2100 
## Progress: 821 / 2100 
## Progress: 822 / 2100 
## Progress: 823 / 2100 
## Progress: 824 / 2100 
## Progress: 825 / 2100 
## Progress: 826 / 2100 
## Progress: 827 / 2100 
## Progress: 828 / 2100 
## Progress: 829 / 2100 
## Progress: 830 / 2100 
## Progress: 831 / 2100 
## Progress: 832 / 2100 
## Progress: 833 / 2100 
## Progress: 834 / 2100 
## Progress: 835 / 2100 
## Progress: 836 / 2100 
## Progress: 837 / 2100 
## Progress: 838 / 2100 
## Progress: 839 / 2100 
## Progress: 840 / 2100 
## Progress: 841 / 2100 
## Progress: 842 / 2100 
## Progress: 843 / 2100 
## Progress: 844 / 2100 
## Progress: 845 / 2100 
## Progress: 846 / 2100 
## Progress: 847 / 2100 
## Progress: 848 / 2100 
## Progress: 849 / 2100 
## Progress: 850 / 2100 
## Progress: 851 / 2100 
## Progress: 852 / 2100 
## Progress: 853 / 2100 
## Progress: 854 / 2100 
## Progress: 855 / 2100 
## Progress: 856 / 2100 
## Progress: 857 / 2100 
## Progress: 858 / 2100 
## Progress: 859 / 2100 
## Progress: 860 / 2100 
## Progress: 861 / 2100 
## Progress: 862 / 2100 
## Progress: 863 / 2100 
## Progress: 864 / 2100 
## Progress: 865 / 2100 
## Progress: 866 / 2100 
## Progress: 867 / 2100 
## Progress: 868 / 2100 
## Progress: 869 / 2100 
## Progress: 870 / 2100 
## Progress: 871 / 2100 
## Progress: 872 / 2100 
## Progress: 873 / 2100 
## Progress: 874 / 2100 
## Progress: 875 / 2100 
## Progress: 876 / 2100 
## Progress: 877 / 2100 
## Progress: 878 / 2100 
## Progress: 879 / 2100 
## Progress: 880 / 2100 
## Progress: 881 / 2100 
## Progress: 882 / 2100 
## Progress: 883 / 2100 
## Progress: 884 / 2100 
## Progress: 885 / 2100 
## Progress: 886 / 2100 
## Progress: 887 / 2100 
## Progress: 888 / 2100 
## Progress: 889 / 2100 
## Progress: 890 / 2100 
## Progress: 891 / 2100 
## Progress: 892 / 2100 
## Progress: 893 / 2100 
## Progress: 894 / 2100 
## Progress: 895 / 2100 
## Progress: 896 / 2100 
## Progress: 897 / 2100 
## Progress: 898 / 2100 
## Progress: 899 / 2100 
## Progress: 900 / 2100 
## Progress: 901 / 2100 
## Progress: 902 / 2100 
## Progress: 903 / 2100 
## Progress: 904 / 2100 
## Progress: 905 / 2100 
## Progress: 906 / 2100 
## Progress: 907 / 2100 
## Progress: 908 / 2100 
## Progress: 909 / 2100 
## Progress: 910 / 2100 
## Progress: 911 / 2100 
## Progress: 912 / 2100 
## Progress: 913 / 2100 
## Progress: 914 / 2100 
## Progress: 915 / 2100 
## Progress: 916 / 2100 
## Progress: 917 / 2100 
## Progress: 918 / 2100 
## Progress: 919 / 2100 
## Progress: 920 / 2100 
## Progress: 921 / 2100 
## Progress: 922 / 2100 
## Progress: 923 / 2100 
## Progress: 924 / 2100 
## Progress: 925 / 2100 
## Progress: 926 / 2100 
## Progress: 927 / 2100 
## Progress: 928 / 2100 
## Progress: 929 / 2100 
## Progress: 930 / 2100 
## Progress: 931 / 2100 
## Progress: 932 / 2100 
## Progress: 933 / 2100 
## Progress: 934 / 2100 
## Progress: 935 / 2100 
## Progress: 936 / 2100 
## Progress: 937 / 2100 
## Progress: 938 / 2100 
## Progress: 939 / 2100 
## Progress: 940 / 2100 
## Progress: 941 / 2100 
## Progress: 942 / 2100 
## Progress: 943 / 2100 
## Progress: 944 / 2100 
## Progress: 945 / 2100 
## Progress: 946 / 2100 
## Progress: 947 / 2100 
## Progress: 948 / 2100 
## Progress: 949 / 2100 
## Progress: 950 / 2100 
## Progress: 951 / 2100 
## Progress: 952 / 2100 
## Progress: 953 / 2100 
## Progress: 954 / 2100 
## Progress: 955 / 2100 
## Progress: 956 / 2100 
## Progress: 957 / 2100 
## Progress: 958 / 2100 
## Progress: 959 / 2100 
## Progress: 960 / 2100 
## Progress: 961 / 2100 
## Progress: 962 / 2100 
## Progress: 963 / 2100 
## Progress: 964 / 2100 
## Progress: 965 / 2100 
## Progress: 966 / 2100 
## Progress: 967 / 2100 
## Progress: 968 / 2100 
## Progress: 969 / 2100 
## Progress: 970 / 2100 
## Progress: 971 / 2100 
## Progress: 972 / 2100 
## Progress: 973 / 2100 
## Progress: 974 / 2100 
## Progress: 975 / 2100 
## Progress: 976 / 2100 
## Progress: 977 / 2100 
## Progress: 978 / 2100 
## Progress: 979 / 2100 
## Progress: 980 / 2100 
## Progress: 981 / 2100 
## Progress: 982 / 2100 
## Progress: 983 / 2100 
## Progress: 984 / 2100 
## Progress: 985 / 2100 
## Progress: 986 / 2100 
## Progress: 987 / 2100 
## Progress: 988 / 2100 
## Progress: 989 / 2100 
## Progress: 990 / 2100 
## Progress: 991 / 2100 
## Progress: 992 / 2100 
## Progress: 993 / 2100 
## Progress: 994 / 2100 
## Progress: 995 / 2100 
## Progress: 996 / 2100 
## Progress: 997 / 2100 
## Progress: 998 / 2100 
## Progress: 999 / 2100 
## Progress: 1000 / 2100 
## Progress: 1001 / 2100 
## Progress: 1002 / 2100 
## Progress: 1003 / 2100 
## Progress: 1004 / 2100 
## Progress: 1005 / 2100 
## Progress: 1006 / 2100 
## Progress: 1007 / 2100 
## Progress: 1008 / 2100 
## Progress: 1009 / 2100 
## Progress: 1010 / 2100 
## Progress: 1011 / 2100 
## Progress: 1012 / 2100 
## Progress: 1013 / 2100 
## Progress: 1014 / 2100 
## Progress: 1015 / 2100 
## Progress: 1016 / 2100 
## Progress: 1017 / 2100 
## Progress: 1018 / 2100 
## Progress: 1019 / 2100 
## Progress: 1020 / 2100 
## Progress: 1021 / 2100 
## Progress: 1022 / 2100 
## Progress: 1023 / 2100 
## Progress: 1024 / 2100 
## Progress: 1025 / 2100 
## Progress: 1026 / 2100 
## Progress: 1027 / 2100 
## Progress: 1028 / 2100 
## Progress: 1029 / 2100 
## Progress: 1030 / 2100 
## Progress: 1031 / 2100 
## Progress: 1032 / 2100 
## Progress: 1033 / 2100 
## Progress: 1034 / 2100 
## Progress: 1035 / 2100 
## Progress: 1036 / 2100 
## Progress: 1037 / 2100 
## Progress: 1038 / 2100 
## Progress: 1039 / 2100 
## Progress: 1040 / 2100 
## Progress: 1041 / 2100 
## Progress: 1042 / 2100 
## Progress: 1043 / 2100 
## Progress: 1044 / 2100 
## Progress: 1045 / 2100 
## Progress: 1046 / 2100 
## Progress: 1047 / 2100 
## Progress: 1048 / 2100 
## Progress: 1049 / 2100 
## Progress: 1050 / 2100 
## Progress: 1051 / 2100 
## Progress: 1052 / 2100 
## Progress: 1053 / 2100 
## Progress: 1054 / 2100 
## Progress: 1055 / 2100 
## Progress: 1056 / 2100 
## Progress: 1057 / 2100 
## Progress: 1058 / 2100 
## Progress: 1059 / 2100 
## Progress: 1060 / 2100 
## Progress: 1061 / 2100 
## Progress: 1062 / 2100 
## Progress: 1063 / 2100 
## Progress: 1064 / 2100 
## Progress: 1065 / 2100 
## Progress: 1066 / 2100 
## Progress: 1067 / 2100 
## Progress: 1068 / 2100 
## Progress: 1069 / 2100 
## Progress: 1070 / 2100 
## Progress: 1071 / 2100 
## Progress: 1072 / 2100 
## Progress: 1073 / 2100 
## Progress: 1074 / 2100 
## Progress: 1075 / 2100 
## Progress: 1076 / 2100 
## Progress: 1077 / 2100 
## Progress: 1078 / 2100 
## Progress: 1079 / 2100 
## Progress: 1080 / 2100 
## Progress: 1081 / 2100 
## Progress: 1082 / 2100 
## Progress: 1083 / 2100 
## Progress: 1084 / 2100 
## Progress: 1085 / 2100 
## Progress: 1086 / 2100 
## Progress: 1087 / 2100 
## Progress: 1088 / 2100 
## Progress: 1089 / 2100 
## Progress: 1090 / 2100 
## Progress: 1091 / 2100 
## Progress: 1092 / 2100 
## Progress: 1093 / 2100 
## Progress: 1094 / 2100 
## Progress: 1095 / 2100 
## Progress: 1096 / 2100 
## Progress: 1097 / 2100 
## Progress: 1098 / 2100 
## Progress: 1099 / 2100 
## Progress: 1100 / 2100 
## Progress: 1101 / 2100 
## Progress: 1102 / 2100 
## Progress: 1103 / 2100 
## Progress: 1104 / 2100 
## Progress: 1105 / 2100 
## Progress: 1106 / 2100 
## Progress: 1107 / 2100 
## Progress: 1108 / 2100 
## Progress: 1109 / 2100 
## Progress: 1110 / 2100 
## Progress: 1111 / 2100 
## Progress: 1112 / 2100 
## Progress: 1113 / 2100 
## Progress: 1114 / 2100 
## Progress: 1115 / 2100 
## Progress: 1116 / 2100 
## Progress: 1117 / 2100 
## Progress: 1118 / 2100 
## Progress: 1119 / 2100 
## Progress: 1120 / 2100 
## Progress: 1121 / 2100 
## Progress: 1122 / 2100 
## Progress: 1123 / 2100 
## Progress: 1124 / 2100 
## Progress: 1125 / 2100 
## Progress: 1126 / 2100 
## Progress: 1127 / 2100 
## Progress: 1128 / 2100 
## Progress: 1129 / 2100 
## Progress: 1130 / 2100 
## Progress: 1131 / 2100 
## Progress: 1132 / 2100 
## Progress: 1133 / 2100 
## Progress: 1134 / 2100 
## Progress: 1135 / 2100 
## Progress: 1136 / 2100 
## Progress: 1137 / 2100 
## Progress: 1138 / 2100 
## Progress: 1139 / 2100 
## Progress: 1140 / 2100 
## Progress: 1141 / 2100 
## Progress: 1142 / 2100 
## Progress: 1143 / 2100 
## Progress: 1144 / 2100 
## Progress: 1145 / 2100 
## Progress: 1146 / 2100 
## Progress: 1147 / 2100 
## Progress: 1148 / 2100 
## Progress: 1149 / 2100 
## Progress: 1150 / 2100 
## Progress: 1151 / 2100 
## Progress: 1152 / 2100 
## Progress: 1153 / 2100 
## Progress: 1154 / 2100 
## Progress: 1155 / 2100 
## Progress: 1156 / 2100 
## Progress: 1157 / 2100 
## Progress: 1158 / 2100 
## Progress: 1159 / 2100 
## Progress: 1160 / 2100 
## Progress: 1161 / 2100 
## Progress: 1162 / 2100 
## Progress: 1163 / 2100 
## Progress: 1164 / 2100 
## Progress: 1165 / 2100 
## Progress: 1166 / 2100 
## Progress: 1167 / 2100 
## Progress: 1168 / 2100 
## Progress: 1169 / 2100 
## Progress: 1170 / 2100 
## Progress: 1171 / 2100 
## Progress: 1172 / 2100 
## Progress: 1173 / 2100 
## Progress: 1174 / 2100 
## Progress: 1175 / 2100 
## Progress: 1176 / 2100 
## Progress: 1177 / 2100 
## Progress: 1178 / 2100 
## Progress: 1179 / 2100 
## Progress: 1180 / 2100 
## Progress: 1181 / 2100 
## Progress: 1182 / 2100 
## Progress: 1183 / 2100 
## Progress: 1184 / 2100 
## Progress: 1185 / 2100 
## Progress: 1186 / 2100 
## Progress: 1187 / 2100 
## Progress: 1188 / 2100 
## Progress: 1189 / 2100 
## Progress: 1190 / 2100 
## Progress: 1191 / 2100 
## Progress: 1192 / 2100 
## Progress: 1193 / 2100 
## Progress: 1194 / 2100 
## Progress: 1195 / 2100 
## Progress: 1196 / 2100 
## Progress: 1197 / 2100 
## Progress: 1198 / 2100 
## Progress: 1199 / 2100 
## Progress: 1200 / 2100 
## Progress: 1201 / 2100 
## Progress: 1202 / 2100 
## Progress: 1203 / 2100 
## Progress: 1204 / 2100 
## Progress: 1205 / 2100 
## Progress: 1206 / 2100 
## Progress: 1207 / 2100 
## Progress: 1208 / 2100 
## Progress: 1209 / 2100 
## Progress: 1210 / 2100 
## Progress: 1211 / 2100 
## Progress: 1212 / 2100 
## Progress: 1213 / 2100 
## Progress: 1214 / 2100 
## Progress: 1215 / 2100 
## Progress: 1216 / 2100 
## Progress: 1217 / 2100 
## Progress: 1218 / 2100 
## Progress: 1219 / 2100 
## Progress: 1220 / 2100 
## Progress: 1221 / 2100 
## Progress: 1222 / 2100 
## Progress: 1223 / 2100 
## Progress: 1224 / 2100 
## Progress: 1225 / 2100 
## Progress: 1226 / 2100 
## Progress: 1227 / 2100 
## Progress: 1228 / 2100 
## Progress: 1229 / 2100 
## Progress: 1230 / 2100 
## Progress: 1231 / 2100 
## Progress: 1232 / 2100 
## Progress: 1233 / 2100 
## Progress: 1234 / 2100 
## Progress: 1235 / 2100 
## Progress: 1236 / 2100 
## Progress: 1237 / 2100 
## Progress: 1238 / 2100 
## Progress: 1239 / 2100 
## Progress: 1240 / 2100 
## Progress: 1241 / 2100 
## Progress: 1242 / 2100 
## Progress: 1243 / 2100 
## Progress: 1244 / 2100 
## Progress: 1245 / 2100 
## Progress: 1246 / 2100 
## Progress: 1247 / 2100 
## Progress: 1248 / 2100 
## Progress: 1249 / 2100 
## Progress: 1250 / 2100 
## Progress: 1251 / 2100 
## Progress: 1252 / 2100 
## Progress: 1253 / 2100 
## Progress: 1254 / 2100 
## Progress: 1255 / 2100 
## Progress: 1256 / 2100 
## Progress: 1257 / 2100 
## Progress: 1258 / 2100 
## Progress: 1259 / 2100 
## Progress: 1260 / 2100 
## Progress: 1261 / 2100 
## Progress: 1262 / 2100 
## Progress: 1263 / 2100 
## Progress: 1264 / 2100 
## Progress: 1265 / 2100 
## Progress: 1266 / 2100 
## Progress: 1267 / 2100 
## Progress: 1268 / 2100 
## Progress: 1269 / 2100 
## Progress: 1270 / 2100 
## Progress: 1271 / 2100 
## Progress: 1272 / 2100 
## Progress: 1273 / 2100 
## Progress: 1274 / 2100 
## Progress: 1275 / 2100 
## Progress: 1276 / 2100 
## Progress: 1277 / 2100 
## Progress: 1278 / 2100 
## Progress: 1279 / 2100 
## Progress: 1280 / 2100 
## Progress: 1281 / 2100 
## Progress: 1282 / 2100 
## Progress: 1283 / 2100 
## Progress: 1284 / 2100 
## Progress: 1285 / 2100 
## Progress: 1286 / 2100 
## Progress: 1287 / 2100 
## Progress: 1288 / 2100 
## Progress: 1289 / 2100 
## Progress: 1290 / 2100 
## Progress: 1291 / 2100 
## Progress: 1292 / 2100 
## Progress: 1293 / 2100 
## Progress: 1294 / 2100 
## Progress: 1295 / 2100 
## Progress: 1296 / 2100 
## Progress: 1297 / 2100 
## Progress: 1298 / 2100 
## Progress: 1299 / 2100 
## Progress: 1300 / 2100 
## Progress: 1301 / 2100 
## Progress: 1302 / 2100 
## Progress: 1303 / 2100 
## Progress: 1304 / 2100 
## Progress: 1305 / 2100 
## Progress: 1306 / 2100 
## Progress: 1307 / 2100 
## Progress: 1308 / 2100 
## Progress: 1309 / 2100 
## Progress: 1310 / 2100 
## Progress: 1311 / 2100 
## Progress: 1312 / 2100 
## Progress: 1313 / 2100 
## Progress: 1314 / 2100 
## Progress: 1315 / 2100 
## Progress: 1316 / 2100 
## Progress: 1317 / 2100 
## Progress: 1318 / 2100 
## Progress: 1319 / 2100 
## Progress: 1320 / 2100 
## Progress: 1321 / 2100 
## Progress: 1322 / 2100 
## Progress: 1323 / 2100 
## Progress: 1324 / 2100 
## Progress: 1325 / 2100 
## Progress: 1326 / 2100 
## Progress: 1327 / 2100 
## Progress: 1328 / 2100 
## Progress: 1329 / 2100 
## Progress: 1330 / 2100 
## Progress: 1331 / 2100 
## Progress: 1332 / 2100 
## Progress: 1333 / 2100 
## Progress: 1334 / 2100 
## Progress: 1335 / 2100 
## Progress: 1336 / 2100 
## Progress: 1337 / 2100 
## Progress: 1338 / 2100 
## Progress: 1339 / 2100 
## Progress: 1340 / 2100 
## Progress: 1341 / 2100 
## Progress: 1342 / 2100 
## Progress: 1343 / 2100 
## Progress: 1344 / 2100 
## Progress: 1345 / 2100 
## Progress: 1346 / 2100 
## Progress: 1347 / 2100 
## Progress: 1348 / 2100 
## Progress: 1349 / 2100 
## Progress: 1350 / 2100 
## Progress: 1351 / 2100 
## Progress: 1352 / 2100 
## Progress: 1353 / 2100 
## Progress: 1354 / 2100 
## Progress: 1355 / 2100 
## Progress: 1356 / 2100 
## Progress: 1357 / 2100 
## Progress: 1358 / 2100 
## Progress: 1359 / 2100 
## Progress: 1360 / 2100 
## Progress: 1361 / 2100 
## Progress: 1362 / 2100 
## Progress: 1363 / 2100 
## Progress: 1364 / 2100 
## Progress: 1365 / 2100 
## Progress: 1366 / 2100 
## Progress: 1367 / 2100 
## Progress: 1368 / 2100 
## Progress: 1369 / 2100 
## Progress: 1370 / 2100 
## Progress: 1371 / 2100 
## Progress: 1372 / 2100 
## Progress: 1373 / 2100 
## Progress: 1374 / 2100 
## Progress: 1375 / 2100 
## Progress: 1376 / 2100 
## Progress: 1377 / 2100 
## Progress: 1378 / 2100 
## Progress: 1379 / 2100 
## Progress: 1380 / 2100 
## Progress: 1381 / 2100 
## Progress: 1382 / 2100 
## Progress: 1383 / 2100 
## Progress: 1384 / 2100 
## Progress: 1385 / 2100 
## Progress: 1386 / 2100 
## Progress: 1387 / 2100 
## Progress: 1388 / 2100 
## Progress: 1389 / 2100 
## Progress: 1390 / 2100 
## Progress: 1391 / 2100 
## Progress: 1392 / 2100 
## Progress: 1393 / 2100 
## Progress: 1394 / 2100 
## Progress: 1395 / 2100 
## Progress: 1396 / 2100 
## Progress: 1397 / 2100 
## Progress: 1398 / 2100 
## Progress: 1399 / 2100 
## Progress: 1400 / 2100 
## Progress: 1401 / 2100 
## Progress: 1402 / 2100 
## Progress: 1403 / 2100 
## Progress: 1404 / 2100 
## Progress: 1405 / 2100 
## Progress: 1406 / 2100 
## Progress: 1407 / 2100 
## Progress: 1408 / 2100 
## Progress: 1409 / 2100 
## Progress: 1410 / 2100 
## Progress: 1411 / 2100 
## Progress: 1412 / 2100 
## Progress: 1413 / 2100 
## Progress: 1414 / 2100 
## Progress: 1415 / 2100 
## Progress: 1416 / 2100 
## Progress: 1417 / 2100 
## Progress: 1418 / 2100 
## Progress: 1419 / 2100 
## Progress: 1420 / 2100 
## Progress: 1421 / 2100 
## Progress: 1422 / 2100 
## Progress: 1423 / 2100 
## Progress: 1424 / 2100 
## Progress: 1425 / 2100 
## Progress: 1426 / 2100 
## Progress: 1427 / 2100 
## Progress: 1428 / 2100 
## Progress: 1429 / 2100 
## Progress: 1430 / 2100 
## Progress: 1431 / 2100 
## Progress: 1432 / 2100 
## Progress: 1433 / 2100 
## Progress: 1434 / 2100 
## Progress: 1435 / 2100 
## Progress: 1436 / 2100 
## Progress: 1437 / 2100 
## Progress: 1438 / 2100 
## Progress: 1439 / 2100 
## Progress: 1440 / 2100 
## Progress: 1441 / 2100 
## Progress: 1442 / 2100 
## Progress: 1443 / 2100 
## Progress: 1444 / 2100 
## Progress: 1445 / 2100 
## Progress: 1446 / 2100 
## Progress: 1447 / 2100 
## Progress: 1448 / 2100 
## Progress: 1449 / 2100 
## Progress: 1450 / 2100 
## Progress: 1451 / 2100 
## Progress: 1452 / 2100 
## Progress: 1453 / 2100 
## Progress: 1454 / 2100 
## Progress: 1455 / 2100 
## Progress: 1456 / 2100 
## Progress: 1457 / 2100 
## Progress: 1458 / 2100 
## Progress: 1459 / 2100 
## Progress: 1460 / 2100 
## Progress: 1461 / 2100 
## Progress: 1462 / 2100 
## Progress: 1463 / 2100 
## Progress: 1464 / 2100 
## Progress: 1465 / 2100 
## Progress: 1466 / 2100 
## Progress: 1467 / 2100 
## Progress: 1468 / 2100 
## Progress: 1469 / 2100 
## Progress: 1470 / 2100 
## Progress: 1471 / 2100 
## Progress: 1472 / 2100 
## Progress: 1473 / 2100 
## Progress: 1474 / 2100 
## Progress: 1475 / 2100 
## Progress: 1476 / 2100 
## Progress: 1477 / 2100 
## Progress: 1478 / 2100 
## Progress: 1479 / 2100 
## Progress: 1480 / 2100 
## Progress: 1481 / 2100 
## Progress: 1482 / 2100 
## Progress: 1483 / 2100 
## Progress: 1484 / 2100 
## Progress: 1485 / 2100 
## Progress: 1486 / 2100 
## Progress: 1487 / 2100 
## Progress: 1488 / 2100 
## Progress: 1489 / 2100 
## Progress: 1490 / 2100 
## Progress: 1491 / 2100 
## Progress: 1492 / 2100 
## Progress: 1493 / 2100 
## Progress: 1494 / 2100 
## Progress: 1495 / 2100 
## Progress: 1496 / 2100 
## Progress: 1497 / 2100 
## Progress: 1498 / 2100 
## Progress: 1499 / 2100 
## Progress: 1500 / 2100 
## Progress: 1501 / 2100 
## Progress: 1502 / 2100 
## Progress: 1503 / 2100 
## Progress: 1504 / 2100 
## Progress: 1505 / 2100 
## Progress: 1506 / 2100 
## Progress: 1507 / 2100 
## Progress: 1508 / 2100 
## Progress: 1509 / 2100 
## Progress: 1510 / 2100 
## Progress: 1511 / 2100 
## Progress: 1512 / 2100 
## Progress: 1513 / 2100 
## Progress: 1514 / 2100 
## Progress: 1515 / 2100 
## Progress: 1516 / 2100 
## Progress: 1517 / 2100 
## Progress: 1518 / 2100 
## Progress: 1519 / 2100 
## Progress: 1520 / 2100 
## Progress: 1521 / 2100 
## Progress: 1522 / 2100 
## Progress: 1523 / 2100 
## Progress: 1524 / 2100 
## Progress: 1525 / 2100 
## Progress: 1526 / 2100 
## Progress: 1527 / 2100 
## Progress: 1528 / 2100 
## Progress: 1529 / 2100 
## Progress: 1530 / 2100 
## Progress: 1531 / 2100 
## Progress: 1532 / 2100 
## Progress: 1533 / 2100 
## Progress: 1534 / 2100 
## Progress: 1535 / 2100 
## Progress: 1536 / 2100 
## Progress: 1537 / 2100 
## Progress: 1538 / 2100 
## Progress: 1539 / 2100 
## Progress: 1540 / 2100 
## Progress: 1541 / 2100 
## Progress: 1542 / 2100 
## Progress: 1543 / 2100 
## Progress: 1544 / 2100 
## Progress: 1545 / 2100 
## Progress: 1546 / 2100 
## Progress: 1547 / 2100 
## Progress: 1548 / 2100 
## Progress: 1549 / 2100 
## Progress: 1550 / 2100 
## Progress: 1551 / 2100 
## Progress: 1552 / 2100 
## Progress: 1553 / 2100 
## Progress: 1554 / 2100 
## Progress: 1555 / 2100 
## Progress: 1556 / 2100 
## Progress: 1557 / 2100 
## Progress: 1558 / 2100 
## Progress: 1559 / 2100 
## Progress: 1560 / 2100 
## Progress: 1561 / 2100 
## Progress: 1562 / 2100 
## Progress: 1563 / 2100 
## Progress: 1564 / 2100 
## Progress: 1565 / 2100 
## Progress: 1566 / 2100 
## Progress: 1567 / 2100 
## Progress: 1568 / 2100 
## Progress: 1569 / 2100 
## Progress: 1570 / 2100 
## Progress: 1571 / 2100 
## Progress: 1572 / 2100 
## Progress: 1573 / 2100 
## Progress: 1574 / 2100 
## Progress: 1575 / 2100 
## Progress: 1576 / 2100 
## Progress: 1577 / 2100 
## Progress: 1578 / 2100 
## Progress: 1579 / 2100 
## Progress: 1580 / 2100 
## Progress: 1581 / 2100 
## Progress: 1582 / 2100 
## Progress: 1583 / 2100 
## Progress: 1584 / 2100 
## Progress: 1585 / 2100 
## Progress: 1586 / 2100 
## Progress: 1587 / 2100 
## Progress: 1588 / 2100 
## Progress: 1589 / 2100 
## Progress: 1590 / 2100 
## Progress: 1591 / 2100 
## Progress: 1592 / 2100 
## Progress: 1593 / 2100 
## Progress: 1594 / 2100 
## Progress: 1595 / 2100 
## Progress: 1596 / 2100 
## Progress: 1597 / 2100 
## Progress: 1598 / 2100 
## Progress: 1599 / 2100 
## Progress: 1600 / 2100 
## Progress: 1601 / 2100 
## Progress: 1602 / 2100 
## Progress: 1603 / 2100 
## Progress: 1604 / 2100 
## Progress: 1605 / 2100 
## Progress: 1606 / 2100 
## Progress: 1607 / 2100 
## Progress: 1608 / 2100 
## Progress: 1609 / 2100 
## Progress: 1610 / 2100 
## Progress: 1611 / 2100 
## Progress: 1612 / 2100 
## Progress: 1613 / 2100 
## Progress: 1614 / 2100 
## Progress: 1615 / 2100 
## Progress: 1616 / 2100 
## Progress: 1617 / 2100 
## Progress: 1618 / 2100 
## Progress: 1619 / 2100 
## Progress: 1620 / 2100 
## Progress: 1621 / 2100 
## Progress: 1622 / 2100 
## Progress: 1623 / 2100 
## Progress: 1624 / 2100 
## Progress: 1625 / 2100 
## Progress: 1626 / 2100 
## Progress: 1627 / 2100 
## Progress: 1628 / 2100 
## Progress: 1629 / 2100 
## Progress: 1630 / 2100 
## Progress: 1631 / 2100 
## Progress: 1632 / 2100 
## Progress: 1633 / 2100 
## Progress: 1634 / 2100 
## Progress: 1635 / 2100 
## Progress: 1636 / 2100 
## Progress: 1637 / 2100 
## Progress: 1638 / 2100 
## Progress: 1639 / 2100 
## Progress: 1640 / 2100 
## Progress: 1641 / 2100 
## Progress: 1642 / 2100 
## Progress: 1643 / 2100 
## Progress: 1644 / 2100 
## Progress: 1645 / 2100 
## Progress: 1646 / 2100 
## Progress: 1647 / 2100 
## Progress: 1648 / 2100 
## Progress: 1649 / 2100 
## Progress: 1650 / 2100 
## Progress: 1651 / 2100 
## Progress: 1652 / 2100 
## Progress: 1653 / 2100 
## Progress: 1654 / 2100 
## Progress: 1655 / 2100 
## Progress: 1656 / 2100 
## Progress: 1657 / 2100 
## Progress: 1658 / 2100 
## Progress: 1659 / 2100 
## Progress: 1660 / 2100 
## Progress: 1661 / 2100 
## Progress: 1662 / 2100 
## Progress: 1663 / 2100 
## Progress: 1664 / 2100 
## Progress: 1665 / 2100 
## Progress: 1666 / 2100 
## Progress: 1667 / 2100 
## Progress: 1668 / 2100 
## Progress: 1669 / 2100 
## Progress: 1670 / 2100 
## Progress: 1671 / 2100 
## Progress: 1672 / 2100 
## Progress: 1673 / 2100 
## Progress: 1674 / 2100 
## Progress: 1675 / 2100 
## Progress: 1676 / 2100 
## Progress: 1677 / 2100 
## Progress: 1678 / 2100 
## Progress: 1679 / 2100 
## Progress: 1680 / 2100 
## Progress: 1681 / 2100 
## Progress: 1682 / 2100 
## Progress: 1683 / 2100 
## Progress: 1684 / 2100 
## Progress: 1685 / 2100 
## Progress: 1686 / 2100 
## Progress: 1687 / 2100 
## Progress: 1688 / 2100 
## Progress: 1689 / 2100 
## Progress: 1690 / 2100 
## Progress: 1691 / 2100 
## Progress: 1692 / 2100 
## Progress: 1693 / 2100 
## Progress: 1694 / 2100 
## Progress: 1695 / 2100 
## Progress: 1696 / 2100 
## Progress: 1697 / 2100 
## Progress: 1698 / 2100 
## Progress: 1699 / 2100 
## Progress: 1700 / 2100 
## Progress: 1701 / 2100 
## Progress: 1702 / 2100 
## Progress: 1703 / 2100 
## Progress: 1704 / 2100 
## Progress: 1705 / 2100 
## Progress: 1706 / 2100 
## Progress: 1707 / 2100 
## Progress: 1708 / 2100 
## Progress: 1709 / 2100 
## Progress: 1710 / 2100 
## Progress: 1711 / 2100 
## Progress: 1712 / 2100 
## Progress: 1713 / 2100 
## Progress: 1714 / 2100 
## Progress: 1715 / 2100 
## Progress: 1716 / 2100 
## Progress: 1717 / 2100 
## Progress: 1718 / 2100 
## Progress: 1719 / 2100 
## Progress: 1720 / 2100 
## Progress: 1721 / 2100 
## Progress: 1722 / 2100 
## Progress: 1723 / 2100 
## Progress: 1724 / 2100 
## Progress: 1725 / 2100 
## Progress: 1726 / 2100 
## Progress: 1727 / 2100 
## Progress: 1728 / 2100 
## Progress: 1729 / 2100 
## Progress: 1730 / 2100 
## Progress: 1731 / 2100 
## Progress: 1732 / 2100 
## Progress: 1733 / 2100 
## Progress: 1734 / 2100 
## Progress: 1735 / 2100 
## Progress: 1736 / 2100 
## Progress: 1737 / 2100 
## Progress: 1738 / 2100 
## Progress: 1739 / 2100 
## Progress: 1740 / 2100 
## Progress: 1741 / 2100 
## Progress: 1742 / 2100 
## Progress: 1743 / 2100 
## Progress: 1744 / 2100 
## Progress: 1745 / 2100 
## Progress: 1746 / 2100 
## Progress: 1747 / 2100 
## Progress: 1748 / 2100 
## Progress: 1749 / 2100 
## Progress: 1750 / 2100 
## Progress: 1751 / 2100 
## Progress: 1752 / 2100 
## Progress: 1753 / 2100 
## Progress: 1754 / 2100 
## Progress: 1755 / 2100 
## Progress: 1756 / 2100 
## Progress: 1757 / 2100 
## Progress: 1758 / 2100 
## Progress: 1759 / 2100 
## Progress: 1760 / 2100 
## Progress: 1761 / 2100 
## Progress: 1762 / 2100 
## Progress: 1763 / 2100 
## Progress: 1764 / 2100 
## Progress: 1765 / 2100 
## Progress: 1766 / 2100 
## Progress: 1767 / 2100 
## Progress: 1768 / 2100 
## Progress: 1769 / 2100 
## Progress: 1770 / 2100 
## Progress: 1771 / 2100 
## Progress: 1772 / 2100 
## Progress: 1773 / 2100 
## Progress: 1774 / 2100 
## Progress: 1775 / 2100 
## Progress: 1776 / 2100 
## Progress: 1777 / 2100 
## Progress: 1778 / 2100 
## Progress: 1779 / 2100 
## Progress: 1780 / 2100 
## Progress: 1781 / 2100 
## Progress: 1782 / 2100 
## Progress: 1783 / 2100 
## Progress: 1784 / 2100 
## Progress: 1785 / 2100 
## Progress: 1786 / 2100 
## Progress: 1787 / 2100 
## Progress: 1788 / 2100 
## Progress: 1789 / 2100 
## Progress: 1790 / 2100 
## Progress: 1791 / 2100 
## Progress: 1792 / 2100 
## Progress: 1793 / 2100 
## Progress: 1794 / 2100 
## Progress: 1795 / 2100 
## Progress: 1796 / 2100 
## Progress: 1797 / 2100 
## Progress: 1798 / 2100 
## Progress: 1799 / 2100 
## Progress: 1800 / 2100 
## Progress: 1801 / 2100 
## Progress: 1802 / 2100 
## Progress: 1803 / 2100 
## Progress: 1804 / 2100 
## Progress: 1805 / 2100 
## Progress: 1806 / 2100 
## Progress: 1807 / 2100 
## Progress: 1808 / 2100 
## Progress: 1809 / 2100 
## Progress: 1810 / 2100 
## Progress: 1811 / 2100 
## Progress: 1812 / 2100 
## Progress: 1813 / 2100 
## Progress: 1814 / 2100 
## Progress: 1815 / 2100 
## Progress: 1816 / 2100 
## Progress: 1817 / 2100 
## Progress: 1818 / 2100 
## Progress: 1819 / 2100 
## Progress: 1820 / 2100 
## Progress: 1821 / 2100 
## Progress: 1822 / 2100 
## Progress: 1823 / 2100 
## Progress: 1824 / 2100 
## Progress: 1825 / 2100 
## Progress: 1826 / 2100 
## Progress: 1827 / 2100 
## Progress: 1828 / 2100 
## Progress: 1829 / 2100 
## Progress: 1830 / 2100 
## Progress: 1831 / 2100 
## Progress: 1832 / 2100 
## Progress: 1833 / 2100 
## Progress: 1834 / 2100 
## Progress: 1835 / 2100 
## Progress: 1836 / 2100 
## Progress: 1837 / 2100 
## Progress: 1838 / 2100 
## Progress: 1839 / 2100 
## Progress: 1840 / 2100 
## Progress: 1841 / 2100 
## Progress: 1842 / 2100 
## Progress: 1843 / 2100 
## Progress: 1844 / 2100 
## Progress: 1845 / 2100 
## Progress: 1846 / 2100 
## Progress: 1847 / 2100 
## Progress: 1848 / 2100 
## Progress: 1849 / 2100 
## Progress: 1850 / 2100 
## Progress: 1851 / 2100 
## Progress: 1852 / 2100 
## Progress: 1853 / 2100 
## Progress: 1854 / 2100 
## Progress: 1855 / 2100 
## Progress: 1856 / 2100 
## Progress: 1857 / 2100 
## Progress: 1858 / 2100 
## Progress: 1859 / 2100 
## Progress: 1860 / 2100 
## Progress: 1861 / 2100 
## Progress: 1862 / 2100 
## Progress: 1863 / 2100 
## Progress: 1864 / 2100 
## Progress: 1865 / 2100 
## Progress: 1866 / 2100 
## Progress: 1867 / 2100 
## Progress: 1868 / 2100 
## Progress: 1869 / 2100 
## Progress: 1870 / 2100 
## Progress: 1871 / 2100 
## Progress: 1872 / 2100 
## Progress: 1873 / 2100 
## Progress: 1874 / 2100 
## Progress: 1875 / 2100 
## Progress: 1876 / 2100 
## Progress: 1877 / 2100 
## Progress: 1878 / 2100 
## Progress: 1879 / 2100 
## Progress: 1880 / 2100 
## Progress: 1881 / 2100 
## Progress: 1882 / 2100 
## Progress: 1883 / 2100 
## Progress: 1884 / 2100 
## Progress: 1885 / 2100 
## Progress: 1886 / 2100 
## Progress: 1887 / 2100 
## Progress: 1888 / 2100 
## Progress: 1889 / 2100 
## Progress: 1890 / 2100 
## Progress: 1891 / 2100 
## Progress: 1892 / 2100 
## Progress: 1893 / 2100 
## Progress: 1894 / 2100 
## Progress: 1895 / 2100 
## Progress: 1896 / 2100 
## Progress: 1897 / 2100 
## Progress: 1898 / 2100 
## Progress: 1899 / 2100 
## Progress: 1900 / 2100 
## Progress: 1901 / 2100 
## Progress: 1902 / 2100 
## Progress: 1903 / 2100 
## Progress: 1904 / 2100 
## Progress: 1905 / 2100 
## Progress: 1906 / 2100 
## Progress: 1907 / 2100 
## Progress: 1908 / 2100 
## Progress: 1909 / 2100 
## Progress: 1910 / 2100 
## Progress: 1911 / 2100 
## Progress: 1912 / 2100 
## Progress: 1913 / 2100 
## Progress: 1914 / 2100 
## Progress: 1915 / 2100 
## Progress: 1916 / 2100 
## Progress: 1917 / 2100 
## Progress: 1918 / 2100 
## Progress: 1919 / 2100 
## Progress: 1920 / 2100 
## Progress: 1921 / 2100 
## Progress: 1922 / 2100 
## Progress: 1923 / 2100 
## Progress: 1924 / 2100 
## Progress: 1925 / 2100 
## Progress: 1926 / 2100 
## Progress: 1927 / 2100 
## Progress: 1928 / 2100 
## Progress: 1929 / 2100 
## Progress: 1930 / 2100 
## Progress: 1931 / 2100 
## Progress: 1932 / 2100 
## Progress: 1933 / 2100 
## Progress: 1934 / 2100 
## Progress: 1935 / 2100 
## Progress: 1936 / 2100 
## Progress: 1937 / 2100 
## Progress: 1938 / 2100 
## Progress: 1939 / 2100 
## Progress: 1940 / 2100 
## Progress: 1941 / 2100 
## Progress: 1942 / 2100 
## Progress: 1943 / 2100 
## Progress: 1944 / 2100 
## Progress: 1945 / 2100 
## Progress: 1946 / 2100 
## Progress: 1947 / 2100 
## Progress: 1948 / 2100 
## Progress: 1949 / 2100 
## Progress: 1950 / 2100 
## Progress: 1951 / 2100 
## Progress: 1952 / 2100 
## Progress: 1953 / 2100 
## Progress: 1954 / 2100 
## Progress: 1955 / 2100 
## Progress: 1956 / 2100 
## Progress: 1957 / 2100 
## Progress: 1958 / 2100 
## Progress: 1959 / 2100 
## Progress: 1960 / 2100 
## Progress: 1961 / 2100 
## Progress: 1962 / 2100 
## Progress: 1963 / 2100 
## Progress: 1964 / 2100 
## Progress: 1965 / 2100 
## Progress: 1966 / 2100 
## Progress: 1967 / 2100 
## Progress: 1968 / 2100 
## Progress: 1969 / 2100 
## Progress: 1970 / 2100 
## Progress: 1971 / 2100 
## Progress: 1972 / 2100 
## Progress: 1973 / 2100 
## Progress: 1974 / 2100 
## Progress: 1975 / 2100 
## Progress: 1976 / 2100 
## Progress: 1977 / 2100 
## Progress: 1978 / 2100 
## Progress: 1979 / 2100 
## Progress: 1980 / 2100 
## Progress: 1981 / 2100 
## Progress: 1982 / 2100 
## Progress: 1983 / 2100 
## Progress: 1984 / 2100 
## Progress: 1985 / 2100 
## Progress: 1986 / 2100 
## Progress: 1987 / 2100 
## Progress: 1988 / 2100 
## Progress: 1989 / 2100 
## Progress: 1990 / 2100 
## Progress: 1991 / 2100 
## Progress: 1992 / 2100 
## Progress: 1993 / 2100 
## Progress: 1994 / 2100 
## Progress: 1995 / 2100 
## Progress: 1996 / 2100 
## Progress: 1997 / 2100 
## Progress: 1998 / 2100 
## Progress: 1999 / 2100 
## Progress: 2000 / 2100 
## Progress: 2001 / 2100 
## Progress: 2002 / 2100 
## Progress: 2003 / 2100 
## Progress: 2004 / 2100 
## Progress: 2005 / 2100 
## Progress: 2006 / 2100 
## Progress: 2007 / 2100 
## Progress: 2008 / 2100 
## Progress: 2009 / 2100 
## Progress: 2010 / 2100 
## Progress: 2011 / 2100 
## Progress: 2012 / 2100 
## Progress: 2013 / 2100 
## Progress: 2014 / 2100 
## Progress: 2015 / 2100 
## Progress: 2016 / 2100 
## Progress: 2017 / 2100 
## Progress: 2018 / 2100 
## Progress: 2019 / 2100 
## Progress: 2020 / 2100 
## Progress: 2021 / 2100 
## Progress: 2022 / 2100 
## Progress: 2023 / 2100 
## Progress: 2024 / 2100 
## Progress: 2025 / 2100 
## Progress: 2026 / 2100 
## Progress: 2027 / 2100 
## Progress: 2028 / 2100 
## Progress: 2029 / 2100 
## Progress: 2030 / 2100 
## Progress: 2031 / 2100 
## Progress: 2032 / 2100 
## Progress: 2033 / 2100 
## Progress: 2034 / 2100 
## Progress: 2035 / 2100 
## Progress: 2036 / 2100 
## Progress: 2037 / 2100 
## Progress: 2038 / 2100 
## Progress: 2039 / 2100 
## Progress: 2040 / 2100 
## Progress: 2041 / 2100 
## Progress: 2042 / 2100 
## Progress: 2043 / 2100 
## Progress: 2044 / 2100 
## Progress: 2045 / 2100 
## Progress: 2046 / 2100 
## Progress: 2047 / 2100 
## Progress: 2048 / 2100 
## Progress: 2049 / 2100 
## Progress: 2050 / 2100 
## Progress: 2051 / 2100 
## Progress: 2052 / 2100 
## Progress: 2053 / 2100 
## Progress: 2054 / 2100 
## Progress: 2055 / 2100 
## Progress: 2056 / 2100 
## Progress: 2057 / 2100 
## Progress: 2058 / 2100 
## Progress: 2059 / 2100 
## Progress: 2060 / 2100 
## Progress: 2061 / 2100 
## Progress: 2062 / 2100 
## Progress: 2063 / 2100 
## Progress: 2064 / 2100 
## Progress: 2065 / 2100 
## Progress: 2066 / 2100 
## Progress: 2067 / 2100 
## Progress: 2068 / 2100 
## Progress: 2069 / 2100 
## Progress: 2070 / 2100 
## Progress: 2071 / 2100 
## Progress: 2072 / 2100 
## Progress: 2073 / 2100 
## Progress: 2074 / 2100 
## Progress: 2075 / 2100 
## Progress: 2076 / 2100 
## Progress: 2077 / 2100 
## Progress: 2078 / 2100 
## Progress: 2079 / 2100 
## Progress: 2080 / 2100 
## Progress: 2081 / 2100 
## Progress: 2082 / 2100 
## Progress: 2083 / 2100 
## Progress: 2084 / 2100 
## Progress: 2085 / 2100 
## Progress: 2086 / 2100 
## Progress: 2087 / 2100 
## Progress: 2088 / 2100 
## Progress: 2089 / 2100 
## Progress: 2090 / 2100 
## Progress: 2091 / 2100 
## Progress: 2092 / 2100 
## Progress: 2093 / 2100 
## Progress: 2094 / 2100 
## Progress: 2095 / 2100 
## Progress: 2096 / 2100 
## Progress: 2097 / 2100 
## Progress: 2098 / 2100 
## Progress: 2099 / 2100 
## Progress: 2100 / 2100
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Create a power plot 
plotPower(act.n, alpha=0.05, powerParam=&amp;quot;y~x2&amp;quot;)
abline(h=.8, lwd=2, lty=2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/tutorial/2019-04-21-r-reg-sim_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Rerun for more fine grained information across wider range of sample sizes
act.n &amp;lt;- sim(model=fitted.model, 
             n =seq(10,1000,10), 
             generate=hypothesized.model, 
             lavaanfun = &amp;quot;sem&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Progress: 1 / 100 
## Progress: 2 / 100 
## Progress: 3 / 100 
## Progress: 4 / 100 
## Progress: 5 / 100 
## Progress: 6 / 100 
## Progress: 7 / 100 
## Progress: 8 / 100 
## Progress: 9 / 100 
## Progress: 10 / 100 
## Progress: 11 / 100 
## Progress: 12 / 100 
## Progress: 13 / 100 
## Progress: 14 / 100 
## Progress: 15 / 100 
## Progress: 16 / 100 
## Progress: 17 / 100 
## Progress: 18 / 100 
## Progress: 19 / 100 
## Progress: 20 / 100 
## Progress: 21 / 100 
## Progress: 22 / 100 
## Progress: 23 / 100 
## Progress: 24 / 100 
## Progress: 25 / 100 
## Progress: 26 / 100 
## Progress: 27 / 100 
## Progress: 28 / 100 
## Progress: 29 / 100 
## Progress: 30 / 100 
## Progress: 31 / 100 
## Progress: 32 / 100 
## Progress: 33 / 100 
## Progress: 34 / 100 
## Progress: 35 / 100 
## Progress: 36 / 100 
## Progress: 37 / 100 
## Progress: 38 / 100 
## Progress: 39 / 100 
## Progress: 40 / 100 
## Progress: 41 / 100 
## Progress: 42 / 100 
## Progress: 43 / 100 
## Progress: 44 / 100 
## Progress: 45 / 100 
## Progress: 46 / 100 
## Progress: 47 / 100 
## Progress: 48 / 100 
## Progress: 49 / 100 
## Progress: 50 / 100 
## Progress: 51 / 100 
## Progress: 52 / 100 
## Progress: 53 / 100 
## Progress: 54 / 100 
## Progress: 55 / 100 
## Progress: 56 / 100 
## Progress: 57 / 100 
## Progress: 58 / 100 
## Progress: 59 / 100 
## Progress: 60 / 100 
## Progress: 61 / 100 
## Progress: 62 / 100 
## Progress: 63 / 100 
## Progress: 64 / 100 
## Progress: 65 / 100 
## Progress: 66 / 100 
## Progress: 67 / 100 
## Progress: 68 / 100 
## Progress: 69 / 100 
## Progress: 70 / 100 
## Progress: 71 / 100 
## Progress: 72 / 100 
## Progress: 73 / 100 
## Progress: 74 / 100 
## Progress: 75 / 100 
## Progress: 76 / 100 
## Progress: 77 / 100 
## Progress: 78 / 100 
## Progress: 79 / 100 
## Progress: 80 / 100 
## Progress: 81 / 100 
## Progress: 82 / 100 
## Progress: 83 / 100 
## Progress: 84 / 100 
## Progress: 85 / 100 
## Progress: 86 / 100 
## Progress: 87 / 100 
## Progress: 88 / 100 
## Progress: 89 / 100 
## Progress: 90 / 100 
## Progress: 91 / 100 
## Progress: 92 / 100 
## Progress: 93 / 100 
## Progress: 94 / 100 
## Progress: 95 / 100 
## Progress: 96 / 100 
## Progress: 97 / 100 
## Progress: 98 / 100 
## Progress: 99 / 100 
## Progress: 100 / 100
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;act.pwr.n &amp;lt;- getPower(act.n, alpha = 0.05)
findPower(act.pwr.n, iv=&amp;quot;N&amp;quot;, power=0.8)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## y~x1 y~x2 y~x3 y~x4 y~~y 
##  801  144  180   16  Inf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next is simulating data from factor models and structural equation models.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why I Write</title>
      <link>/post/why-i-write/</link>
      <pubDate>Wed, 06 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/why-i-write/</guid>
      <description>

&lt;p&gt;Recently, I have begun writing more and more.
After a beer (or a few more than a beer) I was wondering how I ended up writing so much.
Until I entered my current position as a PhD student I never thought writing would be a major aspect of my work.
By work, I mean that I am researcher.
My training is in educational measurement and advanced statistical methodology.
A lot of what I consider my research is banging my head against a wall to get the darn computer software to quit returning errors during an analysis.
So, the prospect of being a writer didn&amp;rsquo;t occur to me at first.&lt;/p&gt;

&lt;p&gt;But, then I realized something important. &lt;em&gt;I want a job.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Yes, my motivation for getting my PhD is to get a job.
To get a job, I need to communicate my research effectively enough to get publications.
Describing complex statistical procedures in any form of plain English is anything but simple when first starting.
One of my goals is to be able to write in a simple way for others to understand.
The issue I find is that writing about structural equation modeling, multilevel modeling, and other complex form of analysis is really easy to be overly complex.
For example, when describing multilevel confirmatory factor analysis (a narely mouthful even to say) is easy to be complex and use sophistocated language that masks my lack of fully understanding the subject matter.
But, this is not useful to anyone.
Especially not for myself.&lt;/p&gt;

&lt;p&gt;This brings me to &lt;em&gt;why I write.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I write to work on my skills with written English.
I find being able to put words in order a skill that will help me no matter what job I end up doing once I graduate.
Writing, just like any skill, take me time to learn.
The more time I spend, the more I will I get out of the prusuit.&lt;/p&gt;

&lt;h3 id=&#34;my-difficulties-with-writing&#34;&gt;My Difficulties with Writing&lt;/h3&gt;

&lt;p&gt;All that information above feels good and all.
But, what I want to work with mostly is being able to explain complex statistical methods in plain language.
In order for me to get to the point where I am more confident in my communication skills I am practicing more.
My major difficulty with writing in the land of advance methods is translating formulas to English.&lt;/p&gt;

&lt;p&gt;One of the topics I am actively researching is mixture modeling.
Mixture modeling is a way of classifying cases into like groups based on observed characteristics.
The methods for mixture modeling are extremely varied.
One of the simplest mixture models is a mixture of univariate normal distributions.
That is also a mouthful that doesn&amp;rsquo;t make much sense.&lt;/p&gt;

&lt;p&gt;So, to backtrack to explain mixture modeling in the most general sense so that I can bring us back to the &amp;ldquo;simple case&amp;rdquo; of univariate mixtures.
We can think of mixture models as a form of classification.
Classification systems are found all around us.
Libraries are prime examples of classification systems.
I&amp;rsquo;m a graduate student and I use the library so much.
Yes, most research articles are online, but I am still using books for a lot of my information.
Many of the best introductions to advanced methods are in textbooks now and my library contains many of the seminal texts.
Without some way of organizing the massive number of books within a library I could never find the right books.
Books could be classified in any number of ways.
One way could be author name.
Another could be date published.
Then one could be by subject matter.
Each of these different methods for classifying books yields a different organization (a different classification system).
The classification system employed by libraries is analogous to the process that mixture modeling aims to mimic.&lt;/p&gt;

&lt;p&gt;Despite mixture modeling being a method of classification, there is amuchmore general aspect to mixture modeling compared to library organizational systems.
In mixture modeling, the classification is unknown at the start.
Based on the observed data, we tried to create the system for classifying people into like groups.
The system may not be the most optimal, but provide a set of rules that can be used to create classes of individuals based on observable characteristics.&lt;/p&gt;

&lt;h2 id=&#34;beyond-developing-skills&#34;&gt;Beyond Developing Skills&lt;/h2&gt;

&lt;p&gt;Being a skilled communicator takes practice, at least from my experience.
The process of developing skills is by no means an easy path.
This is why I write, to create meaningful opportunities for me to communicate topics that I am interested in.
In the posts and tutorials to come I will discuss topics such as&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Research collaborations,&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Working towards being part of a scholarly community,&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Introduction to matrix algebra,&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Simple linear regression through matrix operations, and&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Being stuck with convergence B.S. in mixture modeling.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I am not sure in what order these different topics will come above. But these will come in time.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gut microbiome meta-analysis reveals dysbiosis is independent of body mass index in predicting risk of obesityassociated CRC</title>
      <link>/publication/crc-meta/</link>
      <pubDate>Thu, 28 Feb 2019 00:00:00 -0600</pubDate>
      
      <guid>/publication/crc-meta/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Mental health best practices in NCAA: The bidirectional relationship between mental toughness and self-compassion</title>
      <link>/publication/ncaa-mental-health/</link>
      <pubDate>Thu, 28 Feb 2019 00:00:00 -0600</pubDate>
      
      <guid>/publication/ncaa-mental-health/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Intro to Mixture Modeling</title>
      <link>/tutorial/example/</link>
      <pubDate>Wed, 16 Jan 2019 00:00:00 -0600</pubDate>
      
      <guid>/tutorial/example/</guid>
      <description>&lt;p&gt;In this tutorial, I&amp;rsquo;ll start to discuss the topic of mixture modeling.
Mixture modeling is a generalization of many statistical techniques that you may be familiar with&amp;hellip;
I won&amp;rsquo;t dive into the math behind many of the ideas of mixture modeling&lt;/p&gt;

&lt;p&gt;Note: this is a plain markdown file and not an Rmarkdown (Rmd) file.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to R</title>
      <link>/teaching/2019-01-16-intro-to-r/</link>
      <pubDate>Tue, 15 Jan 2019 21:13:14 -0500</pubDate>
      
      <guid>/teaching/2019-01-16-intro-to-r/</guid>
      <description>


&lt;p&gt;Before we get started with an introduction to simulation, we need to make sure that R and R Studio are downloaded on your computers.
R is an open source and free statistical programming language.
R utilizes user built ‘packages.’
The package library is huge and you can find a package that does nearly anything you need it to and a large number of these are regularly updated.
We will talk about packages a lot over the never few years so this is kind of tertiary.&lt;/p&gt;
&lt;p&gt;Anyways, to install R follow these steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Go to &lt;a href=&#34;https://www.r-project.org/&#34; class=&#34;uri&#34;&gt;https://www.r-project.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Click on ‘to download R’&lt;/li&gt;
&lt;li&gt;Choose a ‘CRAN mirror’, a CRAN mirror is where your computer knows to look on the internet to download the software. I suggest &lt;a href=&#34;https://cran.revolutionanalytics.com/&#34; class=&#34;uri&#34;&gt;https://cran.revolutionanalytics.com/&lt;/a&gt; which is based out of Dallas.&lt;/li&gt;
&lt;li&gt;Select the download that is approriate for your computer.&lt;/li&gt;
&lt;li&gt;Then scroll down to the download: R-3.5.1.pkg (mac) or install for the first time (windows).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These steps will help you install R, next we need to download R Studio.
R Studio is an interface to use R that is much more friendly and easier to work with.
You will see this when you open both.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Go to &lt;a href=&#34;https://www.rstudio.com/products/rstudio/download/&#34; class=&#34;uri&#34;&gt;https://www.rstudio.com/products/rstudio/download/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Scroll down to ‘Installers for Supported Platforms’&lt;/li&gt;
&lt;li&gt;Select either the windows or mac version&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now that we have R and R studio downloaed, let’s dive into an Intro to Simulation.&lt;/p&gt;
&lt;div id=&#34;intro-to-the-world-of-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Intro to the World of R&lt;/h2&gt;
&lt;p&gt;R is a statistical programming language.
Because R is a programming language there is a feel of technicality involved with using the software.
One of the major difficulties I have found with helping others get started with R is getting over the initial struggle.
The initial struggle is &lt;em&gt;OKAY&lt;/em&gt;.
I use R nearly everyday and I am constantly googling how to solve errors and complete my tasks.
Basically, I want to convey that it’s perfectly normal to constantly get error messages and then ask the world of google how to fix your error.&lt;/p&gt;
&lt;p&gt;So, for starters, R can be broken down to being a fancy calculator.
For example, below is a simple case for using R as simple calculator.
We want to find the sum of “1 + 1”.
R will automatically return the result of 2.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1 + 1
## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, this is a little boring.
One of the major advantages of using a programming language is being able to store different operations into what is called an object.
An object within R is simply a letter or word that represent another symbol or value.
For example, we can store the summation of “1 + 1” into a single letter “x”.
R does this assignment of values to words or letters through two different mechanisms.
You can use “=” or “&amp;lt;-” to make R assign values to objects/letters.
However, within R it is customary to only use “&amp;lt;-” for basic assignment while the “=” is reserved for arguments within functions (I will get to this in a moment).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Assign the sum of 1 + 1 to x
x &amp;lt;- 1 + 1
## Simply place &amp;quot;x&amp;quot; to see the value stored
x 
## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;intro-to-simple-functions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Intro to Simple Functions&lt;/h3&gt;
&lt;p&gt;R is a functional programming language, which means that R is designed with tools that take in values to return a result.
Now, this is super vague and not explicit enough to make sense.
One of the simplest functions in R is the “print()” function.
The print function takes in a single argument “x”.
This means that based on the value supplied to the function print with the argument x, R will display the actual value(s) stored in x.
For example, suppose we store the phrase “Hello People” in the object “y”.
We can use the print() function to see what the value of “y” is when supplied to the functin.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Store &amp;quot;Hello People&amp;quot; in y
y &amp;lt;- &amp;quot;Hello People&amp;quot;
## Displaythe value of y
print(x=y) ## x is theargument of print, and we want to print y
## [1] &amp;quot;Hello People&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This concept of storing values into objects to use in functions is &lt;strong&gt;extremely&lt;/strong&gt; important.
We can utilize this idea in any number of ways we can imagine.
The flexibility to choose how we use the simple rules of R provides us with a basis for manipulating data and doing more complex statistical analyses.
But, for now I will talk a little bit about how R can be used to generate values randomly.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to R and Simulating Data</title>
      <link>/tutorial/2018-12-10-r-simple-sim.rmarkdown/</link>
      <pubDate>Tue, 15 Jan 2019 21:13:14 -0500</pubDate>
      
      <guid>/tutorial/2018-12-10-r-simple-sim.rmarkdown/</guid>
      <description>


&lt;p&gt;Before we get started with an introduction to simulation, we need to make sure that R and R Studio are downloaded on your computers.
R is an open source and free statistical programming language.
R utilizes user built ‘packages.’
The package library is huge and you can find a package that does nearly anything you need it to and a large number of these are regularly updated.
We will talk about packages a lot over the never few years so this is kind of tertiary.&lt;/p&gt;
&lt;p&gt;Anyways, to install R follow these steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Go to &lt;a href=&#34;https://www.r-project.org/&#34; class=&#34;uri&#34;&gt;https://www.r-project.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Click on ‘to download R’&lt;/li&gt;
&lt;li&gt;Choose a ‘CRAN mirror’, a CRAN mirror is where your computer knows to look on the internet to download the software. I suggest &lt;a href=&#34;https://cran.revolutionanalytics.com/&#34; class=&#34;uri&#34;&gt;https://cran.revolutionanalytics.com/&lt;/a&gt; which is based out of Dallas.&lt;/li&gt;
&lt;li&gt;Select the download that is approriate for your computer.&lt;/li&gt;
&lt;li&gt;Then scroll down to the download: R-3.5.1.pkg (mac) or install for the first time (windows).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These steps will help you install R, next we need to download R Studio.
R Studio is an interface to use R that is much more friendly and easier to work with.
You will see this when you open both.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Go to &lt;a href=&#34;https://www.rstudio.com/products/rstudio/download/&#34; class=&#34;uri&#34;&gt;https://www.rstudio.com/products/rstudio/download/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Scroll down to ‘Installers for Supported Platforms’&lt;/li&gt;
&lt;li&gt;Select either the windows or mac version&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now that we have R and R studio downloaed, let’s dive into an Intro to Simulation.&lt;/p&gt;
&lt;div id=&#34;intro-to-the-world-of-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Intro to the World of R&lt;/h2&gt;
&lt;p&gt;R is a statistical programming language.
Because R is a programming language there is a feel of technicality involved with using the software.
One of the major difficulties I have found with helping others get started with R is getting over the initial struggle.
The initial struggle is &lt;em&gt;OKAY&lt;/em&gt;.
I use R nearly everyday and I am constantly googling how to solve errors and complete my tasks.
Basically, I want to convey that it’s perfectly normal to constantly get error messages and then ask the world of google how to fix your error.&lt;/p&gt;
&lt;p&gt;So, for starters, R can be broken down to being a fancy calculator.
For example, below is a simple case for using R as simple calculator.
We want to find the sum of “1 + 1”.
R will automatically return the result of 2.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1 + 1
## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, this is a little boring.
One of the major advantages of using a programming language is being able to store different operations into what is called an object.
An object within R is simply a letter or word that represent another symbol or value.
For example, we can store the summation of “1 + 1” into a single letter “x”.
R does this assignment of values to words or letters through two different mechanisms.
You can use “=” or “&amp;lt;-” to make R assign values to objects/letters.
However, within R it is customary to only use “&amp;lt;-” for basic assignment while the “=” is reserved for arguments within functions (I will get to this in a moment).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Assign the sum of 1 + 1 to x
x &amp;lt;- 1 + 1
## Simply place &amp;quot;x&amp;quot; to see the value stored
x 
## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;intro-to-simple-functions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Intro to Simple Functions&lt;/h3&gt;
&lt;p&gt;R is a functional programming language, which means that R is designed with tools that take in values to return a result.
Now, this is super vague and not explicit enough to make sense.
One of the simplest functions in R is the “print()” function.
The print function takes in a single argument “x”.
This means that based on the value supplied to the function print with the argument x, R will display the actual value(s) stored in x.
For example, suppose we store the phrase “Hello People” in the object “y”.
We can use the print() function to see what the value of “y” is when supplied to the functin.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Store &amp;quot;Hello People&amp;quot; in y
y &amp;lt;- &amp;quot;Hello People&amp;quot;
## Displaythe value of y
print(x=y) ## x is theargument of print, and we want to print y
## [1] &amp;quot;Hello People&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This concept of storing values into objects to use in functions is &lt;strong&gt;extremely&lt;/strong&gt; important.
We can utilize this idea in any number of ways we can imagine.
The flexibility to choose how we use the simple rules of R provides us with a basis for manipulating data and doing more complex statistical analyses.
But, for now I will talk a little bit about how R can be used to generate values randomly.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;brief-introduction-to-probability&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;em&gt;Brief&lt;/em&gt; Introduction to Probability&lt;/h2&gt;
&lt;p&gt;Yes, we are starting with probability.
Before we can talk about simulating random data, we need to talk briefly about probability.&lt;/p&gt;
&lt;p&gt;Let’s say we have a quarter.
A quarter has two sides, HEADS and TAILS.
These two outcomes are possibly if we flip a coin once.
A coin is called ‘fair’ if both sides have the same likelihood of occuring.
This means that over repeated flips of our coin, we would expect that each outcome (H vs. T) to occur approximately the same number of times.
Therefore, if a coin is fair, each outcome H and T will occur 50% of the time.
In other words, each outcome has a 0.5 probability of occuring.&lt;/p&gt;
&lt;p&gt;This is definition note: a probability is strictly between 0 and 1. If a probability if said to be outside this range, then the value cannot be a probability.&lt;/p&gt;
&lt;p&gt;Probability is based on taking a &lt;em&gt;sample&lt;/em&gt; from a &lt;em&gt;population&lt;/em&gt;.
A population is what is often of interest, but we rarely have access to the entire population.
The US census is an attempt to get information from the entire US population.
Because of cost and time, we take sufficiently large samples from a population to gain insight into the population.
For example, to empirically test whether a coin is fair, we need to flip a coin many times generate the number of the times each outcomes occurs.
If we flip a coin 1000 times and 500 of these turned out to fair, we likely have good evidence that our coin is fair.
In contrast, if we flipped a coin 3 times and all 3 turned out to heads then the question becomes whether or not we ahve enough evidence to conclude that the coin is &lt;em&gt;not&lt;/em&gt; fair.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulating-a-datum&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulating a Datum&lt;/h2&gt;
&lt;p&gt;In R, we can generate a single sample from a population very easily.
Take the example of a coin flip.
We can specify a population with two outcomes Heads and Tails with each outcome begin equally likely.
From this population, we then take a single draw to obtain a datum.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# Specify the possible outcomes. We do this by telling R to make an object with two values inside. This is done by using the command  c(.). Values within the () are stuck together
population &amp;lt;- c(&amp;#39;HEADS&amp;#39;, &amp;#39;TAILS&amp;#39;)

# Specify the Probability of each outcome in the population. Each value in the c(.) must align with the corresponding outcome in the population object. 
outcomeProb &amp;lt;- c(.5, .5)

# To generate the random draw, we use the following:
# x = defined population
# size = number of sample 
# replace = TRUE/FALSE? Should sampling be done with replacement?
sample(x=population, size=1, prob = outcomeProb, replace = T)
## [1] &amp;quot;TAILS&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;simulating-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulating Data&lt;/h2&gt;
&lt;p&gt;Now, lets simulate more than one datum. We will now simulate 10 coin flips:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# To do so, we simply adjust the size to 10 instead of 1
sample(x=population, size=10, prob = outcomeProb, replace = T)
##  [1] &amp;quot;HEADS&amp;quot; &amp;quot;HEADS&amp;quot; &amp;quot;TAILS&amp;quot; &amp;quot;TAILS&amp;quot; &amp;quot;HEADS&amp;quot; &amp;quot;HEADS&amp;quot; &amp;quot;HEADS&amp;quot; &amp;quot;TAILS&amp;quot;
##  [9] &amp;quot;TAILS&amp;quot; &amp;quot;HEADS&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Reporting Growth Mixture Models</title>
      <link>/post/2018-12-17-gmm-reporting/</link>
      <pubDate>Mon, 17 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-12-17-gmm-reporting/</guid>
      <description>

&lt;p&gt;I am currently wrapping up my work on a project investigating the alcoholic tendencies of adolesences.
The data come from a data study in the United Kingdom, and I was brought on the project to help with the analysis.
For this study, the aim was to investigate the presence of unobserved heterogeneity on the growth trajectories of this construct of &amp;lsquo;alcoholic tendencies.&amp;rsquo;
For this analysis, I implemented a Growth Mixture Model (GMM).
A GMM is a very complex model that allows for the greatest flexibility in terms of statistical modeling.
The aim is to uncover the presence of unobserved groups of people with qualitatively distinct growth patterns.
The term growth may be a little misleading as the pattern may be negative or no change may occur.
I will not dive into the complexities of the modeling of GMMs in this post.
The aim of this post is to show what I have found to be the most useful in terms of reporting GMMs from my readings.&lt;/p&gt;

&lt;p&gt;The source for these guidelines come from textbooks on GMMs such Grimm, Ram and Estabrook (2017) and the great article van de Schoot et al. (2017).&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Descriptive statistics of observed variables. For dichotomous items, report the proportion of 1’s or proportion of high responses.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Measurement model, meaning how the four items measured over time relate to the construct of alcoholic tendencies, why a high value on this trait is bad because of a high value on each of these items.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Measurement invariance – this is where one of the first major limitations, statistical speaking, crops up. Measurement invariance cannot be completely investigated with dichotomous indicators. Below is a brief write-up I started on this topic a few months ago. This should get you started and includes some key references on this topic of measurement invariance with categorical variables. Also, the Grimm et al. (2017) book has some good sections on testing measurement invariance in Mplus (see pgs. 347-350, 381-389).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Describe the idea of linear growth on this latent trait. Where the growth model aims to describe the growth trajectory of the latent variable across the measurement occasions. We only investigated a linear growth curve. Now another limitation: the latent variable may have a non-linear growth trajectory. A nonlinear growth trajectory could be disguised as the presence of a latent class or this could hide the presence of latent classes do to an improperly specified growth pattern.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Methods of mixture modeling – idea that more than one latent class underlies these data. This is one of the main points that needs to be described as we are doing a growth mixture model (GMM). The GMM aims to described qualitatively different trajectories in growth on the latent trait over time. This is also where to describe how in our model, we are allowing for class specific intercepts and slopes in the growth model. Meaning that the classes on average have unique average growth trajectories and unique starting points on the latent trait.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Model selection – along with model summary across class enumerations. We will have estimated the class 1 through class 5 GMMs. We will need to report the log-likelihood, number of parameters, AIC, BIC, ssBIC, LMR-LRT. Along with information on which models had potential convergence issues. Although, hopefully there won’t be much convergence issues once we use more starts…. Anyways, that leads me into a important limitation: estimation of GMM, and mixture models in general, are notorious for being estimated at a local maximum on the likelihood surface and thus we may not have the true optimal solution for this model. We should note how the scale of the latent variable is also arbitrary and therefore the meanstructure was established based on the metric of the observed data and then rescaled to an arbitrary metric for presentation.  This will help us to describe which model is best fitting when looking at profile plots:&lt;/p&gt;

&lt;p&gt;a.  Profile plots will include two types. 1) class average trajectories and 2) class profiles for all individuals by panels of the “raw” factor scores across time. I wrote an R script for you that does this based on two pieces of information the file name and number of classes. This only works for converged models where factor scores were reported.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Model results to report of final model:&lt;/p&gt;

&lt;p&gt;a.  Class sizes&lt;/p&gt;

&lt;p&gt;b.  Entropy&lt;/p&gt;

&lt;p&gt;c.  Plot of the factor scores across time by each class&lt;/p&gt;

&lt;p&gt;d.  Class intercepts and slopes with associated standard errors (SE)&lt;/p&gt;

&lt;p&gt;e.  Effect of interventions across each class and associated SE&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Rens van de Schoot, Marit Sijbrandij, Sonja D. Winter, Sarah Depaoli &amp;amp; Jeroen K. Vermunt (2017). The GRoLTS-Checklist: Guidelines for Reporting on Latent Trajectory Studies, Structural Equation Modeling: A Multidisciplinary Journal, 24:3, 451-467, DOI: 10.1080 /10705511.2016.1247646&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Kevin J. Grimm, Nilam Ram &amp;amp; Ryne Estabrook (2017). Growth modeling: Structural equation and multilevel modeling approaches. New York, NY. Guilford Press.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Starting up Beyond-STAT</title>
      <link>/post/2018-12-07-hello/</link>
      <pubDate>Fri, 07 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-12-07-hello/</guid>
      <description>&lt;p&gt;Hello people (or non-people, I aim to be inclusive)!&lt;/p&gt;

&lt;p&gt;My name is Noah, and this is the start of my website and tutorial center!
I&amp;rsquo;ve been pondering why I should create a wedbsite and the ebst reason I ahve come up with is to have a single place to store many of my ideas and struggles (with R in particular).
The name I came up with Beyond-STAT is mean to sum up my work.
STAT us short for statistics and a plat on words at the same time.
My sister is an ICU nurse and &amp;lsquo;stat&amp;rsquo; is an emergency call (not actually I have discovered, but it is a fun play on words I can take from TV land).
My purpose for this play on words is to describe how I feel like statistics is used as an emergency to bring life back to many datasets.
My career feels like a similar vein at times, but I strive to focus on more than analyzing data to find a result (i.e, I strive to go &amp;lsquo;beyond&amp;rsquo; statitics).
This focus stems from my drive to help other answer complex questions with data in a rigorous manner.
Part of my training has focused on educational measurement and how to rigorously measure complex constructs.
In this manner, the statistical models employed aim to help explain the process by which data were generated.&lt;/p&gt;

&lt;p&gt;Describing processes is one of the major benefits to understanding statistics which I aim to do throguh my work.
This website contains some of my ideas on statistical modeling and measurement models.
Also, this where many of my trial-and-error attempts are also given and maybe they will be helpful to someone.&lt;/p&gt;

&lt;p&gt;Thank you for reading and I hope you find something interesting in my ramblings!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Interaction between the microbiome and TP53 in human lung cancer</title>
      <link>/publication/genome-bio-lung-cancer/</link>
      <pubDate>Wed, 01 Aug 2018 00:00:00 -0500</pubDate>
      
      <guid>/publication/genome-bio-lung-cancer/</guid>
      <description>&lt;p&gt;In this project, I contributed to the statistical analyses and making of figures.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 -0500</pubDate>
      
      <guid>/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
