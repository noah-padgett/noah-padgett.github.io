[{"authors":null,"categories":null,"content":"Each course that I have taught (or co-taught) will appear as a separate level. Each course will have numerous pages eventually as I grow in my catelog of courses. What I hope is for others to be able to use the pages and tutorials specific to each course as a guided learning throughout their learning process.\n","date":1547618400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1547618400,"objectID":"6b0512eeb5f23dcb50d5dbe994643b40","permalink":"/teaching/","publishdate":"2019-01-16T00:00:00-06:00","relpermalink":"/teaching/","section":"teaching","summary":"Each course that I have taught (or co-taught) will appear as a separate level. Each course will have numerous pages eventually as I grow in my catelog of courses. What I hope is for others to be able to use the pages and tutorials specific to each course as a guided learning throughout their learning process.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this folder, I my different blog posts can be accessed on the left.\n","date":1544248800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1544248800,"objectID":"6370009740e1eb2ee0b7974f10ecc91f","permalink":"/post/","publishdate":"2018-12-08T00:00:00-06:00","relpermalink":"/post/","section":"post","summary":"In this folder, I my different blog posts can be accessed on the left.","tags":null,"title":"Posts","type":"docs"},{"authors":null,"categories":null,"content":"In this folder, I am breaking up tutorials into different levels. The varying levels correspond to my conceptualization to how much knowledge may be needed to gain the most from the material presented. Some of the higher level posts will cover topics such as finite mixture models, SEM, and handling complex sample data in analyses. In level-1, I will lay the foundational knowledge I think has helped me with being able to understand these advanced topics.\nThe topics that I plan to discuss (and not in any particular order per se) are\n matrix algebra, simple linear regression, linear regression with categorical indicators, connection between t-test and regression, and conceptualizing measurement error and factor analysis.  ","date":1536469200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536469200,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"/tutorial/","publishdate":"2018-09-09T00:00:00-05:00","relpermalink":"/tutorial/","section":"tutorial","summary":"In this folder, I am breaking up tutorials into different levels. The varying levels correspond to my conceptualization to how much knowledge may be needed to gain the most from the material presented. Some of the higher level posts will cover topics such as finite mixture models, SEM, and handling complex sample data in analyses. In level-1, I will lay the foundational knowledge I think has helped me with being able to understand these advanced topics.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":["R","lavaan","tutorial","CFA","reproduce","factor-analysis","code"],"content":"\rPurpose:\nTo reproduce the maximum likelihood estimates from lavaan.\rTo try, I will be using the HolzingerSwineford1939 dataset that comes with lavaan.\rThese data form a three factor model with 9 items.\nlavaan Results\rset.seed(2)\rlibrary(lavaan)\r## Warning: package \u0026#39;lavaan\u0026#39; was built under R version 3.6.1\r## This is lavaan 0.6-5\r## lavaan is BETA software! Please report any bugs.\rHS.model \u0026lt;- \u0026#39; visual =~ x1 + x2 + x3\rtextual =~ x4 + x5 + x6\rspeed =~ x7 + x8 + x9 \u0026#39;\rfit.la \u0026lt;- cfa(HS.model, data = HolzingerSwineford1939)\rsummary(fit.la, fit.measures = TRUE)\r## lavaan 0.6-5 ended normally after 35 iterations\r## ## Estimator ML\r## Optimization method NLMINB\r## Number of free parameters 21\r## ## Number of observations 301\r## ## Model Test User Model:\r## ## Test statistic 85.306\r## Degrees of freedom 24\r## P-value (Chi-square) 0.000\r## ## Model Test Baseline Model:\r## ## Test statistic 918.852\r## Degrees of freedom 36\r## P-value 0.000\r## ## User Model versus Baseline Model:\r## ## Comparative Fit Index (CFI) 0.931\r## Tucker-Lewis Index (TLI) 0.896\r## ## Loglikelihood and Information Criteria:\r## ## Loglikelihood user model (H0) -3737.745\r## Loglikelihood unrestricted model (H1) -3695.092\r## ## Akaike (AIC) 7517.490\r## Bayesian (BIC) 7595.339\r## Sample-size adjusted Bayesian (BIC) 7528.739\r## ## Root Mean Square Error of Approximation:\r## ## RMSEA 0.092\r## 90 Percent confidence interval - lower 0.071\r## 90 Percent confidence interval - upper 0.114\r## P-value RMSEA \u0026lt;= 0.05 0.001\r## ## Standardized Root Mean Square Residual:\r## ## SRMR 0.065\r## ## Parameter Estimates:\r## ## Information Expected\r## Information saturated (h1) model Structured\r## Standard errors Standard\r## ## Latent Variables:\r## Estimate Std.Err z-value P(\u0026gt;|z|)\r## visual =~ ## x1 1.000 ## x2 0.554 0.100 5.554 0.000\r## x3 0.729 0.109 6.685 0.000\r## textual =~ ## x4 1.000 ## x5 1.113 0.065 17.014 0.000\r## x6 0.926 0.055 16.703 0.000\r## speed =~ ## x7 1.000 ## x8 1.180 0.165 7.152 0.000\r## x9 1.082 0.151 7.155 0.000\r## ## Covariances:\r## Estimate Std.Err z-value P(\u0026gt;|z|)\r## visual ~~ ## textual 0.408 0.074 5.552 0.000\r## speed 0.262 0.056 4.660 0.000\r## textual ~~ ## speed 0.173 0.049 3.518 0.000\r## ## Variances:\r## Estimate Std.Err z-value P(\u0026gt;|z|)\r## .x1 0.549 0.114 4.833 0.000\r## .x2 1.134 0.102 11.146 0.000\r## .x3 0.844 0.091 9.317 0.000\r## .x4 0.371 0.048 7.779 0.000\r## .x5 0.446 0.058 7.642 0.000\r## .x6 0.356 0.043 8.277 0.000\r## .x7 0.799 0.081 9.823 0.000\r## .x8 0.488 0.074 6.573 0.000\r## .x9 0.566 0.071 8.003 0.000\r## visual 0.809 0.145 5.564 0.000\r## textual 0.979 0.112 8.737 0.000\r## speed 0.384 0.086 4.451 0.000\r# Fit value\rfit.la@optim$fx\r## [1] 0.1417035\r\rReproducing lavaan\rThere are four major sections that I will go through\nthe fit function set-up\rthe data and model prep\rthe optimization function and starting value set-up\restimating the model\r\r1. Reproduced Fit Function\rWhenever I have read about CFA/SEM, I have been given this kind of vague idea on how estimation works.\rThe general idea I have read/been talk is that there’s this fit function\r\\[\rF_{ML} = \\frac{1}{2}\\left[\\log\\mid\\mathbf{\\Sigma}(\\theta)\\mid - \\log\\mid \\mathbf{S}\\mid + tr\\left(\\mathbf{S}\\mathbf{\\Sigma}^{-1}(\\theta)\\right) - p\\right]\r\\]\rwhere,\n\r\\(\\mid . \\mid\\) is the determinant of a matrix;\r\\(\\mathbf{S}\\) is the sample covariance matrix;\r\\(\\mathbf{\\Sigma}(\\theta)\\) is the model implied covariance matrix;\r\\(tr(.)\\) is the trace of a matrix;\r\\(\\mathbf{\\Sigma}^{-1}(\\theta)\\) is the inverse of the model implied covariance matrix; and\r\\(p\\) is the number of variables.\r\rA relatively straightforward implementation of this fit function is shown in the following chunk of code.\rThe majority of the cfa.fit() function is based on handling the model, data (X), and iteratively updating parameter estimates (\\(\\theta\\)).\nI first computed the sample related values.\rFor example, the number of variables \\(p\\) is simply the number of columns in the dataset.\rThe sample covariance matrix is also straightforward to get.\nSecondly, the model is unpacked from the model list argument.\rThe model argument is a list of three separate matrices for the 1) lambda (\\(\\Lambda\\))-factor loading matrix, 2) phi (\\(\\Phi\\))-factor covariance matrix, and 3) psi (\\(\\Psi\\))-error (co)variance matrix.\nThird, the individual parameter estimates (\\(\\theta\\)) are then unpacked and placed into the corresponding model components.\rThis was the trickiest part to figure out as this part needs to be dynamically related to the model.\rThe part that I still am not sure is done generally enough is the unpacking of the factor covariance matrix section.\rWell, this whole unpacking could likely be more general, but I will work on this for a later implementation.\nNext, the model implied covariance matrix is estimated.\rThe model implied covariance matrix is computed using \\(\\Sigma(\\theta) = \\Lambda\\Phi\\Lambda^{\\mathrm{T}} + \\Psi\\).\nFinally, the fit function is estimated.\ncfa.fit\u0026lt;-function(theta, X, model){\r# Compue sample statistics\rp\u0026lt;-ncol(X)\rS\u0026lt;-cov(X)\r# unpack model\rlambda \u0026lt;- model[[1]]\rphi \u0026lt;- model[[2]]\rpsi \u0026lt;- model[[3]]\r# number factor loadings\rlam.num \u0026lt;- length(which(is.na(lambda)))\rlambda[which(is.na(lambda))] \u0026lt;- theta[1:lam.num]\r# number elements in factor (co)variance matrix\rphi.num \u0026lt;- length(which(is.na(phi)))\rif(phi.num \u0026gt; 0){\rphi[which(is.na(phi))] \u0026lt;- theta[(lam.num+1):(lam.num+phi.num)]\rphi[upper.tri(phi)] \u0026lt;- phi[lower.tri(phi)]\r}\r# number elements in error (co)variance matrix\rpsi.num \u0026lt;- length(which(is.na(psi)))\rpsi[which(is.na(psi))] \u0026lt;- theta[(lam.num+phi.num+1):(lam.num+phi.num+psi.num)]\r# compute model implied (co)variance matrix\rSigma\u0026lt;-lambda%*%phi%*%(t(lambda)) + psi\r# get inverse of Simga\rsigInv \u0026lt;- solve(Sigma)\r# determinates of S \u0026amp; Sigma\rdetS \u0026lt;- det(S)\rdetSig \u0026lt;- det(Sigma)\r# compute fit function\rfit \u0026lt;- 0.5*(log(detSig) + trace(sigInv%*%S) - log(detS) - p)\r#return fit value return(fit)\r}\r\r2. Data and Model Prep\rThe data prep itself is relatively easy.\rFor this example at least, the data prep is simply extracting the 9 variables (\\(x_1-x_9\\)) from the larger dataframe.\rThe main thing is to make the 9 variables into all numeric variables.\nX \u0026lt;- HolzingerSwineford1939[, paste0(\u0026#39;x\u0026#39;,1:9)]\rThe model prep was a little mode involved.\rHere I needed to set up the specification of the model so that two conditions are met.\nI specify the model I want to, and\rThe model is identified.\r\rThe later of which is the much more technically involved feature.\rIdentification is a big topic for statistical analysis, especially in the domain of latent variable models where identification can be notoriously difficult.\rHowever, the basic idea is that we need to be able to come up with unique estimates for all parameter values.\rIn the CFA model here, the identification can be achieved by fixing the factor loading of one item to 1.\nAs you will probably see, the specification for the factor covariance matrix (\\(\\Phi\\)) is a little funky between it is left as NA along the diagonal and lower triangle, but the upper triangle is left as 0.\rI figure this specification out through trial and error for being able to get the lower triangle duplicated more easily while still keeping the number of unique elements in this matrix to be estimated correct.\rNote however, that in the cfa.fit function I turn the \\(\\Phi\\) matrix into the full appropriate matrix with the upper and lower diagonal being equal.\nnF\u0026lt;-3 # number of factor\rp\u0026lt;-ncol(X) # number of variables\r# model specification for factor loading matrix\r# Note: matrix fills column wise\rlambdaMod \u0026lt;- matrix(ncol=nF, nrow=p,\r# x1,x2, x3, x4,x5, x6, x7,x8,x9\rc(1, NA, NA, 0, 0, 0, 0, 0, 0, #f1\r0, 0, 0, 1, NA, NA, 0, 0, 0, #f2\r0, 0, 0, 0, 0, 0, 1, NA, NA)) #f3\rlambdaMod\r## [,1] [,2] [,3]\r## [1,] 1 0 0\r## [2,] NA 0 0\r## [3,] NA 0 0\r## [4,] 0 1 0\r## [5,] 0 NA 0\r## [6,] 0 NA 0\r## [7,] 0 0 1\r## [8,] 0 0 NA\r## [9,] 0 0 NA\r# factor covariance matrix (lower diagonal + diagonal)\rphiMod \u0026lt;- matrix(nrow=nF,ncol=nF)\rphiMod[upper.tri(phiMod)] \u0026lt;- 0\rphiMod\r## [,1] [,2] [,3]\r## [1,] NA 0 0\r## [2,] NA NA 0\r## [3,] NA NA NA\r# error variances\rpsiMod \u0026lt;- diag(nrow=p)\rdiag(psiMod)\u0026lt;-NA\rpsiMod\r## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\r## [1,] NA 0 0 0 0 0 0 0 0\r## [2,] 0 NA 0 0 0 0 0 0 0\r## [3,] 0 0 NA 0 0 0 0 0 0\r## [4,] 0 0 0 NA 0 0 0 0 0\r## [5,] 0 0 0 0 NA 0 0 0 0\r## [6,] 0 0 0 0 0 NA 0 0 0\r## [7,] 0 0 0 0 0 0 NA 0 0\r## [8,] 0 0 0 0 0 0 0 NA 0\r## [9,] 0 0 0 0 0 0 0 0 NA\r# store as list\rcfaModel \u0026lt;- list(lambdaMod, phiMod, psiMod)\r\r3. Optimization Function and Starting Value Set-Up\rThe starting values were also a little tricky to get figured out a first…\n# get length of each model element\rlam.num \u0026lt;- sum(is.na(c(lambdaMod))==T)\rphi.num \u0026lt;- sum(is.na(c(phiMod))==T)\rpsi.num \u0026lt;- sum(is.na(c(psiMod))==T)\rk\u0026lt;-lam.num+phi.num+psi.num\rsv\u0026lt;-numeric(k)\r# generate starting values\rsv[1:(lam.num)] \u0026lt;- rep(0.25,lam.num)\rsv[(lam.num+1):(lam.num+phi.num)]\u0026lt;- runif(phi.num, 0.05, 1)\rsv[(lam.num+phi.num+1):(lam.num+phi.num+psi.num)] \u0026lt;-runif(psi.num, 0.05, 3)\rLast thing with respect to the estimation.\rWe have to the compute the trace of a matrix.\rFor some reason, I can not find a built in R function that automatically does this…\rSo, I initialized a little function that does it for me to keep track.\n# trace function\rtrace \u0026lt;- function(A) {\rn \u0026lt;- dim(A)[1] # get dimension of matrix\rtr \u0026lt;- 0 # initialize trace value\r# Loop over the diagonal elements of the supplied matrix and add the element to tr\rfor (k in 1:n) {\rl \u0026lt;- A[k,k]\rtr \u0026lt;- tr + l\r}\rreturn(tr[[1]])\r}\r# or one could do sum(diag(A))\r\r4. Estimating the Model\rLavaan estimates models using the fit function described briefly from above along with a numerical methods for optimization.\rNow, that is vague on purpose.\rThe precise methods of numerical approximations for complex functions is a bit out of my range of knowledge at this point, but I have general enough sense of how to use the basics of these methods.\rMANY methods exists for numerically solving complex optimization tasks in high dimensions, I will use two separate R tools and show how vastly different the results can be.\nFirst, I will use the tools lavaan is built on.\rLavaan uses the nlminb.\rI am unsure why this optimization function was selected (the answer may be hidden in the depths of the lavaan help/tutorial pages).\rBut, the nice thing is that this function is pretty straightforward to use.\rThe following chunk of code utilize the function in a similar manner that lavaan does I believe.\rThe control argument supplies a list of different evaluation criteria, which I have taken from the fitted fit.la object from above.\rWhen I was testing this code out, I found that changing most of these settings had neglible impact on the results from this simple model.\nSecondly, I extracted the corresponding parameter estimated and unpacked them into the objects lambda, phi, and psi.\rThis was so that I could show the resulting model pieces.\nfit1 \u0026lt;- nlminb(sv, cfa.fit, X=X, model=cfaModel,\rcontrol=list(eval.max=20000, iter.max=10000,\rabs.tol=2.2e-15, rel.tol=1e-10,\rx.tol=1.5e-8,xf.tol=2.2e-14))\r# value of the fit function\rfit1$objective\r## [1] 0.1417035\r# unpack parameter estimates\rfit\u0026lt;-fit1\rlambda \u0026lt;- cfaModel[[1]]\rphi \u0026lt;- cfaModel[[2]]\rpsi \u0026lt;- cfaModel[[3]]\r# number factor loadings\rlam.num \u0026lt;- length(which(is.na(lambda)))\rlambda[which(is.na(lambda))] \u0026lt;- fit$par[1:lam.num]\r# number elements in factor (co)variance matrix\rphi.num \u0026lt;- length(which(is.na(phi)))\rif(phi.num \u0026gt; 0){\rphi[which(is.na(phi))] \u0026lt;- fit$par[(lam.num+1):(lam.num+phi.num)]\r}\r# number elements in error (co)variance matrix\rpsi.num \u0026lt;- length(which(is.na(psi)))\rpsi[which(is.na(psi))] \u0026lt;- fit$par[(lam.num+phi.num+1):(lam.num+phi.num+psi.num)]\rprint(round(lambda,3))\r## [,1] [,2] [,3]\r## [1,] 1.000 0.000 0.000\r## [2,] 0.554 0.000 0.000\r## [3,] 0.729 0.000 0.000\r## [4,] 0.000 1.000 0.000\r## [5,] 0.000 1.113 0.000\r## [6,] 0.000 0.926 0.000\r## [7,] 0.000 0.000 1.000\r## [8,] 0.000 0.000 1.180\r## [9,] 0.000 0.000 1.082\rprint(round(phi,3))\r## [,1] [,2] [,3]\r## [1,] 0.812 0.000 0.000\r## [2,] 0.410 0.983 0.000\r## [3,] 0.263 0.174 0.385\rprint(round(diag(psi),3))\r## [1] 0.551 1.138 0.847 0.372 0.448 0.357 0.802 0.489 0.568\rI also found an alternative optimization function in R called optim(.).\rThe really nice part about this function is that the Hessian matrix can be more easily computed.\rBut, the major downside is that optim was not used by lavaan can may yield slightly different results thatn lavaan.\rHowever, I think once the lower-bounds for parameters are placed the functions yeild identical results.\n#using optim instead of nlminb\r# (easier to get hessian matrix)\r# BUT, I need to supply lower limits for the variance parameters\rlb \u0026lt;- numeric(k)\rlb[1:(lam.num)] \u0026lt;- -Inf\r# phi\rlb.phi\u0026lt;- matrix(nrow=nF,ncol=nF)\rdiag(lb.phi)\u0026lt;- 0.001\rlb.phi[lower.tri(lb.phi)] \u0026lt;- -Inf\rlb[(lam.num+1):(lam.num+phi.num)]\u0026lt;- c(lb.phi[lower.tri(lb.phi, diag=T)])\r# phi\rlb[(lam.num+phi.num+1):(lam.num+phi.num+psi.num)] \u0026lt;- 0.001\rfit2 \u0026lt;- optim(sv, cfa.fit, X=X, model=cfaModel,\rmethod=\u0026#39;L-BFGS-B\u0026#39;, hessian = T, lower=lb,\rcontrol=list(maxit=10000))\r# value of the fit function\rfit2$value\r## [1] 0.1417035\r# unpack parameter estimates\rfit\u0026lt;-fit2\rlambda \u0026lt;- cfaModel[[1]]\rphi \u0026lt;- cfaModel[[2]]\rpsi \u0026lt;- cfaModel[[3]]\r# number factor loadings\rlam.num \u0026lt;- length(which(is.na(lambda)))\rlambda[which(is.na(lambda))] \u0026lt;- fit$par[1:lam.num]\r# number elements in factor (co)variance matrix\rphi.num \u0026lt;- length(which(is.na(phi)))\rif(phi.num \u0026gt; 0){\rphi[which(is.na(phi))] \u0026lt;- fit$par[(lam.num+1):(lam.num+phi.num)]\r}\r# number elements in error (co)variance matrix\rpsi.num \u0026lt;- length(which(is.na(psi)))\rpsi[which(is.na(psi))] \u0026lt;- fit$par[(lam.num+phi.num+1):(lam.num+phi.num+psi.num)]\rprint(lambda)\r## [,1] [,2] [,3]\r## [1,] 1.0000000 0.0000000 0.000000\r## [2,] 0.5535270 0.0000000 0.000000\r## [3,] 0.7293599 0.0000000 0.000000\r## [4,] 0.0000000 1.0000000 0.000000\r## [5,] 0.0000000 1.1130382 0.000000\r## [6,] 0.0000000 0.9261204 0.000000\r## [7,] 0.0000000 0.0000000 1.000000\r## [8,] 0.0000000 0.0000000 1.179954\r## [9,] 0.0000000 0.0000000 1.081630\rprint(phi)\r## [,1] [,2] [,3]\r## [1,] 0.8120324 0.0000000 0.000000\r## [2,] 0.4096243 0.9828315 0.000000\r## [3,] 0.2631030 0.1740852 0.384998\rprint(diag(psi))\r## [1] 0.5508865 1.1375943 0.8471442 0.3723983 0.4477594 0.3573981 0.8020822\r## [8] 0.4893649 0.5679775\rOne thing that I know is lacking in this replication is the computation of the standard errors.\rStandard errors are a much more complicated computation.\rI know they are related to the 2nd derivative of the likelihood function.\rBut I have not figured out how to set up this computation myself yet.\n\r\r","date":1572134400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572134400,"objectID":"46a5a577c41a645f20841e6fe2b5afc5","permalink":"/tutorial/2019-10-27-lavaan-cfa-reproduction/","publishdate":"2019-10-27T00:00:00Z","relpermalink":"/tutorial/2019-10-27-lavaan-cfa-reproduction/","section":"tutorial","summary":"Purpose:\nTo reproduce the maximum likelihood estimates from lavaan.\rTo try, I will be using the HolzingerSwineford1939 dataset that comes with lavaan.\rThese data form a three factor model with 9 items.\nlavaan Results\rset.seed(2)\rlibrary(lavaan)\r## Warning: package \u0026#39;lavaan\u0026#39; was built under R version 3.6.1\r## This is lavaan 0.6-5\r## lavaan is BETA software! Please report any bugs.\rHS.model \u0026lt;- \u0026#39; visual =~ x1 + x2 + x3\rtextual =~ x4 + x5 + x6\rspeed =~ x7 + x8 + x9 \u0026#39;\rfit.","tags":["R","lavaan","CFA","reproduce","tutorial","code"],"title":"Reproducing Lavaan from (almost) Scratch","type":"tutorial"},{"authors":null,"categories":["R","Stan","tutorial"],"content":"\rSo I was listing to one of my favorite podcasts (DataFramed) a few months ago and a particularly interest guest was on talking.\rThe guest was Michael Betancourt.\nReading about the REALLY cool HMC\rHMC is the shorthand for Hamiltonian Monte Carlo.\n\rTrouble Getting Started\rThe Getting Started page (‘https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started’) is extremely thorough and gives some excellent pointerr.\rThe page boils down to using the following R commands to get started\nif (file.exists(\u0026quot;.RData\u0026quot;)) file.remove(\u0026quot;.RData\u0026quot;)\rinstall.packages(\u0026quot;rstan\u0026quot;, repos = \u0026quot;https://cloud.r-project.org/\u0026quot;, dependencies = TRUE)\rlibrary(rstan)\roptions(mc.cores = parallel::detectCores())\rrstan_options(auto_write = TRUE)\rschools_dat \u0026lt;- list(J = 8, y = c(28, 8, -3, 7, -1, 1, 18, 12),\rsigma = c(15, 10, 16, 11, 9, 11, 10, 18))\rfit \u0026lt;- stan(file = \u0026#39;8schools.stan\u0026#39;, data = schools_dat)\rThe Stan file is also incredibly easy to set up by just going up to ‘Add File -\u0026gt; Stan File’.\rSuper simple…\nOr NOT…\nI kept getting the following error through back at me.\nError in compileCode(f, code, language = language, verbose = verbose) : Compilation ERROR, function(s)/method(s) not created! sh: g++: command not found make: *** [C:/PROGRA~1/R/R-3.6.0/etc/x64/Makeconf:215: file197870bf1a48.o] Error 127 In addition: Warning message: In system(cmd, intern = !verbose) : running command 'C:/PROGRA~1/R/R-3.6.0/bin/x64/R CMD SHLIB file197870bf1a48.cpp 2\u0026gt; file197870bf1a48.cpp.err.txt' had status 1 Error in sink(type = \"output\") : invalid connection\nSuper frustrating…\nAnyways, it turns out that after too much time spent Googling and reading stackexchange and GitHub issues that I found how to I was able to fix this annoying startup issue.\rThere isa file that is created when Stan is being installed that Windows somehow uses to run C++ code from R (I have no clue how this happens…) and there is just a simple setting that I needed to add in order to get Stan to work.\rHere is the link to where I found the solution (https://github.com/stan-dev/rstan/issues/633)\nHere is how I fixed the issue through R:\nUninstall rstan so that I can reinstall it properly\rAutomatically edit the .Makrvars file\rIn this file, make sure the following line is the only line contained in the file: ‘CXX14=$(BINPREF)g++ -O2 -march=native -mtune=nativee’\rInstall rstan\rStart-Up Stan\rRun Example (takes a minute)\r\r# Step 1\rremove.packages(\u0026#39;rstan\u0026#39;)\r# Steps 2-3\rdotR \u0026lt;- file.path(Sys.getenv(\u0026quot;HOME\u0026quot;), \u0026quot;.R\u0026quot;)\rif (!file.exists(dotR)) dir.create(dotR)\rM \u0026lt;- file.path(dotR, ifelse(.Platform$OS.type == \u0026quot;windows\u0026quot;, \u0026quot;Makevars.win\u0026quot;, \u0026quot;Makevars\u0026quot;))\rif (!file.exists(M)) file.create(M)\rcat(\u0026quot;CXX14=$(BINPREF)g++ -O2 -march=native -mtune=nativee\u0026quot;,\rfile = M, sep = \u0026quot;\\n\u0026quot;, append = TRUE)\r# Step 4\rif (file.exists(\u0026quot;.RData\u0026quot;)) file.remove(\u0026quot;.RData\u0026quot;)\rinstall.packages(\u0026quot;rstan\u0026quot;, repos = \u0026quot;https://cloud.r-project.org/\u0026quot;, dependencies = TRUE)\r# Step 5\rlibrary(rstan)\roptions(mc.cores = parallel::detectCores())\rrstan_options(auto_write = TRUE)\r# Step 6\rschools_dat \u0026lt;- list(J = 8, y = c(28, 8, -3, 7, -1, 1, 18, 12),\rsigma = c(15, 10, 16, 11, 9, 11, 10, 18))\rfit \u0026lt;- stan(file = \u0026#39;8schools.stan\u0026#39;, data = schools_dat)\rNote: I need to come back and edit this content…\n\r","date":1567728000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567728000,"objectID":"97dac805fe8fc06eefbc8e84c0a88f63","permalink":"/tutorial/2019-06-06-rstan-humble/","publishdate":"2019-09-06T00:00:00Z","relpermalink":"/tutorial/2019-06-06-rstan-humble/","section":"tutorial","summary":"So I was listing to one of my favorite podcasts (DataFramed) a few months ago and a particularly interest guest was on talking.\rThe guest was Michael Betancourt.\nReading about the REALLY cool HMC\rHMC is the shorthand for Hamiltonian Monte Carlo.\n\rTrouble Getting Started\rThe Getting Started page (‘https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started’) is extremely thorough and gives some excellent pointerr.\rThe page boils down to using the following R commands to get started","tags":["R","Stan","trouble","tutorial"],"title":"Stumbling with RStan","type":"tutorial"},{"authors":["R. Noah Padgett","Grant B. Morgan"],"categories":null,"content":"","date":1559970000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559970000,"objectID":"765531d0dbf2a23528b975a23c95bd24","permalink":"/publication/rasch-fit/","publishdate":"2019-06-08T00:00:00-05:00","relpermalink":"/publication/rasch-fit/","section":"publication","summary":"The purpose of this paper is to examine the sensitivity of commonly used Rasch fit measures to different distributions of error in item responses. Using Monte Carlo methods, we generated 10 different measurement error conditions within the Rasch rating scale model or partial credit model, and we recorded the estimates of INFIT MNSQ, OUTFIT MNSQ, and person separation reliability for each error distribution condition. INFIT MNSQ and OUTFIT MNSQ were not sensitive to error distributions when the distribution was the same across items. When the error distribution varies across items, INFIT MNSQ and OUTFIT MNSQ detected items with higher levels of measurement error as potentially misfitting. The Rasch person separation reliability statistic was sensitive to varying levels of measurement error, as expected. Our findings have implications for the use of fit measures in diagnosing model misfit.","tags":["INFIT MNSQ","OUTFIT MNSQ","measurement error","Rasch measurement","partial credit model"],"title":"An examination of sensitivity to measurement error of Rasch residual-based fit statistics","type":"publication"},{"authors":["Faucher, M. A.","Greathouse, K. L.","Padgett, R. N.","Hastings-Tolsma, M.","Sheikh, A.","Choudhury, A."],"categories":null,"content":"In this project, I contributed to the statistical analyses and making of figures.\n","date":1559970000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559970000,"objectID":"8acd91585c6649e6f81a536607806aa1","permalink":"/publication/microbiome-gestational-weight-pilot/","publishdate":"2019-06-08T00:00:00-05:00","relpermalink":"/publication/microbiome-gestational-weight-pilot/","section":"publication","summary":"Forthcoming","tags":null,"title":"Exploration of the vaginal and gut microbiome in African American women by body mass index, class of obesity, and gestational weight gain: A pilot study","type":"publication"},{"authors":null,"categories":["R","simulation","tutorial"],"content":" This file extends the introduction I gave to simulating data from coin flips. Now, the goal is to show how the basics extend and can easily be used for helpful parts of study design planning.\nSimulating More Complex Models We can easily extend the basic concepts of simulating from a simple population to simulating data from a more complex data structure. This is how we can simulate a simple regression model with one outcome (Y) and five predictors (x1, x2, etc.).\nlibrary(lavaan)  ## This is lavaan 0.6-3  ## lavaan is BETA software! Please report any bugs.  # what we think the relationship are hypothesized.model \u0026lt;- ' y ~ .1*x1 + .2*x2 + -.2*x3 + .5*x4 y ~~ 1*y ' ## Simulate 1 dataset pop.data \u0026lt;- simulateData(hypothesized.model, sample.nobs=1000, seed=2 ## Seed is for reproducing results ) ## Visualize scatterplot matrix plot(pop.data)  ## Estimate the model we care about fitted.model \u0026lt;- ' y ~ x1 + x2 + x3 + x4 ' fitted.out \u0026lt;- sem( fitted.model, data=pop.data,estimator=\u0026quot;ML\u0026quot;) ## Get summary statistics of results summary(fitted.out, rsquare=TRUE)  ## lavaan 0.6-3 ended normally after 10 iterations ## ## Optimization method NLMINB ## Number of free parameters 5 ## ## Number of observations 1000 ## ## Estimator ML ## Model Fit Test Statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Information Expected ## Information saturated (h1) model Structured ## Standard Errors Standard ## ## Regressions: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## y ~ ## x1 0.111 0.032 3.402 0.001 ## x2 0.225 0.031 7.196 0.000 ## x3 -0.240 0.032 -7.449 0.000 ## x4 0.542 0.034 15.841 0.000 ## ## Variances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## .y 1.036 0.046 22.361 0.000 ## ## R-Square: ## Estimate ## y 0.273  Simulation for Power Calculation library(simsem)  ## Warning: package 'simsem' was built under R version 3.5.3  ##  ## #################################################################  ## This is simsem 0.5-14  ## simsem is BETA software! Please report any bugs.  ## simsem was first developed at the University of Kansas Center for  ## Research Methods and Data Analysis, under NSF Grant 1053160.  ## #################################################################  ## ## Attaching package: 'simsem'  ## The following object is masked from 'package:lavaan': ## ## inspect  ## Simulate 100 datasets of size 100 from the population act.power \u0026lt;- sim(nRep=100, generate=hypothesized.model, model=fitted.model, n =100, lavaanfun = \u0026quot;sem\u0026quot;)  ## Progress: 1 / 100 ## Progress: 2 / 100 ## Progress: 3 / 100 ## Progress: 4 / 100 ## Progress: 5 / 100 ## Progress: 6 / 100 ## Progress: 7 / 100 ## Progress: 8 / 100 ## Progress: 9 / 100 ## Progress: 10 / 100 ## Progress: 11 / 100 ## Progress: 12 / 100 ## Progress: 13 / 100 ## Progress: 14 / 100 ## Progress: 15 / 100 ## Progress: 16 / 100 ## Progress: 17 / 100 ## Progress: 18 / 100 ## Progress: 19 / 100 ## Progress: 20 / 100 ## Progress: 21 / 100 ## Progress: 22 / 100 ## Progress: 23 / 100 ## Progress: 24 / 100 ## Progress: 25 / 100 ## Progress: 26 / 100 ## Progress: 27 / 100 ## Progress: 28 / 100 ## Progress: 29 / 100 ## Progress: 30 / 100 ## Progress: 31 / 100 ## Progress: 32 / 100 ## Progress: 33 / 100 ## Progress: 34 / 100 ## Progress: 35 / 100 ## Progress: 36 / 100 ## Progress: 37 / 100 ## Progress: 38 / 100 ## Progress: 39 / 100 ## Progress: 40 / 100 ## Progress: 41 / 100 ## Progress: 42 / 100 ## Progress: 43 / 100 ## Progress: 44 / 100 ## Progress: 45 / 100 ## Progress: 46 / 100 ## Progress: 47 / 100 ## Progress: 48 / 100 ## Progress: 49 / 100 ## Progress: 50 / 100 ## Progress: 51 / 100 ## Progress: 52 / 100 ## Progress: 53 / 100 ## Progress: 54 / 100 ## Progress: 55 / 100 ## Progress: 56 / 100 ## Progress: 57 / 100 ## Progress: 58 / 100 ## Progress: 59 / 100 ## Progress: 60 / 100 ## Progress: 61 / 100 ## Progress: 62 / 100 ## Progress: 63 / 100 ## Progress: 64 / 100 ## Progress: 65 / 100 ## Progress: 66 / 100 ## Progress: 67 / 100 ## Progress: 68 / 100 ## Progress: 69 / 100 ## Progress: 70 / 100 ## Progress: 71 / 100 ## Progress: 72 / 100 ## Progress: 73 / 100 ## Progress: 74 / 100 ## Progress: 75 / 100 ## Progress: 76 / 100 ## Progress: 77 / 100 ## Progress: 78 / 100 ## Progress: 79 / 100 ## Progress: 80 / 100 ## Progress: 81 / 100 ## Progress: 82 / 100 ## Progress: 83 / 100 ## Progress: 84 / 100 ## Progress: 85 / 100 ## Progress: 86 / 100 ## Progress: 87 / 100 ## Progress: 88 / 100 ## Progress: 89 / 100 ## Progress: 90 / 100 ## Progress: 91 / 100 ## Progress: 92 / 100 ## Progress: 93 / 100 ## Progress: 94 / 100 ## Progress: 95 / 100 ## Progress: 96 / 100 ## Progress: 97 / 100 ## Progress: 98 / 100 ## Progress: 99 / 100 ## Progress: 100 / 100  ## Extract summary information sim.parms\u0026lt;-summaryParam(act.power,alpha = 0.05,detail=TRUE) sim.parms  ## Estimate Average Estimate SD Average SE Power (Not equal 0) ## y~x1 0.1097629 0.09554286 0.10086173 0.20 ## y~x2 0.2049147 0.09947488 0.09872815 0.54 ## y~x3 -0.1977827 0.10642876 0.09910948 0.50 ## y~x4 0.4910222 0.10093184 0.09851506 1.00 ## y~~y 0.9383137 0.13987441 0.13269760 1.00 ## Std Est Std Est SD Std Ave SE Average Param Average Bias Coverage ## y~x1 0.09445416 0.08459703 0.08512030 0.1 0.009762922 0.94 ## y~x2 0.17723176 0.08270002 0.08374237 0.2 0.004914709 0.95 ## y~x3 -0.17213624 0.08947771 0.08402363 -0.2 0.002217263 0.94 ## y~x4 0.42868274 0.07843796 0.07464199 0.5 -0.008977780 0.96 ## y~~y 0.71830659 0.07635515 0.06891527 1.0 -0.061686276 0.83 ## Rel Bias Std Bias Rel SE Bias Not Cover Below Not Cover Above ## y~x1 0.09762922 0.10218369 0.055669976 0.04 0.02 ## y~x2 0.02457354 0.04940653 -0.007506752 0.03 0.02 ## y~x3 -0.01108631 0.02083331 -0.068771645 0.03 0.03 ## y~x4 -0.01795556 -0.08894894 -0.023944657 0.02 0.02 ## y~~y -0.06168628 -0.44101188 -0.051308941 0.00 0.17 ## Average CI Width SD CI Width ## y~x1 0.3953707 0.04252852 ## y~x2 0.3870072 0.03941553 ## y~x3 0.3885020 0.04763438 ## y~x4 0.3861719 0.03956116 ## y~~y 0.5201650 0.07754099  Simulation for Sample Size Determination Now, we want to calculate the power for a range of sample sizes. By calculating power across a range we are able to identify the sample size needed to reach power of .80 fora particular parameter of interest. Let\u0026rsquo;s try to answer this question: What is sample size I need if I want to detect the effect of x2 on y?\n## Simulate data for a range of sample sizes # seq(100,500,10): pick samples sizes from 100 to 500 going by 10 each time, so we will have 100, 110, 120, 130, ... , 500. # the rep(..., 100): each sample size is replicated 100 times, so we will effectively run 100 datasets with sample size 110, then 100 datasets with sample size 500, etc. act.n \u0026lt;- sim(model=fitted.model, generate=hypothesized.model, n = rep(seq(100,300,10), 100), lavaanfun = \u0026quot;sem\u0026quot;)  ## Progress: 1 / 2100 ## Progress: 2 / 2100 ## Progress: 3 / 2100 ## Progress: 4 / 2100 ## Progress: 5 / 2100 ## Progress: 6 / 2100 ## Progress: 7 / 2100 ## Progress: 8 / 2100 ## Progress: 9 / 2100 ## Progress: 10 / 2100 ## Progress: 11 / 2100 ## Progress: 12 / 2100 ## Progress: 13 / 2100 ## Progress: 14 / 2100 ## Progress: 15 / 2100 ## Progress: 16 / 2100 ## Progress: 17 / 2100 ## Progress: 18 / 2100 ## Progress: 19 / 2100 ## Progress: 20 / 2100 ## Progress: 21 / 2100 ## Progress: 22 / 2100 ## Progress: 23 / 2100 ## Progress: 24 / 2100 ## Progress: 25 / 2100 ## Progress: 26 / 2100 ## Progress: 27 / 2100 ## Progress: 28 / 2100 ## Progress: 29 / 2100 ## Progress: 30 / 2100 ## Progress: 31 / 2100 ## Progress: 32 / 2100 ## Progress: 33 / 2100 ## Progress: 34 / 2100 ## Progress: 35 / 2100 ## Progress: 36 / 2100 ## Progress: 37 / 2100 ## Progress: 38 / 2100 ## Progress: 39 / 2100 ## Progress: 40 / 2100 ## Progress: 41 / 2100 ## Progress: 42 / 2100 ## Progress: 43 / 2100 ## Progress: 44 / 2100 ## Progress: 45 / 2100 ## Progress: 46 / 2100 ## Progress: 47 / 2100 ## Progress: 48 / 2100 ## Progress: 49 / 2100 ## Progress: 50 / 2100 ## Progress: 51 / 2100 ## Progress: 52 / 2100 ## Progress: 53 / 2100 ## Progress: 54 / 2100 ## Progress: 55 / 2100 ## Progress: 56 / 2100 ## Progress: 57 / 2100 ## Progress: 58 / 2100 ## Progress: 59 / 2100 ## Progress: 60 / 2100 ## Progress: 61 / 2100 ## Progress: 62 / 2100 ## Progress: 63 / 2100 ## Progress: 64 / 2100 ## Progress: 65 / 2100 ## Progress: 66 / 2100 ## Progress: 67 / 2100 ## Progress: 68 / 2100 ## Progress: 69 / 2100 ## Progress: 70 / 2100 ## Progress: 71 / 2100 ## Progress: 72 / 2100 ## Progress: 73 / 2100 ## Progress: 74 / 2100 ## Progress: 75 / 2100 ## Progress: 76 / 2100 ## Progress: 77 / 2100 ## Progress: 78 / 2100 ## Progress: 79 / 2100 ## Progress: 80 / 2100 ## Progress: 81 / 2100 ## Progress: 82 / 2100 ## Progress: 83 / 2100 ## Progress: 84 / 2100 ## Progress: 85 / 2100 ## Progress: 86 / 2100 ## Progress: 87 / 2100 ## Progress: 88 / 2100 ## Progress: 89 / 2100 ## Progress: 90 / 2100 ## Progress: 91 / 2100 ## Progress: 92 / 2100 ## Progress: 93 / 2100 ## Progress: 94 / 2100 ## Progress: 95 / 2100 ## Progress: 96 / 2100 ## Progress: 97 / 2100 ## Progress: 98 / 2100 ## Progress: 99 / 2100 ## Progress: 100 / 2100 ## Progress: 101 / 2100 ## Progress: 102 / 2100 ## Progress: 103 / 2100 ## Progress: 104 / 2100 ## Progress: 105 / 2100 ## Progress: 106 / 2100 ## Progress: 107 / 2100 ## Progress: 108 / 2100 ## Progress: 109 / 2100 ## Progress: 110 / 2100 ## Progress: 111 / 2100 ## Progress: 112 / 2100 ## Progress: 113 / 2100 ## Progress: 114 / 2100 ## Progress: 115 / 2100 ## Progress: 116 / 2100 ## Progress: 117 / 2100 ## Progress: 118 / 2100 ## Progress: 119 / 2100 ## Progress: 120 / 2100 ## Progress: 121 / 2100 ## Progress: 122 / 2100 ## Progress: 123 / 2100 ## Progress: 124 / 2100 ## Progress: 125 / 2100 ## Progress: 126 / 2100 ## Progress: 127 / 2100 ## Progress: 128 / 2100 ## Progress: 129 / 2100 ## Progress: 130 / 2100 ## Progress: 131 / 2100 ## Progress: 132 / 2100 ## Progress: 133 / 2100 ## Progress: 134 / 2100 ## Progress: 135 / 2100 ## Progress: 136 / 2100 ## Progress: 137 / 2100 ## Progress: 138 / 2100 ## Progress: 139 / 2100 ## Progress: 140 / 2100 ## Progress: 141 / 2100 ## Progress: 142 / 2100 ## Progress: 143 / 2100 ## Progress: 144 / 2100 ## Progress: 145 / 2100 ## Progress: 146 / 2100 ## Progress: 147 / 2100 ## Progress: 148 / 2100 ## Progress: 149 / 2100 ## Progress: 150 / 2100 ## Progress: 151 / 2100 ## Progress: 152 / 2100 ## Progress: 153 / 2100 ## Progress: 154 / 2100 ## Progress: 155 / 2100 ## Progress: 156 / 2100 ## Progress: 157 / 2100 ## Progress: 158 / 2100 ## Progress: 159 / 2100 ## Progress: 160 / 2100 ## Progress: 161 / 2100 ## Progress: 162 / 2100 ## Progress: 163 / 2100 ## Progress: 164 / 2100 ## Progress: 165 / 2100 ## Progress: 166 / 2100 ## Progress: 167 / 2100 ## Progress: 168 / 2100 ## Progress: 169 / 2100 ## Progress: 170 / 2100 ## Progress: 171 / 2100 ## Progress: 172 / 2100 ## Progress: 173 / 2100 ## Progress: 174 / 2100 ## Progress: 175 / 2100 ## Progress: 176 / 2100 ## Progress: 177 / 2100 ## Progress: 178 / 2100 ## Progress: 179 / 2100 ## Progress: 180 / 2100 ## Progress: 181 / 2100 ## Progress: 182 / 2100 ## Progress: 183 / 2100 ## Progress: 184 / 2100 ## Progress: 185 / 2100 ## Progress: 186 / 2100 ## Progress: 187 / 2100 ## Progress: 188 / 2100 ## Progress: 189 / 2100 ## Progress: 190 / 2100 ## Progress: 191 / 2100 ## Progress: 192 / 2100 ## Progress: 193 / 2100 ## Progress: 194 / 2100 ## Progress: 195 / 2100 ## Progress: 196 / 2100 ## Progress: 197 / 2100 ## Progress: 198 / 2100 ## Progress: 199 / 2100 ## Progress: 200 / 2100 ## Progress: 201 / 2100 ## Progress: 202 / 2100 ## Progress: 203 / 2100 ## Progress: 204 / 2100 ## Progress: 205 / 2100 ## Progress: 206 / 2100 ## Progress: 207 / 2100 ## Progress: 208 / 2100 ## Progress: 209 / 2100 ## Progress: 210 / 2100 ## Progress: 211 / 2100 ## Progress: 212 / 2100 ## Progress: 213 / 2100 ## Progress: 214 / 2100 ## Progress: 215 / 2100 ## Progress: 216 / 2100 ## Progress: 217 / 2100 ## Progress: 218 / 2100 ## Progress: 219 / 2100 ## Progress: 220 / 2100 ## Progress: 221 / 2100 ## Progress: 222 / 2100 ## Progress: 223 / 2100 ## Progress: 224 / 2100 ## Progress: 225 / 2100 ## Progress: 226 / 2100 ## Progress: 227 / 2100 ## Progress: 228 / 2100 ## Progress: 229 / 2100 ## Progress: 230 / 2100 ## Progress: 231 / 2100 ## Progress: 232 / 2100 ## Progress: 233 / 2100 ## Progress: 234 / 2100 ## Progress: 235 / 2100 ## Progress: 236 / 2100 ## Progress: 237 / 2100 ## Progress: 238 / 2100 ## Progress: 239 / 2100 ## Progress: 240 / 2100 ## Progress: 241 / 2100 ## Progress: 242 / 2100 ## Progress: 243 / 2100 ## Progress: 244 / 2100 ## Progress: 245 / 2100 ## Progress: 246 / 2100 ## Progress: 247 / 2100 ## Progress: 248 / 2100 ## Progress: 249 / 2100 ## Progress: 250 / 2100 ## Progress: 251 / 2100 ## Progress: 252 / 2100 ## Progress: 253 / 2100 ## Progress: 254 / 2100 ## Progress: 255 / 2100 ## Progress: 256 / 2100 ## Progress: 257 / 2100 ## Progress: 258 / 2100 ## Progress: 259 / 2100 ## Progress: 260 / 2100 ## Progress: 261 / 2100 ## Progress: 262 / 2100 ## Progress: 263 / 2100 ## Progress: 264 / 2100 ## Progress: 265 / 2100 ## Progress: 266 / 2100 ## Progress: 267 / 2100 ## Progress: 268 / 2100 ## Progress: 269 / 2100 ## Progress: 270 / 2100 ## Progress: 271 / 2100 ## Progress: 272 / 2100 ## Progress: 273 / 2100 ## Progress: 274 / 2100 ## Progress: 275 / 2100 ## Progress: 276 / 2100 ## Progress: 277 / 2100 ## Progress: 278 / 2100 ## Progress: 279 / 2100 ## Progress: 280 / 2100 ## Progress: 281 / 2100 ## Progress: 282 / 2100 ## Progress: 283 / 2100 ## Progress: 284 / 2100 ## Progress: 285 / 2100 ## Progress: 286 / 2100 ## Progress: 287 / 2100 ## Progress: 288 / 2100 ## Progress: 289 / 2100 ## Progress: 290 / 2100 ## Progress: 291 / 2100 ## Progress: 292 / 2100 ## Progress: 293 / 2100 ## Progress: 294 / 2100 ## Progress: 295 / 2100 ## Progress: 296 / 2100 ## Progress: 297 / 2100 ## Progress: 298 / 2100 ## Progress: 299 / 2100 ## Progress: 300 / 2100 ## Progress: 301 / 2100 ## Progress: 302 / 2100 ## Progress: 303 / 2100 ## Progress: 304 / 2100 ## Progress: 305 / 2100 ## Progress: 306 / 2100 ## Progress: 307 / 2100 ## Progress: 308 / 2100 ## Progress: 309 / 2100 ## Progress: 310 / 2100 ## Progress: 311 / 2100 ## Progress: 312 / 2100 ## Progress: 313 / 2100 ## Progress: 314 / 2100 ## Progress: 315 / 2100 ## Progress: 316 / 2100 ## Progress: 317 / 2100 ## Progress: 318 / 2100 ## Progress: 319 / 2100 ## Progress: 320 / 2100 ## Progress: 321 / 2100 ## Progress: 322 / 2100 ## Progress: 323 / 2100 ## Progress: 324 / 2100 ## Progress: 325 / 2100 ## Progress: 326 / 2100 ## Progress: 327 / 2100 ## Progress: 328 / 2100 ## Progress: 329 / 2100 ## Progress: 330 / 2100 ## Progress: 331 / 2100 ## Progress: 332 / 2100 ## Progress: 333 / 2100 ## Progress: 334 / 2100 ## Progress: 335 / 2100 ## Progress: 336 / 2100 ## Progress: 337 / 2100 ## Progress: 338 / 2100 ## Progress: 339 / 2100 ## Progress: 340 / 2100 ## Progress: 341 / 2100 ## Progress: 342 / 2100 ## Progress: 343 / 2100 ## Progress: 344 / 2100 ## Progress: 345 / 2100 ## Progress: 346 / 2100 ## Progress: 347 / 2100 ## Progress: 348 / 2100 ## Progress: 349 / 2100 ## Progress: 350 / 2100 ## Progress: 351 / 2100 ## Progress: 352 / 2100 ## Progress: 353 / 2100 ## Progress: 354 / 2100 ## Progress: 355 / 2100 ## Progress: 356 / 2100 ## Progress: 357 / 2100 ## Progress: 358 / 2100 ## Progress: 359 / 2100 ## Progress: 360 / 2100 ## Progress: 361 / 2100 ## Progress: 362 / 2100 ## Progress: 363 / 2100 ## Progress: 364 / 2100 ## Progress: 365 / 2100 ## Progress: 366 / 2100 ## Progress: 367 / 2100 ## Progress: 368 / 2100 ## Progress: 369 / 2100 ## Progress: 370 / 2100 ## Progress: 371 / 2100 ## Progress: 372 / 2100 ## Progress: 373 / 2100 ## Progress: 374 / 2100 ## Progress: 375 / 2100 ## Progress: 376 / 2100 ## Progress: 377 / 2100 ## Progress: 378 / 2100 ## Progress: 379 / 2100 ## Progress: 380 / 2100 ## Progress: 381 / 2100 ## Progress: 382 / 2100 ## Progress: 383 / 2100 ## Progress: 384 / 2100 ## Progress: 385 / 2100 ## Progress: 386 / 2100 ## Progress: 387 / 2100 ## Progress: 388 / 2100 ## Progress: 389 / 2100 ## Progress: 390 / 2100 ## Progress: 391 / 2100 ## Progress: 392 / 2100 ## Progress: 393 / 2100 ## Progress: 394 / 2100 ## Progress: 395 / 2100 ## Progress: 396 / 2100 ## Progress: 397 / 2100 ## Progress: 398 / 2100 ## Progress: 399 / 2100 ## Progress: 400 / 2100 ## Progress: 401 / 2100 ## Progress: 402 / 2100 ## Progress: 403 / 2100 ## Progress: 404 / 2100 ## Progress: 405 / 2100 ## Progress: 406 / 2100 ## Progress: 407 / 2100 ## Progress: 408 / 2100 ## Progress: 409 / 2100 ## Progress: 410 / 2100 ## Progress: 411 / 2100 ## Progress: 412 / 2100 ## Progress: 413 / 2100 ## Progress: 414 / 2100 ## Progress: 415 / 2100 ## Progress: 416 / 2100 ## Progress: 417 / 2100 ## Progress: 418 / 2100 ## Progress: 419 / 2100 ## Progress: 420 / 2100 ## Progress: 421 / 2100 ## Progress: 422 / 2100 ## Progress: 423 / 2100 ## Progress: 424 / 2100 ## Progress: 425 / 2100 ## Progress: 426 / 2100 ## Progress: 427 / 2100 ## Progress: 428 / 2100 ## Progress: 429 / 2100 ## Progress: 430 / 2100 ## Progress: 431 / 2100 ## Progress: 432 / 2100 ## Progress: 433 / 2100 ## Progress: 434 / 2100 ## Progress: 435 / 2100 ## Progress: 436 / 2100 ## Progress: 437 / 2100 ## Progress: 438 / 2100 ## Progress: 439 / 2100 ## Progress: 440 / 2100 ## Progress: 441 / 2100 ## Progress: 442 / 2100 ## Progress: 443 / 2100 ## Progress: 444 / 2100 ## Progress: 445 / 2100 ## Progress: 446 / 2100 ## Progress: 447 / 2100 ## Progress: 448 / 2100 ## Progress: 449 / 2100 ## Progress: 450 / 2100 ## Progress: 451 / 2100 ## Progress: 452 / 2100 ## Progress: 453 / 2100 ## Progress: 454 / 2100 ## Progress: 455 / 2100 ## Progress: 456 / 2100 ## Progress: 457 / 2100 ## Progress: 458 / 2100 ## Progress: 459 / 2100 ## Progress: 460 / 2100 ## Progress: 461 / 2100 ## Progress: 462 / 2100 ## Progress: 463 / 2100 ## Progress: 464 / 2100 ## Progress: 465 / 2100 ## Progress: 466 / 2100 ## Progress: 467 / 2100 ## Progress: 468 / 2100 ## Progress: 469 / 2100 ## Progress: 470 / 2100 ## Progress: 471 / 2100 ## Progress: 472 / 2100 ## Progress: 473 / 2100 ## Progress: 474 / 2100 ## Progress: 475 / 2100 ## Progress: 476 / 2100 ## Progress: 477 / 2100 ## Progress: 478 / 2100 ## Progress: 479 / 2100 ## Progress: 480 / 2100 ## Progress: 481 / 2100 ## Progress: 482 / 2100 ## Progress: 483 / 2100 ## Progress: 484 / 2100 ## Progress: 485 / 2100 ## Progress: 486 / 2100 ## Progress: 487 / 2100 ## Progress: 488 / 2100 ## Progress: 489 / 2100 ## Progress: 490 / 2100 ## Progress: 491 / 2100 ## Progress: 492 / 2100 ## Progress: 493 / 2100 ## Progress: 494 / 2100 ## Progress: 495 / 2100 ## Progress: 496 / 2100 ## Progress: 497 / 2100 ## Progress: 498 / 2100 ## Progress: 499 / 2100 ## Progress: 500 / 2100 ## Progress: 501 / 2100 ## Progress: 502 / 2100 ## Progress: 503 / 2100 ## Progress: 504 / 2100 ## Progress: 505 / 2100 ## Progress: 506 / 2100 ## Progress: 507 / 2100 ## Progress: 508 / 2100 ## Progress: 509 / 2100 ## Progress: 510 / 2100 ## Progress: 511 / 2100 ## Progress: 512 / 2100 ## Progress: 513 / 2100 ## Progress: 514 / 2100 ## Progress: 515 / 2100 ## Progress: 516 / 2100 ## Progress: 517 / 2100 ## Progress: 518 / 2100 ## Progress: 519 / 2100 ## Progress: 520 / 2100 ## Progress: 521 / 2100 ## Progress: 522 / 2100 ## Progress: 523 / 2100 ## Progress: 524 / 2100 ## Progress: 525 / 2100 ## Progress: 526 / 2100 ## Progress: 527 / 2100 ## Progress: 528 / 2100 ## Progress: 529 / 2100 ## Progress: 530 / 2100 ## Progress: 531 / 2100 ## Progress: 532 / 2100 ## Progress: 533 / 2100 ## Progress: 534 / 2100 ## Progress: 535 / 2100 ## Progress: 536 / 2100 ## Progress: 537 / 2100 ## Progress: 538 / 2100 ## Progress: 539 / 2100 ## Progress: 540 / 2100 ## Progress: 541 / 2100 ## Progress: 542 / 2100 ## Progress: 543 / 2100 ## Progress: 544 / 2100 ## Progress: 545 / 2100 ## Progress: 546 / 2100 ## Progress: 547 / 2100 ## Progress: 548 / 2100 ## Progress: 549 / 2100 ## Progress: 550 / 2100 ## Progress: 551 / 2100 ## Progress: 552 / 2100 ## Progress: 553 / 2100 ## Progress: 554 / 2100 ## Progress: 555 / 2100 ## Progress: 556 / 2100 ## Progress: 557 / 2100 ## Progress: 558 / 2100 ## Progress: 559 / 2100 ## Progress: 560 / 2100 ## Progress: 561 / 2100 ## Progress: 562 / 2100 ## Progress: 563 / 2100 ## Progress: 564 / 2100 ## Progress: 565 / 2100 ## Progress: 566 / 2100 ## Progress: 567 / 2100 ## Progress: 568 / 2100 ## Progress: 569 / 2100 ## Progress: 570 / 2100 ## Progress: 571 / 2100 ## Progress: 572 / 2100 ## Progress: 573 / 2100 ## Progress: 574 / 2100 ## Progress: 575 / 2100 ## Progress: 576 / 2100 ## Progress: 577 / 2100 ## Progress: 578 / 2100 ## Progress: 579 / 2100 ## Progress: 580 / 2100 ## Progress: 581 / 2100 ## Progress: 582 / 2100 ## Progress: 583 / 2100 ## Progress: 584 / 2100 ## Progress: 585 / 2100 ## Progress: 586 / 2100 ## Progress: 587 / 2100 ## Progress: 588 / 2100 ## Progress: 589 / 2100 ## Progress: 590 / 2100 ## Progress: 591 / 2100 ## Progress: 592 / 2100 ## Progress: 593 / 2100 ## Progress: 594 / 2100 ## Progress: 595 / 2100 ## Progress: 596 / 2100 ## Progress: 597 / 2100 ## Progress: 598 / 2100 ## Progress: 599 / 2100 ## Progress: 600 / 2100 ## Progress: 601 / 2100 ## Progress: 602 / 2100 ## Progress: 603 / 2100 ## Progress: 604 / 2100 ## Progress: 605 / 2100 ## Progress: 606 / 2100 ## Progress: 607 / 2100 ## Progress: 608 / 2100 ## Progress: 609 / 2100 ## Progress: 610 / 2100 ## Progress: 611 / 2100 ## Progress: 612 / 2100 ## Progress: 613 / 2100 ## Progress: 614 / 2100 ## Progress: 615 / 2100 ## Progress: 616 / 2100 ## Progress: 617 / 2100 ## Progress: 618 / 2100 ## Progress: 619 / 2100 ## Progress: 620 / 2100 ## Progress: 621 / 2100 ## Progress: 622 / 2100 ## Progress: 623 / 2100 ## Progress: 624 / 2100 ## Progress: 625 / 2100 ## Progress: 626 / 2100 ## Progress: 627 / 2100 ## Progress: 628 / 2100 ## Progress: 629 / 2100 ## Progress: 630 / 2100 ## Progress: 631 / 2100 ## Progress: 632 / 2100 ## Progress: 633 / 2100 ## Progress: 634 / 2100 ## Progress: 635 / 2100 ## Progress: 636 / 2100 ## Progress: 637 / 2100 ## Progress: 638 / 2100 ## Progress: 639 / 2100 ## Progress: 640 / 2100 ## Progress: 641 / 2100 ## Progress: 642 / 2100 ## Progress: 643 / 2100 ## Progress: 644 / 2100 ## Progress: 645 / 2100 ## Progress: 646 / 2100 ## Progress: 647 / 2100 ## Progress: 648 / 2100 ## Progress: 649 / 2100 ## Progress: 650 / 2100 ## Progress: 651 / 2100 ## Progress: 652 / 2100 ## Progress: 653 / 2100 ## Progress: 654 / 2100 ## Progress: 655 / 2100 ## Progress: 656 / 2100 ## Progress: 657 / 2100 ## Progress: 658 / 2100 ## Progress: 659 / 2100 ## Progress: 660 / 2100 ## Progress: 661 / 2100 ## Progress: 662 / 2100 ## Progress: 663 / 2100 ## Progress: 664 / 2100 ## Progress: 665 / 2100 ## Progress: 666 / 2100 ## Progress: 667 / 2100 ## Progress: 668 / 2100 ## Progress: 669 / 2100 ## Progress: 670 / 2100 ## Progress: 671 / 2100 ## Progress: 672 / 2100 ## Progress: 673 / 2100 ## Progress: 674 / 2100 ## Progress: 675 / 2100 ## Progress: 676 / 2100 ## Progress: 677 / 2100 ## Progress: 678 / 2100 ## Progress: 679 / 2100 ## Progress: 680 / 2100 ## Progress: 681 / 2100 ## Progress: 682 / 2100 ## Progress: 683 / 2100 ## Progress: 684 / 2100 ## Progress: 685 / 2100 ## Progress: 686 / 2100 ## Progress: 687 / 2100 ## Progress: 688 / 2100 ## Progress: 689 / 2100 ## Progress: 690 / 2100 ## Progress: 691 / 2100 ## Progress: 692 / 2100 ## Progress: 693 / 2100 ## Progress: 694 / 2100 ## Progress: 695 / 2100 ## Progress: 696 / 2100 ## Progress: 697 / 2100 ## Progress: 698 / 2100 ## Progress: 699 / 2100 ## Progress: 700 / 2100 ## Progress: 701 / 2100 ## Progress: 702 / 2100 ## Progress: 703 / 2100 ## Progress: 704 / 2100 ## Progress: 705 / 2100 ## Progress: 706 / 2100 ## Progress: 707 / 2100 ## Progress: 708 / 2100 ## Progress: 709 / 2100 ## Progress: 710 / 2100 ## Progress: 711 / 2100 ## Progress: 712 / 2100 ## Progress: 713 / 2100 ## Progress: 714 / 2100 ## Progress: 715 / 2100 ## Progress: 716 / 2100 ## Progress: 717 / 2100 ## Progress: 718 / 2100 ## Progress: 719 / 2100 ## Progress: 720 / 2100 ## Progress: 721 / 2100 ## Progress: 722 / 2100 ## Progress: 723 / 2100 ## Progress: 724 / 2100 ## Progress: 725 / 2100 ## Progress: 726 / 2100 ## Progress: 727 / 2100 ## Progress: 728 / 2100 ## Progress: 729 / 2100 ## Progress: 730 / 2100 ## Progress: 731 / 2100 ## Progress: 732 / 2100 ## Progress: 733 / 2100 ## Progress: 734 / 2100 ## Progress: 735 / 2100 ## Progress: 736 / 2100 ## Progress: 737 / 2100 ## Progress: 738 / 2100 ## Progress: 739 / 2100 ## Progress: 740 / 2100 ## Progress: 741 / 2100 ## Progress: 742 / 2100 ## Progress: 743 / 2100 ## Progress: 744 / 2100 ## Progress: 745 / 2100 ## Progress: 746 / 2100 ## Progress: 747 / 2100 ## Progress: 748 / 2100 ## Progress: 749 / 2100 ## Progress: 750 / 2100 ## Progress: 751 / 2100 ## Progress: 752 / 2100 ## Progress: 753 / 2100 ## Progress: 754 / 2100 ## Progress: 755 / 2100 ## Progress: 756 / 2100 ## Progress: 757 / 2100 ## Progress: 758 / 2100 ## Progress: 759 / 2100 ## Progress: 760 / 2100 ## Progress: 761 / 2100 ## Progress: 762 / 2100 ## Progress: 763 / 2100 ## Progress: 764 / 2100 ## Progress: 765 / 2100 ## Progress: 766 / 2100 ## Progress: 767 / 2100 ## Progress: 768 / 2100 ## Progress: 769 / 2100 ## Progress: 770 / 2100 ## Progress: 771 / 2100 ## Progress: 772 / 2100 ## Progress: 773 / 2100 ## Progress: 774 / 2100 ## Progress: 775 / 2100 ## Progress: 776 / 2100 ## Progress: 777 / 2100 ## Progress: 778 / 2100 ## Progress: 779 / 2100 ## Progress: 780 / 2100 ## Progress: 781 / 2100 ## Progress: 782 / 2100 ## Progress: 783 / 2100 ## Progress: 784 / 2100 ## Progress: 785 / 2100 ## Progress: 786 / 2100 ## Progress: 787 / 2100 ## Progress: 788 / 2100 ## Progress: 789 / 2100 ## Progress: 790 / 2100 ## Progress: 791 / 2100 ## Progress: 792 / 2100 ## Progress: 793 / 2100 ## Progress: 794 / 2100 ## Progress: 795 / 2100 ## Progress: 796 / 2100 ## Progress: 797 / 2100 ## Progress: 798 / 2100 ## Progress: 799 / 2100 ## Progress: 800 / 2100 ## Progress: 801 / 2100 ## Progress: 802 / 2100 ## Progress: 803 / 2100 ## Progress: 804 / 2100 ## Progress: 805 / 2100 ## Progress: 806 / 2100 ## Progress: 807 / 2100 ## Progress: 808 / 2100 ## Progress: 809 / 2100 ## Progress: 810 / 2100 ## Progress: 811 / 2100 ## Progress: 812 / 2100 ## Progress: 813 / 2100 ## Progress: 814 / 2100 ## Progress: 815 / 2100 ## Progress: 816 / 2100 ## Progress: 817 / 2100 ## Progress: 818 / 2100 ## Progress: 819 / 2100 ## Progress: 820 / 2100 ## Progress: 821 / 2100 ## Progress: 822 / 2100 ## Progress: 823 / 2100 ## Progress: 824 / 2100 ## Progress: 825 / 2100 ## Progress: 826 / 2100 ## Progress: 827 / 2100 ## Progress: 828 / 2100 ## Progress: 829 / 2100 ## Progress: 830 / 2100 ## Progress: 831 / 2100 ## Progress: 832 / 2100 ## Progress: 833 / 2100 ## Progress: 834 / 2100 ## Progress: 835 / 2100 ## Progress: 836 / 2100 ## Progress: 837 / 2100 ## Progress: 838 / 2100 ## Progress: 839 / 2100 ## Progress: 840 / 2100 ## Progress: 841 / 2100 ## Progress: 842 / 2100 ## Progress: 843 / 2100 ## Progress: 844 / 2100 ## Progress: 845 / 2100 ## Progress: 846 / 2100 ## Progress: 847 / 2100 ## Progress: 848 / 2100 ## Progress: 849 / 2100 ## Progress: 850 / 2100 ## Progress: 851 / 2100 ## Progress: 852 / 2100 ## Progress: 853 / 2100 ## Progress: 854 / 2100 ## Progress: 855 / 2100 ## Progress: 856 / 2100 ## Progress: 857 / 2100 ## Progress: 858 / 2100 ## Progress: 859 / 2100 ## Progress: 860 / 2100 ## Progress: 861 / 2100 ## Progress: 862 / 2100 ## Progress: 863 / 2100 ## Progress: 864 / 2100 ## Progress: 865 / 2100 ## Progress: 866 / 2100 ## Progress: 867 / 2100 ## Progress: 868 / 2100 ## Progress: 869 / 2100 ## Progress: 870 / 2100 ## Progress: 871 / 2100 ## Progress: 872 / 2100 ## Progress: 873 / 2100 ## Progress: 874 / 2100 ## Progress: 875 / 2100 ## Progress: 876 / 2100 ## Progress: 877 / 2100 ## Progress: 878 / 2100 ## Progress: 879 / 2100 ## Progress: 880 / 2100 ## Progress: 881 / 2100 ## Progress: 882 / 2100 ## Progress: 883 / 2100 ## Progress: 884 / 2100 ## Progress: 885 / 2100 ## Progress: 886 / 2100 ## Progress: 887 / 2100 ## Progress: 888 / 2100 ## Progress: 889 / 2100 ## Progress: 890 / 2100 ## Progress: 891 / 2100 ## Progress: 892 / 2100 ## Progress: 893 / 2100 ## Progress: 894 / 2100 ## Progress: 895 / 2100 ## Progress: 896 / 2100 ## Progress: 897 / 2100 ## Progress: 898 / 2100 ## Progress: 899 / 2100 ## Progress: 900 / 2100 ## Progress: 901 / 2100 ## Progress: 902 / 2100 ## Progress: 903 / 2100 ## Progress: 904 / 2100 ## Progress: 905 / 2100 ## Progress: 906 / 2100 ## Progress: 907 / 2100 ## Progress: 908 / 2100 ## Progress: 909 / 2100 ## Progress: 910 / 2100 ## Progress: 911 / 2100 ## Progress: 912 / 2100 ## Progress: 913 / 2100 ## Progress: 914 / 2100 ## Progress: 915 / 2100 ## Progress: 916 / 2100 ## Progress: 917 / 2100 ## Progress: 918 / 2100 ## Progress: 919 / 2100 ## Progress: 920 / 2100 ## Progress: 921 / 2100 ## Progress: 922 / 2100 ## Progress: 923 / 2100 ## Progress: 924 / 2100 ## Progress: 925 / 2100 ## Progress: 926 / 2100 ## Progress: 927 / 2100 ## Progress: 928 / 2100 ## Progress: 929 / 2100 ## Progress: 930 / 2100 ## Progress: 931 / 2100 ## Progress: 932 / 2100 ## Progress: 933 / 2100 ## Progress: 934 / 2100 ## Progress: 935 / 2100 ## Progress: 936 / 2100 ## Progress: 937 / 2100 ## Progress: 938 / 2100 ## Progress: 939 / 2100 ## Progress: 940 / 2100 ## Progress: 941 / 2100 ## Progress: 942 / 2100 ## Progress: 943 / 2100 ## Progress: 944 / 2100 ## Progress: 945 / 2100 ## Progress: 946 / 2100 ## Progress: 947 / 2100 ## Progress: 948 / 2100 ## Progress: 949 / 2100 ## Progress: 950 / 2100 ## Progress: 951 / 2100 ## Progress: 952 / 2100 ## Progress: 953 / 2100 ## Progress: 954 / 2100 ## Progress: 955 / 2100 ## Progress: 956 / 2100 ## Progress: 957 / 2100 ## Progress: 958 / 2100 ## Progress: 959 / 2100 ## Progress: 960 / 2100 ## Progress: 961 / 2100 ## Progress: 962 / 2100 ## Progress: 963 / 2100 ## Progress: 964 / 2100 ## Progress: 965 / 2100 ## Progress: 966 / 2100 ## Progress: 967 / 2100 ## Progress: 968 / 2100 ## Progress: 969 / 2100 ## Progress: 970 / 2100 ## Progress: 971 / 2100 ## Progress: 972 / 2100 ## Progress: 973 / 2100 ## Progress: 974 / 2100 ## Progress: 975 / 2100 ## Progress: 976 / 2100 ## Progress: 977 / 2100 ## Progress: 978 / 2100 ## Progress: 979 / 2100 ## Progress: 980 / 2100 ## Progress: 981 / 2100 ## Progress: 982 / 2100 ## Progress: 983 / 2100 ## Progress: 984 / 2100 ## Progress: 985 / 2100 ## Progress: 986 / 2100 ## Progress: 987 / 2100 ## Progress: 988 / 2100 ## Progress: 989 / 2100 ## Progress: 990 / 2100 ## Progress: 991 / 2100 ## Progress: 992 / 2100 ## Progress: 993 / 2100 ## Progress: 994 / 2100 ## Progress: 995 / 2100 ## Progress: 996 / 2100 ## Progress: 997 / 2100 ## Progress: 998 / 2100 ## Progress: 999 / 2100 ## Progress: 1000 / 2100 ## Progress: 1001 / 2100 ## Progress: 1002 / 2100 ## Progress: 1003 / 2100 ## Progress: 1004 / 2100 ## Progress: 1005 / 2100 ## Progress: 1006 / 2100 ## Progress: 1007 / 2100 ## Progress: 1008 / 2100 ## Progress: 1009 / 2100 ## Progress: 1010 / 2100 ## Progress: 1011 / 2100 ## Progress: 1012 / 2100 ## Progress: 1013 / 2100 ## Progress: 1014 / 2100 ## Progress: 1015 / 2100 ## Progress: 1016 / 2100 ## Progress: 1017 / 2100 ## Progress: 1018 / 2100 ## Progress: 1019 / 2100 ## Progress: 1020 / 2100 ## Progress: 1021 / 2100 ## Progress: 1022 / 2100 ## Progress: 1023 / 2100 ## Progress: 1024 / 2100 ## Progress: 1025 / 2100 ## Progress: 1026 / 2100 ## Progress: 1027 / 2100 ## Progress: 1028 / 2100 ## Progress: 1029 / 2100 ## Progress: 1030 / 2100 ## Progress: 1031 / 2100 ## Progress: 1032 / 2100 ## Progress: 1033 / 2100 ## Progress: 1034 / 2100 ## Progress: 1035 / 2100 ## Progress: 1036 / 2100 ## Progress: 1037 / 2100 ## Progress: 1038 / 2100 ## Progress: 1039 / 2100 ## Progress: 1040 / 2100 ## Progress: 1041 / 2100 ## Progress: 1042 / 2100 ## Progress: 1043 / 2100 ## Progress: 1044 / 2100 ## Progress: 1045 / 2100 ## Progress: 1046 / 2100 ## Progress: 1047 / 2100 ## Progress: 1048 / 2100 ## Progress: 1049 / 2100 ## Progress: 1050 / 2100 ## Progress: 1051 / 2100 ## Progress: 1052 / 2100 ## Progress: 1053 / 2100 ## Progress: 1054 / 2100 ## Progress: 1055 / 2100 ## Progress: 1056 / 2100 ## Progress: 1057 / 2100 ## Progress: 1058 / 2100 ## Progress: 1059 / 2100 ## Progress: 1060 / 2100 ## Progress: 1061 / 2100 ## Progress: 1062 / 2100 ## Progress: 1063 / 2100 ## Progress: 1064 / 2100 ## Progress: 1065 / 2100 ## Progress: 1066 / 2100 ## Progress: 1067 / 2100 ## Progress: 1068 / 2100 ## Progress: 1069 / 2100 ## Progress: 1070 / 2100 ## Progress: 1071 / 2100 ## Progress: 1072 / 2100 ## Progress: 1073 / 2100 ## Progress: 1074 / 2100 ## Progress: 1075 / 2100 ## Progress: 1076 / 2100 ## Progress: 1077 / 2100 ## Progress: 1078 / 2100 ## Progress: 1079 / 2100 ## Progress: 1080 / 2100 ## Progress: 1081 / 2100 ## Progress: 1082 / 2100 ## Progress: 1083 / 2100 ## Progress: 1084 / 2100 ## Progress: 1085 / 2100 ## Progress: 1086 / 2100 ## Progress: 1087 / 2100 ## Progress: 1088 / 2100 ## Progress: 1089 / 2100 ## Progress: 1090 / 2100 ## Progress: 1091 / 2100 ## Progress: 1092 / 2100 ## Progress: 1093 / 2100 ## Progress: 1094 / 2100 ## Progress: 1095 / 2100 ## Progress: 1096 / 2100 ## Progress: 1097 / 2100 ## Progress: 1098 / 2100 ## Progress: 1099 / 2100 ## Progress: 1100 / 2100 ## Progress: 1101 / 2100 ## Progress: 1102 / 2100 ## Progress: 1103 / 2100 ## Progress: 1104 / 2100 ## Progress: 1105 / 2100 ## Progress: 1106 / 2100 ## Progress: 1107 / 2100 ## Progress: 1108 / 2100 ## Progress: 1109 / 2100 ## Progress: 1110 / 2100 ## Progress: 1111 / 2100 ## Progress: 1112 / 2100 ## Progress: 1113 / 2100 ## Progress: 1114 / 2100 ## Progress: 1115 / 2100 ## Progress: 1116 / 2100 ## Progress: 1117 / 2100 ## Progress: 1118 / 2100 ## Progress: 1119 / 2100 ## Progress: 1120 / 2100 ## Progress: 1121 / 2100 ## Progress: 1122 / 2100 ## Progress: 1123 / 2100 ## Progress: 1124 / 2100 ## Progress: 1125 / 2100 ## Progress: 1126 / 2100 ## Progress: 1127 / 2100 ## Progress: 1128 / 2100 ## Progress: 1129 / 2100 ## Progress: 1130 / 2100 ## Progress: 1131 / 2100 ## Progress: 1132 / 2100 ## Progress: 1133 / 2100 ## Progress: 1134 / 2100 ## Progress: 1135 / 2100 ## Progress: 1136 / 2100 ## Progress: 1137 / 2100 ## Progress: 1138 / 2100 ## Progress: 1139 / 2100 ## Progress: 1140 / 2100 ## Progress: 1141 / 2100 ## Progress: 1142 / 2100 ## Progress: 1143 / 2100 ## Progress: 1144 / 2100 ## Progress: 1145 / 2100 ## Progress: 1146 / 2100 ## Progress: 1147 / 2100 ## Progress: 1148 / 2100 ## Progress: 1149 / 2100 ## Progress: 1150 / 2100 ## Progress: 1151 / 2100 ## Progress: 1152 / 2100 ## Progress: 1153 / 2100 ## Progress: 1154 / 2100 ## Progress: 1155 / 2100 ## Progress: 1156 / 2100 ## Progress: 1157 / 2100 ## Progress: 1158 / 2100 ## Progress: 1159 / 2100 ## Progress: 1160 / 2100 ## Progress: 1161 / 2100 ## Progress: 1162 / 2100 ## Progress: 1163 / 2100 ## Progress: 1164 / 2100 ## Progress: 1165 / 2100 ## Progress: 1166 / 2100 ## Progress: 1167 / 2100 ## Progress: 1168 / 2100 ## Progress: 1169 / 2100 ## Progress: 1170 / 2100 ## Progress: 1171 / 2100 ## Progress: 1172 / 2100 ## Progress: 1173 / 2100 ## Progress: 1174 / 2100 ## Progress: 1175 / 2100 ## Progress: 1176 / 2100 ## Progress: 1177 / 2100 ## Progress: 1178 / 2100 ## Progress: 1179 / 2100 ## Progress: 1180 / 2100 ## Progress: 1181 / 2100 ## Progress: 1182 / 2100 ## Progress: 1183 / 2100 ## Progress: 1184 / 2100 ## Progress: 1185 / 2100 ## Progress: 1186 / 2100 ## Progress: 1187 / 2100 ## Progress: 1188 / 2100 ## Progress: 1189 / 2100 ## Progress: 1190 / 2100 ## Progress: 1191 / 2100 ## Progress: 1192 / 2100 ## Progress: 1193 / 2100 ## Progress: 1194 / 2100 ## Progress: 1195 / 2100 ## Progress: 1196 / 2100 ## Progress: 1197 / 2100 ## Progress: 1198 / 2100 ## Progress: 1199 / 2100 ## Progress: 1200 / 2100 ## Progress: 1201 / 2100 ## Progress: 1202 / 2100 ## Progress: 1203 / 2100 ## Progress: 1204 / 2100 ## Progress: 1205 / 2100 ## Progress: 1206 / 2100 ## Progress: 1207 / 2100 ## Progress: 1208 / 2100 ## Progress: 1209 / 2100 ## Progress: 1210 / 2100 ## Progress: 1211 / 2100 ## Progress: 1212 / 2100 ## Progress: 1213 / 2100 ## Progress: 1214 / 2100 ## Progress: 1215 / 2100 ## Progress: 1216 / 2100 ## Progress: 1217 / 2100 ## Progress: 1218 / 2100 ## Progress: 1219 / 2100 ## Progress: 1220 / 2100 ## Progress: 1221 / 2100 ## Progress: 1222 / 2100 ## Progress: 1223 / 2100 ## Progress: 1224 / 2100 ## Progress: 1225 / 2100 ## Progress: 1226 / 2100 ## Progress: 1227 / 2100 ## Progress: 1228 / 2100 ## Progress: 1229 / 2100 ## Progress: 1230 / 2100 ## Progress: 1231 / 2100 ## Progress: 1232 / 2100 ## Progress: 1233 / 2100 ## Progress: 1234 / 2100 ## Progress: 1235 / 2100 ## Progress: 1236 / 2100 ## Progress: 1237 / 2100 ## Progress: 1238 / 2100 ## Progress: 1239 / 2100 ## Progress: 1240 / 2100 ## Progress: 1241 / 2100 ## Progress: 1242 / 2100 ## Progress: 1243 / 2100 ## Progress: 1244 / 2100 ## Progress: 1245 / 2100 ## Progress: 1246 / 2100 ## Progress: 1247 / 2100 ## Progress: 1248 / 2100 ## Progress: 1249 / 2100 ## Progress: 1250 / 2100 ## Progress: 1251 / 2100 ## Progress: 1252 / 2100 ## Progress: 1253 / 2100 ## Progress: 1254 / 2100 ## Progress: 1255 / 2100 ## Progress: 1256 / 2100 ## Progress: 1257 / 2100 ## Progress: 1258 / 2100 ## Progress: 1259 / 2100 ## Progress: 1260 / 2100 ## Progress: 1261 / 2100 ## Progress: 1262 / 2100 ## Progress: 1263 / 2100 ## Progress: 1264 / 2100 ## Progress: 1265 / 2100 ## Progress: 1266 / 2100 ## Progress: 1267 / 2100 ## Progress: 1268 / 2100 ## Progress: 1269 / 2100 ## Progress: 1270 / 2100 ## Progress: 1271 / 2100 ## Progress: 1272 / 2100 ## Progress: 1273 / 2100 ## Progress: 1274 / 2100 ## Progress: 1275 / 2100 ## Progress: 1276 / 2100 ## Progress: 1277 / 2100 ## Progress: 1278 / 2100 ## Progress: 1279 / 2100 ## Progress: 1280 / 2100 ## Progress: 1281 / 2100 ## Progress: 1282 / 2100 ## Progress: 1283 / 2100 ## Progress: 1284 / 2100 ## Progress: 1285 / 2100 ## Progress: 1286 / 2100 ## Progress: 1287 / 2100 ## Progress: 1288 / 2100 ## Progress: 1289 / 2100 ## Progress: 1290 / 2100 ## Progress: 1291 / 2100 ## Progress: 1292 / 2100 ## Progress: 1293 / 2100 ## Progress: 1294 / 2100 ## Progress: 1295 / 2100 ## Progress: 1296 / 2100 ## Progress: 1297 / 2100 ## Progress: 1298 / 2100 ## Progress: 1299 / 2100 ## Progress: 1300 / 2100 ## Progress: 1301 / 2100 ## Progress: 1302 / 2100 ## Progress: 1303 / 2100 ## Progress: 1304 / 2100 ## Progress: 1305 / 2100 ## Progress: 1306 / 2100 ## Progress: 1307 / 2100 ## Progress: 1308 / 2100 ## Progress: 1309 / 2100 ## Progress: 1310 / 2100 ## Progress: 1311 / 2100 ## Progress: 1312 / 2100 ## Progress: 1313 / 2100 ## Progress: 1314 / 2100 ## Progress: 1315 / 2100 ## Progress: 1316 / 2100 ## Progress: 1317 / 2100 ## Progress: 1318 / 2100 ## Progress: 1319 / 2100 ## Progress: 1320 / 2100 ## Progress: 1321 / 2100 ## Progress: 1322 / 2100 ## Progress: 1323 / 2100 ## Progress: 1324 / 2100 ## Progress: 1325 / 2100 ## Progress: 1326 / 2100 ## Progress: 1327 / 2100 ## Progress: 1328 / 2100 ## Progress: 1329 / 2100 ## Progress: 1330 / 2100 ## Progress: 1331 / 2100 ## Progress: 1332 / 2100 ## Progress: 1333 / 2100 ## Progress: 1334 / 2100 ## Progress: 1335 / 2100 ## Progress: 1336 / 2100 ## Progress: 1337 / 2100 ## Progress: 1338 / 2100 ## Progress: 1339 / 2100 ## Progress: 1340 / 2100 ## Progress: 1341 / 2100 ## Progress: 1342 / 2100 ## Progress: 1343 / 2100 ## Progress: 1344 / 2100 ## Progress: 1345 / 2100 ## Progress: 1346 / 2100 ## Progress: 1347 / 2100 ## Progress: 1348 / 2100 ## Progress: 1349 / 2100 ## Progress: 1350 / 2100 ## Progress: 1351 / 2100 ## Progress: 1352 / 2100 ## Progress: 1353 / 2100 ## Progress: 1354 / 2100 ## Progress: 1355 / 2100 ## Progress: 1356 / 2100 ## Progress: 1357 / 2100 ## Progress: 1358 / 2100 ## Progress: 1359 / 2100 ## Progress: 1360 / 2100 ## Progress: 1361 / 2100 ## Progress: 1362 / 2100 ## Progress: 1363 / 2100 ## Progress: 1364 / 2100 ## Progress: 1365 / 2100 ## Progress: 1366 / 2100 ## Progress: 1367 / 2100 ## Progress: 1368 / 2100 ## Progress: 1369 / 2100 ## Progress: 1370 / 2100 ## Progress: 1371 / 2100 ## Progress: 1372 / 2100 ## Progress: 1373 / 2100 ## Progress: 1374 / 2100 ## Progress: 1375 / 2100 ## Progress: 1376 / 2100 ## Progress: 1377 / 2100 ## Progress: 1378 / 2100 ## Progress: 1379 / 2100 ## Progress: 1380 / 2100 ## Progress: 1381 / 2100 ## Progress: 1382 / 2100 ## Progress: 1383 / 2100 ## Progress: 1384 / 2100 ## Progress: 1385 / 2100 ## Progress: 1386 / 2100 ## Progress: 1387 / 2100 ## Progress: 1388 / 2100 ## Progress: 1389 / 2100 ## Progress: 1390 / 2100 ## Progress: 1391 / 2100 ## Progress: 1392 / 2100 ## Progress: 1393 / 2100 ## Progress: 1394 / 2100 ## Progress: 1395 / 2100 ## Progress: 1396 / 2100 ## Progress: 1397 / 2100 ## Progress: 1398 / 2100 ## Progress: 1399 / 2100 ## Progress: 1400 / 2100 ## Progress: 1401 / 2100 ## Progress: 1402 / 2100 ## Progress: 1403 / 2100 ## Progress: 1404 / 2100 ## Progress: 1405 / 2100 ## Progress: 1406 / 2100 ## Progress: 1407 / 2100 ## Progress: 1408 / 2100 ## Progress: 1409 / 2100 ## Progress: 1410 / 2100 ## Progress: 1411 / 2100 ## Progress: 1412 / 2100 ## Progress: 1413 / 2100 ## Progress: 1414 / 2100 ## Progress: 1415 / 2100 ## Progress: 1416 / 2100 ## Progress: 1417 / 2100 ## Progress: 1418 / 2100 ## Progress: 1419 / 2100 ## Progress: 1420 / 2100 ## Progress: 1421 / 2100 ## Progress: 1422 / 2100 ## Progress: 1423 / 2100 ## Progress: 1424 / 2100 ## Progress: 1425 / 2100 ## Progress: 1426 / 2100 ## Progress: 1427 / 2100 ## Progress: 1428 / 2100 ## Progress: 1429 / 2100 ## Progress: 1430 / 2100 ## Progress: 1431 / 2100 ## Progress: 1432 / 2100 ## Progress: 1433 / 2100 ## Progress: 1434 / 2100 ## Progress: 1435 / 2100 ## Progress: 1436 / 2100 ## Progress: 1437 / 2100 ## Progress: 1438 / 2100 ## Progress: 1439 / 2100 ## Progress: 1440 / 2100 ## Progress: 1441 / 2100 ## Progress: 1442 / 2100 ## Progress: 1443 / 2100 ## Progress: 1444 / 2100 ## Progress: 1445 / 2100 ## Progress: 1446 / 2100 ## Progress: 1447 / 2100 ## Progress: 1448 / 2100 ## Progress: 1449 / 2100 ## Progress: 1450 / 2100 ## Progress: 1451 / 2100 ## Progress: 1452 / 2100 ## Progress: 1453 / 2100 ## Progress: 1454 / 2100 ## Progress: 1455 / 2100 ## Progress: 1456 / 2100 ## Progress: 1457 / 2100 ## Progress: 1458 / 2100 ## Progress: 1459 / 2100 ## Progress: 1460 / 2100 ## Progress: 1461 / 2100 ## Progress: 1462 / 2100 ## Progress: 1463 / 2100 ## Progress: 1464 / 2100 ## Progress: 1465 / 2100 ## Progress: 1466 / 2100 ## Progress: 1467 / 2100 ## Progress: 1468 / 2100 ## Progress: 1469 / 2100 ## Progress: 1470 / 2100 ## Progress: 1471 / 2100 ## Progress: 1472 / 2100 ## Progress: 1473 / 2100 ## Progress: 1474 / 2100 ## Progress: 1475 / 2100 ## Progress: 1476 / 2100 ## Progress: 1477 / 2100 ## Progress: 1478 / 2100 ## Progress: 1479 / 2100 ## Progress: 1480 / 2100 ## Progress: 1481 / 2100 ## Progress: 1482 / 2100 ## Progress: 1483 / 2100 ## Progress: 1484 / 2100 ## Progress: 1485 / 2100 ## Progress: 1486 / 2100 ## Progress: 1487 / 2100 ## Progress: 1488 / 2100 ## Progress: 1489 / 2100 ## Progress: 1490 / 2100 ## Progress: 1491 / 2100 ## Progress: 1492 / 2100 ## Progress: 1493 / 2100 ## Progress: 1494 / 2100 ## Progress: 1495 / 2100 ## Progress: 1496 / 2100 ## Progress: 1497 / 2100 ## Progress: 1498 / 2100 ## Progress: 1499 / 2100 ## Progress: 1500 / 2100 ## Progress: 1501 / 2100 ## Progress: 1502 / 2100 ## Progress: 1503 / 2100 ## Progress: 1504 / 2100 ## Progress: 1505 / 2100 ## Progress: 1506 / 2100 ## Progress: 1507 / 2100 ## Progress: 1508 / 2100 ## Progress: 1509 / 2100 ## Progress: 1510 / 2100 ## Progress: 1511 / 2100 ## Progress: 1512 / 2100 ## Progress: 1513 / 2100 ## Progress: 1514 / 2100 ## Progress: 1515 / 2100 ## Progress: 1516 / 2100 ## Progress: 1517 / 2100 ## Progress: 1518 / 2100 ## Progress: 1519 / 2100 ## Progress: 1520 / 2100 ## Progress: 1521 / 2100 ## Progress: 1522 / 2100 ## Progress: 1523 / 2100 ## Progress: 1524 / 2100 ## Progress: 1525 / 2100 ## Progress: 1526 / 2100 ## Progress: 1527 / 2100 ## Progress: 1528 / 2100 ## Progress: 1529 / 2100 ## Progress: 1530 / 2100 ## Progress: 1531 / 2100 ## Progress: 1532 / 2100 ## Progress: 1533 / 2100 ## Progress: 1534 / 2100 ## Progress: 1535 / 2100 ## Progress: 1536 / 2100 ## Progress: 1537 / 2100 ## Progress: 1538 / 2100 ## Progress: 1539 / 2100 ## Progress: 1540 / 2100 ## Progress: 1541 / 2100 ## Progress: 1542 / 2100 ## Progress: 1543 / 2100 ## Progress: 1544 / 2100 ## Progress: 1545 / 2100 ## Progress: 1546 / 2100 ## Progress: 1547 / 2100 ## Progress: 1548 / 2100 ## Progress: 1549 / 2100 ## Progress: 1550 / 2100 ## Progress: 1551 / 2100 ## Progress: 1552 / 2100 ## Progress: 1553 / 2100 ## Progress: 1554 / 2100 ## Progress: 1555 / 2100 ## Progress: 1556 / 2100 ## Progress: 1557 / 2100 ## Progress: 1558 / 2100 ## Progress: 1559 / 2100 ## Progress: 1560 / 2100 ## Progress: 1561 / 2100 ## Progress: 1562 / 2100 ## Progress: 1563 / 2100 ## Progress: 1564 / 2100 ## Progress: 1565 / 2100 ## Progress: 1566 / 2100 ## Progress: 1567 / 2100 ## Progress: 1568 / 2100 ## Progress: 1569 / 2100 ## Progress: 1570 / 2100 ## Progress: 1571 / 2100 ## Progress: 1572 / 2100 ## Progress: 1573 / 2100 ## Progress: 1574 / 2100 ## Progress: 1575 / 2100 ## Progress: 1576 / 2100 ## Progress: 1577 / 2100 ## Progress: 1578 / 2100 ## Progress: 1579 / 2100 ## Progress: 1580 / 2100 ## Progress: 1581 / 2100 ## Progress: 1582 / 2100 ## Progress: 1583 / 2100 ## Progress: 1584 / 2100 ## Progress: 1585 / 2100 ## Progress: 1586 / 2100 ## Progress: 1587 / 2100 ## Progress: 1588 / 2100 ## Progress: 1589 / 2100 ## Progress: 1590 / 2100 ## Progress: 1591 / 2100 ## Progress: 1592 / 2100 ## Progress: 1593 / 2100 ## Progress: 1594 / 2100 ## Progress: 1595 / 2100 ## Progress: 1596 / 2100 ## Progress: 1597 / 2100 ## Progress: 1598 / 2100 ## Progress: 1599 / 2100 ## Progress: 1600 / 2100 ## Progress: 1601 / 2100 ## Progress: 1602 / 2100 ## Progress: 1603 / 2100 ## Progress: 1604 / 2100 ## Progress: 1605 / 2100 ## Progress: 1606 / 2100 ## Progress: 1607 / 2100 ## Progress: 1608 / 2100 ## Progress: 1609 / 2100 ## Progress: 1610 / 2100 ## Progress: 1611 / 2100 ## Progress: 1612 / 2100 ## Progress: 1613 / 2100 ## Progress: 1614 / 2100 ## Progress: 1615 / 2100 ## Progress: 1616 / 2100 ## Progress: 1617 / 2100 ## Progress: 1618 / 2100 ## Progress: 1619 / 2100 ## Progress: 1620 / 2100 ## Progress: 1621 / 2100 ## Progress: 1622 / 2100 ## Progress: 1623 / 2100 ## Progress: 1624 / 2100 ## Progress: 1625 / 2100 ## Progress: 1626 / 2100 ## Progress: 1627 / 2100 ## Progress: 1628 / 2100 ## Progress: 1629 / 2100 ## Progress: 1630 / 2100 ## Progress: 1631 / 2100 ## Progress: 1632 / 2100 ## Progress: 1633 / 2100 ## Progress: 1634 / 2100 ## Progress: 1635 / 2100 ## Progress: 1636 / 2100 ## Progress: 1637 / 2100 ## Progress: 1638 / 2100 ## Progress: 1639 / 2100 ## Progress: 1640 / 2100 ## Progress: 1641 / 2100 ## Progress: 1642 / 2100 ## Progress: 1643 / 2100 ## Progress: 1644 / 2100 ## Progress: 1645 / 2100 ## Progress: 1646 / 2100 ## Progress: 1647 / 2100 ## Progress: 1648 / 2100 ## Progress: 1649 / 2100 ## Progress: 1650 / 2100 ## Progress: 1651 / 2100 ## Progress: 1652 / 2100 ## Progress: 1653 / 2100 ## Progress: 1654 / 2100 ## Progress: 1655 / 2100 ## Progress: 1656 / 2100 ## Progress: 1657 / 2100 ## Progress: 1658 / 2100 ## Progress: 1659 / 2100 ## Progress: 1660 / 2100 ## Progress: 1661 / 2100 ## Progress: 1662 / 2100 ## Progress: 1663 / 2100 ## Progress: 1664 / 2100 ## Progress: 1665 / 2100 ## Progress: 1666 / 2100 ## Progress: 1667 / 2100 ## Progress: 1668 / 2100 ## Progress: 1669 / 2100 ## Progress: 1670 / 2100 ## Progress: 1671 / 2100 ## Progress: 1672 / 2100 ## Progress: 1673 / 2100 ## Progress: 1674 / 2100 ## Progress: 1675 / 2100 ## Progress: 1676 / 2100 ## Progress: 1677 / 2100 ## Progress: 1678 / 2100 ## Progress: 1679 / 2100 ## Progress: 1680 / 2100 ## Progress: 1681 / 2100 ## Progress: 1682 / 2100 ## Progress: 1683 / 2100 ## Progress: 1684 / 2100 ## Progress: 1685 / 2100 ## Progress: 1686 / 2100 ## Progress: 1687 / 2100 ## Progress: 1688 / 2100 ## Progress: 1689 / 2100 ## Progress: 1690 / 2100 ## Progress: 1691 / 2100 ## Progress: 1692 / 2100 ## Progress: 1693 / 2100 ## Progress: 1694 / 2100 ## Progress: 1695 / 2100 ## Progress: 1696 / 2100 ## Progress: 1697 / 2100 ## Progress: 1698 / 2100 ## Progress: 1699 / 2100 ## Progress: 1700 / 2100 ## Progress: 1701 / 2100 ## Progress: 1702 / 2100 ## Progress: 1703 / 2100 ## Progress: 1704 / 2100 ## Progress: 1705 / 2100 ## Progress: 1706 / 2100 ## Progress: 1707 / 2100 ## Progress: 1708 / 2100 ## Progress: 1709 / 2100 ## Progress: 1710 / 2100 ## Progress: 1711 / 2100 ## Progress: 1712 / 2100 ## Progress: 1713 / 2100 ## Progress: 1714 / 2100 ## Progress: 1715 / 2100 ## Progress: 1716 / 2100 ## Progress: 1717 / 2100 ## Progress: 1718 / 2100 ## Progress: 1719 / 2100 ## Progress: 1720 / 2100 ## Progress: 1721 / 2100 ## Progress: 1722 / 2100 ## Progress: 1723 / 2100 ## Progress: 1724 / 2100 ## Progress: 1725 / 2100 ## Progress: 1726 / 2100 ## Progress: 1727 / 2100 ## Progress: 1728 / 2100 ## Progress: 1729 / 2100 ## Progress: 1730 / 2100 ## Progress: 1731 / 2100 ## Progress: 1732 / 2100 ## Progress: 1733 / 2100 ## Progress: 1734 / 2100 ## Progress: 1735 / 2100 ## Progress: 1736 / 2100 ## Progress: 1737 / 2100 ## Progress: 1738 / 2100 ## Progress: 1739 / 2100 ## Progress: 1740 / 2100 ## Progress: 1741 / 2100 ## Progress: 1742 / 2100 ## Progress: 1743 / 2100 ## Progress: 1744 / 2100 ## Progress: 1745 / 2100 ## Progress: 1746 / 2100 ## Progress: 1747 / 2100 ## Progress: 1748 / 2100 ## Progress: 1749 / 2100 ## Progress: 1750 / 2100 ## Progress: 1751 / 2100 ## Progress: 1752 / 2100 ## Progress: 1753 / 2100 ## Progress: 1754 / 2100 ## Progress: 1755 / 2100 ## Progress: 1756 / 2100 ## Progress: 1757 / 2100 ## Progress: 1758 / 2100 ## Progress: 1759 / 2100 ## Progress: 1760 / 2100 ## Progress: 1761 / 2100 ## Progress: 1762 / 2100 ## Progress: 1763 / 2100 ## Progress: 1764 / 2100 ## Progress: 1765 / 2100 ## Progress: 1766 / 2100 ## Progress: 1767 / 2100 ## Progress: 1768 / 2100 ## Progress: 1769 / 2100 ## Progress: 1770 / 2100 ## Progress: 1771 / 2100 ## Progress: 1772 / 2100 ## Progress: 1773 / 2100 ## Progress: 1774 / 2100 ## Progress: 1775 / 2100 ## Progress: 1776 / 2100 ## Progress: 1777 / 2100 ## Progress: 1778 / 2100 ## Progress: 1779 / 2100 ## Progress: 1780 / 2100 ## Progress: 1781 / 2100 ## Progress: 1782 / 2100 ## Progress: 1783 / 2100 ## Progress: 1784 / 2100 ## Progress: 1785 / 2100 ## Progress: 1786 / 2100 ## Progress: 1787 / 2100 ## Progress: 1788 / 2100 ## Progress: 1789 / 2100 ## Progress: 1790 / 2100 ## Progress: 1791 / 2100 ## Progress: 1792 / 2100 ## Progress: 1793 / 2100 ## Progress: 1794 / 2100 ## Progress: 1795 / 2100 ## Progress: 1796 / 2100 ## Progress: 1797 / 2100 ## Progress: 1798 / 2100 ## Progress: 1799 / 2100 ## Progress: 1800 / 2100 ## Progress: 1801 / 2100 ## Progress: 1802 / 2100 ## Progress: 1803 / 2100 ## Progress: 1804 / 2100 ## Progress: 1805 / 2100 ## Progress: 1806 / 2100 ## Progress: 1807 / 2100 ## Progress: 1808 / 2100 ## Progress: 1809 / 2100 ## Progress: 1810 / 2100 ## Progress: 1811 / 2100 ## Progress: 1812 / 2100 ## Progress: 1813 / 2100 ## Progress: 1814 / 2100 ## Progress: 1815 / 2100 ## Progress: 1816 / 2100 ## Progress: 1817 / 2100 ## Progress: 1818 / 2100 ## Progress: 1819 / 2100 ## Progress: 1820 / 2100 ## Progress: 1821 / 2100 ## Progress: 1822 / 2100 ## Progress: 1823 / 2100 ## Progress: 1824 / 2100 ## Progress: 1825 / 2100 ## Progress: 1826 / 2100 ## Progress: 1827 / 2100 ## Progress: 1828 / 2100 ## Progress: 1829 / 2100 ## Progress: 1830 / 2100 ## Progress: 1831 / 2100 ## Progress: 1832 / 2100 ## Progress: 1833 / 2100 ## Progress: 1834 / 2100 ## Progress: 1835 / 2100 ## Progress: 1836 / 2100 ## Progress: 1837 / 2100 ## Progress: 1838 / 2100 ## Progress: 1839 / 2100 ## Progress: 1840 / 2100 ## Progress: 1841 / 2100 ## Progress: 1842 / 2100 ## Progress: 1843 / 2100 ## Progress: 1844 / 2100 ## Progress: 1845 / 2100 ## Progress: 1846 / 2100 ## Progress: 1847 / 2100 ## Progress: 1848 / 2100 ## Progress: 1849 / 2100 ## Progress: 1850 / 2100 ## Progress: 1851 / 2100 ## Progress: 1852 / 2100 ## Progress: 1853 / 2100 ## Progress: 1854 / 2100 ## Progress: 1855 / 2100 ## Progress: 1856 / 2100 ## Progress: 1857 / 2100 ## Progress: 1858 / 2100 ## Progress: 1859 / 2100 ## Progress: 1860 / 2100 ## Progress: 1861 / 2100 ## Progress: 1862 / 2100 ## Progress: 1863 / 2100 ## Progress: 1864 / 2100 ## Progress: 1865 / 2100 ## Progress: 1866 / 2100 ## Progress: 1867 / 2100 ## Progress: 1868 / 2100 ## Progress: 1869 / 2100 ## Progress: 1870 / 2100 ## Progress: 1871 / 2100 ## Progress: 1872 / 2100 ## Progress: 1873 / 2100 ## Progress: 1874 / 2100 ## Progress: 1875 / 2100 ## Progress: 1876 / 2100 ## Progress: 1877 / 2100 ## Progress: 1878 / 2100 ## Progress: 1879 / 2100 ## Progress: 1880 / 2100 ## Progress: 1881 / 2100 ## Progress: 1882 / 2100 ## Progress: 1883 / 2100 ## Progress: 1884 / 2100 ## Progress: 1885 / 2100 ## Progress: 1886 / 2100 ## Progress: 1887 / 2100 ## Progress: 1888 / 2100 ## Progress: 1889 / 2100 ## Progress: 1890 / 2100 ## Progress: 1891 / 2100 ## Progress: 1892 / 2100 ## Progress: 1893 / 2100 ## Progress: 1894 / 2100 ## Progress: 1895 / 2100 ## Progress: 1896 / 2100 ## Progress: 1897 / 2100 ## Progress: 1898 / 2100 ## Progress: 1899 / 2100 ## Progress: 1900 / 2100 ## Progress: 1901 / 2100 ## Progress: 1902 / 2100 ## Progress: 1903 / 2100 ## Progress: 1904 / 2100 ## Progress: 1905 / 2100 ## Progress: 1906 / 2100 ## Progress: 1907 / 2100 ## Progress: 1908 / 2100 ## Progress: 1909 / 2100 ## Progress: 1910 / 2100 ## Progress: 1911 / 2100 ## Progress: 1912 / 2100 ## Progress: 1913 / 2100 ## Progress: 1914 / 2100 ## Progress: 1915 / 2100 ## Progress: 1916 / 2100 ## Progress: 1917 / 2100 ## Progress: 1918 / 2100 ## Progress: 1919 / 2100 ## Progress: 1920 / 2100 ## Progress: 1921 / 2100 ## Progress: 1922 / 2100 ## Progress: 1923 / 2100 ## Progress: 1924 / 2100 ## Progress: 1925 / 2100 ## Progress: 1926 / 2100 ## Progress: 1927 / 2100 ## Progress: 1928 / 2100 ## Progress: 1929 / 2100 ## Progress: 1930 / 2100 ## Progress: 1931 / 2100 ## Progress: 1932 / 2100 ## Progress: 1933 / 2100 ## Progress: 1934 / 2100 ## Progress: 1935 / 2100 ## Progress: 1936 / 2100 ## Progress: 1937 / 2100 ## Progress: 1938 / 2100 ## Progress: 1939 / 2100 ## Progress: 1940 / 2100 ## Progress: 1941 / 2100 ## Progress: 1942 / 2100 ## Progress: 1943 / 2100 ## Progress: 1944 / 2100 ## Progress: 1945 / 2100 ## Progress: 1946 / 2100 ## Progress: 1947 / 2100 ## Progress: 1948 / 2100 ## Progress: 1949 / 2100 ## Progress: 1950 / 2100 ## Progress: 1951 / 2100 ## Progress: 1952 / 2100 ## Progress: 1953 / 2100 ## Progress: 1954 / 2100 ## Progress: 1955 / 2100 ## Progress: 1956 / 2100 ## Progress: 1957 / 2100 ## Progress: 1958 / 2100 ## Progress: 1959 / 2100 ## Progress: 1960 / 2100 ## Progress: 1961 / 2100 ## Progress: 1962 / 2100 ## Progress: 1963 / 2100 ## Progress: 1964 / 2100 ## Progress: 1965 / 2100 ## Progress: 1966 / 2100 ## Progress: 1967 / 2100 ## Progress: 1968 / 2100 ## Progress: 1969 / 2100 ## Progress: 1970 / 2100 ## Progress: 1971 / 2100 ## Progress: 1972 / 2100 ## Progress: 1973 / 2100 ## Progress: 1974 / 2100 ## Progress: 1975 / 2100 ## Progress: 1976 / 2100 ## Progress: 1977 / 2100 ## Progress: 1978 / 2100 ## Progress: 1979 / 2100 ## Progress: 1980 / 2100 ## Progress: 1981 / 2100 ## Progress: 1982 / 2100 ## Progress: 1983 / 2100 ## Progress: 1984 / 2100 ## Progress: 1985 / 2100 ## Progress: 1986 / 2100 ## Progress: 1987 / 2100 ## Progress: 1988 / 2100 ## Progress: 1989 / 2100 ## Progress: 1990 / 2100 ## Progress: 1991 / 2100 ## Progress: 1992 / 2100 ## Progress: 1993 / 2100 ## Progress: 1994 / 2100 ## Progress: 1995 / 2100 ## Progress: 1996 / 2100 ## Progress: 1997 / 2100 ## Progress: 1998 / 2100 ## Progress: 1999 / 2100 ## Progress: 2000 / 2100 ## Progress: 2001 / 2100 ## Progress: 2002 / 2100 ## Progress: 2003 / 2100 ## Progress: 2004 / 2100 ## Progress: 2005 / 2100 ## Progress: 2006 / 2100 ## Progress: 2007 / 2100 ## Progress: 2008 / 2100 ## Progress: 2009 / 2100 ## Progress: 2010 / 2100 ## Progress: 2011 / 2100 ## Progress: 2012 / 2100 ## Progress: 2013 / 2100 ## Progress: 2014 / 2100 ## Progress: 2015 / 2100 ## Progress: 2016 / 2100 ## Progress: 2017 / 2100 ## Progress: 2018 / 2100 ## Progress: 2019 / 2100 ## Progress: 2020 / 2100 ## Progress: 2021 / 2100 ## Progress: 2022 / 2100 ## Progress: 2023 / 2100 ## Progress: 2024 / 2100 ## Progress: 2025 / 2100 ## Progress: 2026 / 2100 ## Progress: 2027 / 2100 ## Progress: 2028 / 2100 ## Progress: 2029 / 2100 ## Progress: 2030 / 2100 ## Progress: 2031 / 2100 ## Progress: 2032 / 2100 ## Progress: 2033 / 2100 ## Progress: 2034 / 2100 ## Progress: 2035 / 2100 ## Progress: 2036 / 2100 ## Progress: 2037 / 2100 ## Progress: 2038 / 2100 ## Progress: 2039 / 2100 ## Progress: 2040 / 2100 ## Progress: 2041 / 2100 ## Progress: 2042 / 2100 ## Progress: 2043 / 2100 ## Progress: 2044 / 2100 ## Progress: 2045 / 2100 ## Progress: 2046 / 2100 ## Progress: 2047 / 2100 ## Progress: 2048 / 2100 ## Progress: 2049 / 2100 ## Progress: 2050 / 2100 ## Progress: 2051 / 2100 ## Progress: 2052 / 2100 ## Progress: 2053 / 2100 ## Progress: 2054 / 2100 ## Progress: 2055 / 2100 ## Progress: 2056 / 2100 ## Progress: 2057 / 2100 ## Progress: 2058 / 2100 ## Progress: 2059 / 2100 ## Progress: 2060 / 2100 ## Progress: 2061 / 2100 ## Progress: 2062 / 2100 ## Progress: 2063 / 2100 ## Progress: 2064 / 2100 ## Progress: 2065 / 2100 ## Progress: 2066 / 2100 ## Progress: 2067 / 2100 ## Progress: 2068 / 2100 ## Progress: 2069 / 2100 ## Progress: 2070 / 2100 ## Progress: 2071 / 2100 ## Progress: 2072 / 2100 ## Progress: 2073 / 2100 ## Progress: 2074 / 2100 ## Progress: 2075 / 2100 ## Progress: 2076 / 2100 ## Progress: 2077 / 2100 ## Progress: 2078 / 2100 ## Progress: 2079 / 2100 ## Progress: 2080 / 2100 ## Progress: 2081 / 2100 ## Progress: 2082 / 2100 ## Progress: 2083 / 2100 ## Progress: 2084 / 2100 ## Progress: 2085 / 2100 ## Progress: 2086 / 2100 ## Progress: 2087 / 2100 ## Progress: 2088 / 2100 ## Progress: 2089 / 2100 ## Progress: 2090 / 2100 ## Progress: 2091 / 2100 ## Progress: 2092 / 2100 ## Progress: 2093 / 2100 ## Progress: 2094 / 2100 ## Progress: 2095 / 2100 ## Progress: 2096 / 2100 ## Progress: 2097 / 2100 ## Progress: 2098 / 2100 ## Progress: 2099 / 2100 ## Progress: 2100 / 2100  ## Create a power plot plotPower(act.n, alpha=0.05, powerParam=\u0026quot;y~x2\u0026quot;) abline(h=.8, lwd=2, lty=2)  ## Rerun for more fine grained information across wider range of sample sizes act.n \u0026lt;- sim(model=fitted.model, n =seq(10,1000,10), generate=hypothesized.model, lavaanfun = \u0026quot;sem\u0026quot;)  ## Progress: 1 / 100 ## Progress: 2 / 100 ## Progress: 3 / 100 ## Progress: 4 / 100 ## Progress: 5 / 100 ## Progress: 6 / 100 ## Progress: 7 / 100 ## Progress: 8 / 100 ## Progress: 9 / 100 ## Progress: 10 / 100 ## Progress: 11 / 100 ## Progress: 12 / 100 ## Progress: 13 / 100 ## Progress: 14 / 100 ## Progress: 15 / 100 ## Progress: 16 / 100 ## Progress: 17 / 100 ## Progress: 18 / 100 ## Progress: 19 / 100 ## Progress: 20 / 100 ## Progress: 21 / 100 ## Progress: 22 / 100 ## Progress: 23 / 100 ## Progress: 24 / 100 ## Progress: 25 / 100 ## Progress: 26 / 100 ## Progress: 27 / 100 ## Progress: 28 / 100 ## Progress: 29 / 100 ## Progress: 30 / 100 ## Progress: 31 / 100 ## Progress: 32 / 100 ## Progress: 33 / 100 ## Progress: 34 / 100 ## Progress: 35 / 100 ## Progress: 36 / 100 ## Progress: 37 / 100 ## Progress: 38 / 100 ## Progress: 39 / 100 ## Progress: 40 / 100 ## Progress: 41 / 100 ## Progress: 42 / 100 ## Progress: 43 / 100 ## Progress: 44 / 100 ## Progress: 45 / 100 ## Progress: 46 / 100 ## Progress: 47 / 100 ## Progress: 48 / 100 ## Progress: 49 / 100 ## Progress: 50 / 100 ## Progress: 51 / 100 ## Progress: 52 / 100 ## Progress: 53 / 100 ## Progress: 54 / 100 ## Progress: 55 / 100 ## Progress: 56 / 100 ## Progress: 57 / 100 ## Progress: 58 / 100 ## Progress: 59 / 100 ## Progress: 60 / 100 ## Progress: 61 / 100 ## Progress: 62 / 100 ## Progress: 63 / 100 ## Progress: 64 / 100 ## Progress: 65 / 100 ## Progress: 66 / 100 ## Progress: 67 / 100 ## Progress: 68 / 100 ## Progress: 69 / 100 ## Progress: 70 / 100 ## Progress: 71 / 100 ## Progress: 72 / 100 ## Progress: 73 / 100 ## Progress: 74 / 100 ## Progress: 75 / 100 ## Progress: 76 / 100 ## Progress: 77 / 100 ## Progress: 78 / 100 ## Progress: 79 / 100 ## Progress: 80 / 100 ## Progress: 81 / 100 ## Progress: 82 / 100 ## Progress: 83 / 100 ## Progress: 84 / 100 ## Progress: 85 / 100 ## Progress: 86 / 100 ## Progress: 87 / 100 ## Progress: 88 / 100 ## Progress: 89 / 100 ## Progress: 90 / 100 ## Progress: 91 / 100 ## Progress: 92 / 100 ## Progress: 93 / 100 ## Progress: 94 / 100 ## Progress: 95 / 100 ## Progress: 96 / 100 ## Progress: 97 / 100 ## Progress: 98 / 100 ## Progress: 99 / 100 ## Progress: 100 / 100  act.pwr.n \u0026lt;- getPower(act.n, alpha = 0.05) findPower(act.pwr.n, iv=\u0026quot;N\u0026quot;, power=0.8)  ## y~x1 y~x2 y~x3 y~x4 y~~y ## 801 144 180 16 Inf  Next is simulating data from factor models and structural equation models.\n","date":1555200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555200000,"objectID":"598b2b8a607cd7072896ac090a1ac06b","permalink":"/tutorial/2019-04-21-r-reg-sim/","publishdate":"2019-04-14T00:00:00Z","relpermalink":"/tutorial/2019-04-21-r-reg-sim/","section":"tutorial","summary":"This file extends the introduction I gave to simulating data from coin flips. Now, the goal is to show how the basics extend and can easily be used for helpful parts of study design planning.\nSimulating More Complex Models We can easily extend the basic concepts of simulating from a simple population to simulating data from a more complex data structure. This is how we can simulate a simple regression model with one outcome (Y) and five predictors (x1, x2, etc.","tags":["R","simulation","tutorial"],"title":"Simulating Data from Regression Models","type":"tutorial"},{"authors":null,"categories":["Writing","Work-flow","grad-life"],"content":" Recently, I have begun writing more and more. After a beer (or a few more than a beer) I was wondering how I ended up writing so much. Until I entered my current position as a PhD student I never thought writing would be a major aspect of my work. By work, I mean that I am researcher. My training is in educational measurement and advanced statistical methodology. A lot of what I consider my research is banging my head against a wall to get the darn computer software to quit returning errors during an analysis. So, the prospect of being a writer didn\u0026rsquo;t occur to me at first.\nBut, then I realized something important. I want a job.\nYes, my motivation for getting my PhD is to get a job. To get a job, I need to communicate my research effectively enough to get publications. Describing complex statistical procedures in any form of plain English is anything but simple when first starting. One of my goals is to be able to write in a simple way for others to understand. The issue I find is that writing about structural equation modeling, multilevel modeling, and other complex form of analysis is really easy to be overly complex. For example, when describing multilevel confirmatory factor analysis (a narely mouthful even to say) is easy to be complex and use sophistocated language that masks my lack of fully understanding the subject matter. But, this is not useful to anyone. Especially not for myself.\nThis brings me to why I write.\nI write to work on my skills with written English. I find being able to put words in order a skill that will help me no matter what job I end up doing once I graduate. Writing, just like any skill, take me time to learn. The more time I spend, the more I will I get out of the prusuit.\nMy Difficulties with Writing All that information above feels good and all. But, what I want to work with mostly is being able to explain complex statistical methods in plain language. In order for me to get to the point where I am more confident in my communication skills I am practicing more. My major difficulty with writing in the land of advance methods is translating formulas to English.\nOne of the topics I am actively researching is mixture modeling. Mixture modeling is a way of classifying cases into like groups based on observed characteristics. The methods for mixture modeling are extremely varied. One of the simplest mixture models is a mixture of univariate normal distributions. That is also a mouthful that doesn\u0026rsquo;t make much sense.\nSo, to backtrack to explain mixture modeling in the most general sense so that I can bring us back to the \u0026ldquo;simple case\u0026rdquo; of univariate mixtures. We can think of mixture models as a form of classification. Classification systems are found all around us. Libraries are prime examples of classification systems. I\u0026rsquo;m a graduate student and I use the library so much. Yes, most research articles are online, but I am still using books for a lot of my information. Many of the best introductions to advanced methods are in textbooks now and my library contains many of the seminal texts. Without some way of organizing the massive number of books within a library I could never find the right books. Books could be classified in any number of ways. One way could be author name. Another could be date published. Then one could be by subject matter. Each of these different methods for classifying books yields a different organization (a different classification system). The classification system employed by libraries is analogous to the process that mixture modeling aims to mimic.\nDespite mixture modeling being a method of classification, there is amuchmore general aspect to mixture modeling compared to library organizational systems. In mixture modeling, the classification is unknown at the start. Based on the observed data, we tried to create the system for classifying people into like groups. The system may not be the most optimal, but provide a set of rules that can be used to create classes of individuals based on observable characteristics.\nBeyond Developing Skills Being a skilled communicator takes practice, at least from my experience. The process of developing skills is by no means an easy path. This is why I write, to create meaningful opportunities for me to communicate topics that I am interested in. In the posts and tutorials to come I will discuss topics such as\n Research collaborations,\n Working towards being part of a scholarly community,\n Introduction to matrix algebra,\n Simple linear regression through matrix operations, and\n Being stuck with convergence B.S. in mixture modeling.\n  I am not sure in what order these different topics will come above. But these will come in time.\n","date":1551830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551830400,"objectID":"f9eff544149e9d66f6a61bcb47a2cc79","permalink":"/post/why-i-write/","publishdate":"2019-03-06T00:00:00Z","relpermalink":"/post/why-i-write/","section":"post","summary":"Recently, I have begun writing more and more. After a beer (or a few more than a beer) I was wondering how I ended up writing so much. Until I entered my current position as a PhD student I never thought writing would be a major aspect of my work. By work, I mean that I am researcher. My training is in educational measurement and advanced statistical methodology. A lot of what I consider my research is banging my head against a wall to get the darn computer software to quit returning errors during an analysis.","tags":[],"title":"Why I Write","type":"post"},{"authors":["K. Leigh Greathouse","James Robert White","R. Noah Padgett","Brittany G. Perrotta","Gregory D. Jenkins","Nicholas Chia","Jun Chen"],"categories":null,"content":"","date":1551333600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551333600,"objectID":"1d054f82665d0cce4d5cda9467fcfec6","permalink":"/publication/crc-meta/","publishdate":"2019-02-28T00:00:00-06:00","relpermalink":"/publication/crc-meta/","section":"publication","summary":"Objective Obesity is a risk factor for colorectal cancer (CRC), accounting for more than 14% of CRC incidence. Microbial dysbiosis and chronic inflammation are common characteristics in both obesity and CRC. Human and murine studies, together, demonstrate the significant impact of the microbiome in governing energy metabolism and CRC development; yet, little is understood about the contribution of the microbiome to development of obesity associated CRC as compared to individuals who are not obese. Design In this study, we conducted a meta-analysis using five publicly available stool and tissue-based 16S rRNA and whole genome sequencing (WGS) data sets of CRC microbiome studies. High-resolution analysis was employed for 16S rRNA data, which allowed us to achieve species-level information to compare with WGS. Results Characterisation of the confounders between studies, 16S rRNA variable region and sequencing method did not reveal any significant effect on alpha diversity in CRC prediction. Both 16S rRNA and WGS were equally variable in their ability to predict CRC. Results from diversity analysis confirmed lower diversity in obese individuals without CRC; however, no universal differences were found in diversity between obese and non-obese individuals with CRC. When examining taxonomic differences, the probability of being classified as CRC did not change significantly in obese individuals for all taxa tested. However, random forest classification was able to distinguish CRC and non-CRC stool when body mass index was added to the model. Conclusion Overall, microbial dysbiosis was not a significant factor in explaining the higher risk of colon cancer among individuals with obesity.","tags":[],"title":"Gut microbiome meta-analysis reveals dysbiosis is independent of body mass index in predicting risk of obesityassociated CRC","type":"publication"},{"authors":["R. Noah Padgett","Forsse, J. S.","Papadakis, Z.","Deal, P. J.","Stamatis, A."],"categories":null,"content":"","date":1551333600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551333600,"objectID":"00fc8eb0e1c80f53e92efd8b5b33ab43","permalink":"/publication/ncaa-mental-health/","publishdate":"2019-02-28T00:00:00-06:00","relpermalink":"/publication/ncaa-mental-health/","section":"publication","summary":"Based on National Collegiate Athletic Association (NCAA) reports, student-athletes' well-being is compromised by sub-clinical issues of mental health (MH) disorders, such as depression and anxiety. Preliminary data have shown a positive relationship between mental toughness (MT) and MH, self-compassion (SC) and MH, and SC and MT. To date, possible indirect causal relationships between these three constructs have not been investigated. PURPOSE: To confirm the three aforementioned relationships in NCAA athletes and explore the mediation role of MT and SC on the SC-MH and MT-MH relationships, respectively. Hypotheses: (1) MT will correlate positively with MH, (2) SC will correlate positively with MH, (3) MT will correlate positively with SC, (4) MT will mediate the SC-MH relationship, and (5) SC will mediate the MT-MH relationship. METHODS: The Mental Toughness Index, the Self-Compassion Scale, and the Mental Health Continuum-Short Form were uploaded on Qualtrics. NCAA athletes were invited to participate via email. The sample (n=466) was predominantly Division III, White, female, freshmen, soccer players, and in-season (M age =19.8, SD=1.8). The analysis consisted of two parts. In the first, bivariate correlations were computed among MT, SC, and MH. In the second, a structural equation model was constructed to test the bidirectional relationship between MT and SC, where MT and SC also had direct effects on MH. All analyses were completed in R. RESULTS: The findings showed a positive relationship between MT and MH (r=0.371, p","tags":[],"title":"Mental health best practices in NCAA: The bidirectional relationship between mental toughness and self-compassion","type":"publication"},{"authors":null,"categories":["R","teaching"],"content":"\rBefore we get started with an introduction to simulation, we need to make sure that R and R Studio are downloaded on your computers.\rR is an open source and free statistical programming language.\rR utilizes user built ‘packages.’\rThe package library is huge and you can find a package that does nearly anything you need it to and a large number of these are regularly updated.\rWe will talk about packages a lot over the never few years so this is kind of tertiary.\nAnyways, to install R follow these steps:\nGo to https://www.r-project.org/\rClick on ‘to download R’\rChoose a ‘CRAN mirror’, a CRAN mirror is where your computer knows to look on the internet to download the software. I suggest https://cran.revolutionanalytics.com/ which is based out of Dallas.\rSelect the download that is approriate for your computer.\rThen scroll down to the download: R-3.5.1.pkg (mac) or install for the first time (windows).\r\rThese steps will help you install R, next we need to download R Studio.\rR Studio is an interface to use R that is much more friendly and easier to work with.\rYou will see this when you open both.\nGo to https://www.rstudio.com/products/rstudio/download/\rScroll down to ‘Installers for Supported Platforms’\rSelect either the windows or mac version\r\rNow that we have R and R studio downloaed, let’s dive into an Intro to Simulation.\nIntro to the World of R\rR is a statistical programming language.\rBecause R is a programming language there is a feel of technicality involved with using the software.\rOne of the major difficulties I have found with helping others get started with R is getting over the initial struggle.\rThe initial struggle is OKAY.\rI use R nearly everyday and I am constantly googling how to solve errors and complete my tasks.\rBasically, I want to convey that it’s perfectly normal to constantly get error messages and then ask the world of google how to fix your error.\nSo, for starters, R can be broken down to being a fancy calculator.\rFor example, below is a simple case for using R as simple calculator.\rWe want to find the sum of “1 + 1”.\rR will automatically return the result of 2.\n1 + 1\r## [1] 2\rNow, this is a little boring.\rOne of the major advantages of using a programming language is being able to store different operations into what is called an object.\rAn object within R is simply a letter or word that represent another symbol or value.\rFor example, we can store the summation of “1 + 1” into a single letter “x”.\rR does this assignment of values to words or letters through two different mechanisms.\rYou can use “=” or “\u0026lt;-” to make R assign values to objects/letters.\rHowever, within R it is customary to only use “\u0026lt;-” for basic assignment while the “=” is reserved for arguments within functions (I will get to this in a moment).\n## Assign the sum of 1 + 1 to x\rx \u0026lt;- 1 + 1\r## Simply place \u0026quot;x\u0026quot; to see the value stored\rx ## [1] 2\rIntro to Simple Functions\rR is a functional programming language, which means that R is designed with tools that take in values to return a result.\rNow, this is super vague and not explicit enough to make sense.\rOne of the simplest functions in R is the “print()” function.\rThe print function takes in a single argument “x”.\rThis means that based on the value supplied to the function print with the argument x, R will display the actual value(s) stored in x.\rFor example, suppose we store the phrase “Hello People” in the object “y”.\rWe can use the print() function to see what the value of “y” is when supplied to the functin.\n## Store \u0026quot;Hello People\u0026quot; in y\ry \u0026lt;- \u0026quot;Hello People\u0026quot;\r## Displaythe value of y\rprint(x=y) ## x is theargument of print, and we want to print y\r## [1] \u0026quot;Hello People\u0026quot;\rThis concept of storing values into objects to use in functions is extremely important.\rWe can utilize this idea in any number of ways we can imagine.\rThe flexibility to choose how we use the simple rules of R provides us with a basis for manipulating data and doing more complex statistical analyses.\rBut, for now I will talk a little bit about how R can be used to generate values randomly.\n\r\r","date":1547604794,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547604794,"objectID":"bd0d52c2e5400964ac35f2fbede4304e","permalink":"/teaching/2019-01-16-intro-to-r/","publishdate":"2019-01-15T21:13:14-05:00","relpermalink":"/teaching/2019-01-16-intro-to-r/","section":"teaching","summary":"Before we get started with an introduction to simulation, we need to make sure that R and R Studio are downloaded on your computers.\rR is an open source and free statistical programming language.\rR utilizes user built ‘packages.’\rThe package library is huge and you can find a package that does nearly anything you need it to and a large number of these are regularly updated.\rWe will talk about packages a lot over the never few years so this is kind of tertiary.","tags":["R","teaching"],"title":"Introduction to R","type":"teaching"},{"authors":null,"categories":["R","simulation","tutorial"],"content":"\rBefore we get started with an introduction to simulation, we need to make sure that R and R Studio are downloaded on your computers.\rR is an open source and free statistical programming language.\rR utilizes user built ‘packages.’\rThe package library is huge and you can find a package that does nearly anything you need it to and a large number of these are regularly updated.\rWe will talk about packages a lot over the never few years so this is kind of tertiary.\nAnyways, to install R follow these steps:\nGo to https://www.r-project.org/\rClick on ‘to download R’\rChoose a ‘CRAN mirror’, a CRAN mirror is where your computer knows to look on the internet to download the software. I suggest https://cran.revolutionanalytics.com/ which is based out of Dallas.\rSelect the download that is approriate for your computer.\rThen scroll down to the download: R-3.5.1.pkg (mac) or install for the first time (windows).\r\rThese steps will help you install R, next we need to download R Studio.\rR Studio is an interface to use R that is much more friendly and easier to work with.\rYou will see this when you open both.\nGo to https://www.rstudio.com/products/rstudio/download/\rScroll down to ‘Installers for Supported Platforms’\rSelect either the windows or mac version\r\rNow that we have R and R studio downloaed, let’s dive into an Intro to Simulation.\nIntro to the World of R\rR is a statistical programming language.\rBecause R is a programming language there is a feel of technicality involved with using the software.\rOne of the major difficulties I have found with helping others get started with R is getting over the initial struggle.\rThe initial struggle is OKAY.\rI use R nearly everyday and I am constantly googling how to solve errors and complete my tasks.\rBasically, I want to convey that it’s perfectly normal to constantly get error messages and then ask the world of google how to fix your error.\nSo, for starters, R can be broken down to being a fancy calculator.\rFor example, below is a simple case for using R as simple calculator.\rWe want to find the sum of “1 + 1”.\rR will automatically return the result of 2.\n1 + 1\r## [1] 2\rNow, this is a little boring.\rOne of the major advantages of using a programming language is being able to store different operations into what is called an object.\rAn object within R is simply a letter or word that represent another symbol or value.\rFor example, we can store the summation of “1 + 1” into a single letter “x”.\rR does this assignment of values to words or letters through two different mechanisms.\rYou can use “=” or “\u0026lt;-” to make R assign values to objects/letters.\rHowever, within R it is customary to only use “\u0026lt;-” for basic assignment while the “=” is reserved for arguments within functions (I will get to this in a moment).\n## Assign the sum of 1 + 1 to x\rx \u0026lt;- 1 + 1\r## Simply place \u0026quot;x\u0026quot; to see the value stored\rx ## [1] 2\rIntro to Simple Functions\rR is a functional programming language, which means that R is designed with tools that take in values to return a result.\rNow, this is super vague and not explicit enough to make sense.\rOne of the simplest functions in R is the “print()” function.\rThe print function takes in a single argument “x”.\rThis means that based on the value supplied to the function print with the argument x, R will display the actual value(s) stored in x.\rFor example, suppose we store the phrase “Hello People” in the object “y”.\rWe can use the print() function to see what the value of “y” is when supplied to the functin.\n## Store \u0026quot;Hello People\u0026quot; in y\ry \u0026lt;- \u0026quot;Hello People\u0026quot;\r## Displaythe value of y\rprint(x=y) ## x is theargument of print, and we want to print y\r## [1] \u0026quot;Hello People\u0026quot;\rThis concept of storing values into objects to use in functions is extremely important.\rWe can utilize this idea in any number of ways we can imagine.\rThe flexibility to choose how we use the simple rules of R provides us with a basis for manipulating data and doing more complex statistical analyses.\rBut, for now I will talk a little bit about how R can be used to generate values randomly.\n\r\rBrief Introduction to Probability\rYes, we are starting with probability.\rBefore we can talk about simulating random data, we need to talk briefly about probability.\nLet’s say we have a quarter.\rA quarter has two sides, HEADS and TAILS.\rThese two outcomes are possibly if we flip a coin once.\rA coin is called ‘fair’ if both sides have the same likelihood of occuring.\rThis means that over repeated flips of our coin, we would expect that each outcome (H vs. T) to occur approximately the same number of times.\rTherefore, if a coin is fair, each outcome H and T will occur 50% of the time.\rIn other words, each outcome has a 0.5 probability of occuring.\nThis is definition note: a probability is strictly between 0 and 1. If a probability if said to be outside this range, then the value cannot be a probability.\nProbability is based on taking a sample from a population.\rA population is what is often of interest, but we rarely have access to the entire population.\rThe US census is an attempt to get information from the entire US population.\rBecause of cost and time, we take sufficiently large samples from a population to gain insight into the population.\rFor example, to empirically test whether a coin is fair, we need to flip a coin many times generate the number of the times each outcomes occurs.\rIf we flip a coin 1000 times and 500 of these turned out to fair, we likely have good evidence that our coin is fair.\rIn contrast, if we flipped a coin 3 times and all 3 turned out to heads then the question becomes whether or not we ahve enough evidence to conclude that the coin is not fair.\n\rSimulating a Datum\rIn R, we can generate a single sample from a population very easily.\rTake the example of a coin flip.\rWe can specify a population with two outcomes Heads and Tails with each outcome begin equally likely.\rFrom this population, we then take a single draw to obtain a datum.\n\r# Specify the possible outcomes. We do this by telling R to make an object with two values inside. This is done by using the command c(.). Values within the () are stuck together\rpopulation \u0026lt;- c(\u0026#39;HEADS\u0026#39;, \u0026#39;TAILS\u0026#39;)\r# Specify the Probability of each outcome in the population. Each value in the c(.) must align with the corresponding outcome in the population object. outcomeProb \u0026lt;- c(.5, .5)\r# To generate the random draw, we use the following:\r# x = defined population\r# size = number of sample # replace = TRUE/FALSE? Should sampling be done with replacement?\rsample(x=population, size=1, prob = outcomeProb, replace = T)\r## [1] \u0026quot;TAILS\u0026quot;\r\rSimulating Data\rNow, lets simulate more than one datum. We will now simulate 10 coin flips:\n# To do so, we simply adjust the size to 10 instead of 1\rsample(x=population, size=10, prob = outcomeProb, replace = T)\r## [1] \u0026quot;HEADS\u0026quot; \u0026quot;HEADS\u0026quot; \u0026quot;TAILS\u0026quot; \u0026quot;TAILS\u0026quot; \u0026quot;HEADS\u0026quot; \u0026quot;HEADS\u0026quot; \u0026quot;HEADS\u0026quot; \u0026quot;TAILS\u0026quot;\r## [9] \u0026quot;TAILS\u0026quot; \u0026quot;HEADS\u0026quot;\r\r","date":1547604794,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547604794,"objectID":"cd98911c4381d0d523a9e748848cad67","permalink":"/tutorial/2018-12-10-r-simple-sim.rmarkdown/","publishdate":"2019-01-15T21:13:14-05:00","relpermalink":"/tutorial/2018-12-10-r-simple-sim.rmarkdown/","section":"tutorial","summary":"Before we get started with an introduction to simulation, we need to make sure that R and R Studio are downloaded on your computers.\rR is an open source and free statistical programming language.\rR utilizes user built ‘packages.’\rThe package library is huge and you can find a package that does nearly anything you need it to and a large number of these are regularly updated.\rWe will talk about packages a lot over the never few years so this is kind of tertiary.","tags":["R","simulation","tutorial"],"title":"Introduction to R and Simulating Data","type":"tutorial"},{"authors":null,"categories":["GMM","Mixture Modeling","Reporting"],"content":" I am currently wrapping up my work on a project investigating the alcoholic tendencies of adolesences. The data come from a data study in the United Kingdom, and I was brought on the project to help with the analysis. For this study, the aim was to investigate the presence of unobserved heterogeneity on the growth trajectories of this construct of \u0026lsquo;alcoholic tendencies.\u0026rsquo; For this analysis, I implemented a Growth Mixture Model (GMM). A GMM is a very complex model that allows for the greatest flexibility in terms of statistical modeling. The aim is to uncover the presence of unobserved groups of people with qualitatively distinct growth patterns. The term growth may be a little misleading as the pattern may be negative or no change may occur. I will not dive into the complexities of the modeling of GMMs in this post. The aim of this post is to show what I have found to be the most useful in terms of reporting GMMs from my readings.\nThe source for these guidelines come from textbooks on GMMs such Grimm, Ram and Estabrook (2017) and the great article van de Schoot et al. (2017).\n Descriptive statistics of observed variables. For dichotomous items, report the proportion of 1’s or proportion of high responses.\n Measurement model, meaning how the four items measured over time relate to the construct of alcoholic tendencies, why a high value on this trait is bad because of a high value on each of these items.\n Measurement invariance – this is where one of the first major limitations, statistical speaking, crops up. Measurement invariance cannot be completely investigated with dichotomous indicators. Below is a brief write-up I started on this topic a few months ago. This should get you started and includes some key references on this topic of measurement invariance with categorical variables. Also, the Grimm et al. (2017) book has some good sections on testing measurement invariance in Mplus (see pgs. 347-350, 381-389).\n Describe the idea of linear growth on this latent trait. Where the growth model aims to describe the growth trajectory of the latent variable across the measurement occasions. We only investigated a linear growth curve. Now another limitation: the latent variable may have a non-linear growth trajectory. A nonlinear growth trajectory could be disguised as the presence of a latent class or this could hide the presence of latent classes do to an improperly specified growth pattern.\n Methods of mixture modeling – idea that more than one latent class underlies these data. This is one of the main points that needs to be described as we are doing a growth mixture model (GMM). The GMM aims to described qualitatively different trajectories in growth on the latent trait over time. This is also where to describe how in our model, we are allowing for class specific intercepts and slopes in the growth model. Meaning that the classes on average have unique average growth trajectories and unique starting points on the latent trait.\n Model selection – along with model summary across class enumerations. We will have estimated the class 1 through class 5 GMMs. We will need to report the log-likelihood, number of parameters, AIC, BIC, ssBIC, LMR-LRT. Along with information on which models had potential convergence issues. Although, hopefully there won’t be much convergence issues once we use more starts…. Anyways, that leads me into a important limitation: estimation of GMM, and mixture models in general, are notorious for being estimated at a local maximum on the likelihood surface and thus we may not have the true optimal solution for this model. We should note how the scale of the latent variable is also arbitrary and therefore the meanstructure was established based on the metric of the observed data and then rescaled to an arbitrary metric for presentation. This will help us to describe which model is best fitting when looking at profile plots:\na. Profile plots will include two types. 1) class average trajectories and 2) class profiles for all individuals by panels of the “raw” factor scores across time. I wrote an R script for you that does this based on two pieces of information the file name and number of classes. This only works for converged models where factor scores were reported.\n Model results to report of final model:\na. Class sizes\nb. Entropy\nc. Plot of the factor scores across time by each class\nd. Class intercepts and slopes with associated standard errors (SE)\ne. Effect of interventions across each class and associated SE\n  References  Rens van de Schoot, Marit Sijbrandij, Sonja D. Winter, Sarah Depaoli \u0026amp; Jeroen K. Vermunt (2017). The GRoLTS-Checklist: Guidelines for Reporting on Latent Trajectory Studies, Structural Equation Modeling: A Multidisciplinary Journal, 24:3, 451-467, DOI: 10.1080 /10705511.2016.1247646\n Kevin J. Grimm, Nilam Ram \u0026amp; Ryne Estabrook (2017). Growth modeling: Structural equation and multilevel modeling approaches. New York, NY. Guilford Press.\n  ","date":1545004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545004800,"objectID":"0cd7c6666fe792a9a8130b7074db42dc","permalink":"/post/2018-12-17-gmm-reporting/","publishdate":"2018-12-17T00:00:00Z","relpermalink":"/post/2018-12-17-gmm-reporting/","section":"post","summary":"I am currently wrapping up my work on a project investigating the alcoholic tendencies of adolesences. The data come from a data study in the United Kingdom, and I was brought on the project to help with the analysis. For this study, the aim was to investigate the presence of unobserved heterogeneity on the growth trajectories of this construct of \u0026lsquo;alcoholic tendencies.\u0026rsquo; For this analysis, I implemented a Growth Mixture Model (GMM).","tags":[],"title":"Reporting Growth Mixture Models","type":"post"},{"authors":null,"categories":["Introduction"],"content":"Hello people (or non-people, I aim to be inclusive)!\nMy name is Noah, and this is the start of my website and tutorial center! I\u0026rsquo;ve been pondering why I should create a wedbsite and the ebst reason I ahve come up with is to have a single place to store many of my ideas and struggles (with R in particular). The name I came up with Beyond-STAT is mean to sum up my work. STAT us short for statistics and a plat on words at the same time. My sister is an ICU nurse and \u0026lsquo;stat\u0026rsquo; is an emergency call (not actually I have discovered, but it is a fun play on words I can take from TV land). My purpose for this play on words is to describe how I feel like statistics is used as an emergency to bring life back to many datasets. My career feels like a similar vein at times, but I strive to focus on more than analyzing data to find a result (i.e, I strive to go \u0026lsquo;beyond\u0026rsquo; statitics). This focus stems from my drive to help other answer complex questions with data in a rigorous manner. Part of my training has focused on educational measurement and how to rigorously measure complex constructs. In this manner, the statistical models employed aim to help explain the process by which data were generated.\nDescribing processes is one of the major benefits to understanding statistics which I aim to do throguh my work. This website contains some of my ideas on statistical modeling and measurement models. Also, this where many of my trial-and-error attempts are also given and maybe they will be helpful to someone.\nThank you for reading and I hope you find something interesting in my ramblings!\n","date":1544140800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544140800,"objectID":"bd80904a6478fe5ab9da95c091d6c505","permalink":"/post/2018-12-07-hello/","publishdate":"2018-12-07T00:00:00Z","relpermalink":"/post/2018-12-07-hello/","section":"post","summary":"Hello people (or non-people, I aim to be inclusive)!\nMy name is Noah, and this is the start of my website and tutorial center! I\u0026rsquo;ve been pondering why I should create a wedbsite and the ebst reason I ahve come up with is to have a single place to store many of my ideas and struggles (with R in particular). The name I came up with Beyond-STAT is mean to sum up my work.","tags":[],"title":"Starting up Beyond-STAT","type":"post"},{"authors":["K. Leigh Greathouse","James R. White","Ashely J. Vargas","Valery V. Bliskovsky","Jessica A. Beck","Natalia von Muhlinen","Eric C. Polley","Elise D. Bowman","Mohammed A. Khan","Ana I. Robles","Tomer Cooks","Bríd M. Ryan","R. Noah Padgett","Amiran H. Dzutsev","Giorgio Trinchieri","Marbin A. Pineda","Sven Bilke","Paul S. Meltzer","Alexis N. Hokenstad","Tricia M. Stickrod","Marina R. Walther-Antonio","Joshua P. Earl","Joshua C. Mell","Jaroslaw E. Krol","Sergey V. Balashov","Archana S. Bhat","Garth D. Ehrlich","Alex Valm","Clayton Deming","Sean Conlan","Julia Oh","Julie A. Segre","Curtis C. Harris"],"categories":null,"content":"In this project, I contributed to the statistical analyses and making of figures.\n","date":1533099600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533099600,"objectID":"085f9d14c1323e7f6296762507bcc50d","permalink":"/publication/genome-bio-lung-cancer/","publishdate":"2018-08-01T00:00:00-05:00","relpermalink":"/publication/genome-bio-lung-cancer/","section":"publication","summary":"Background: Lung cancer is the leading cancer diagnosis worldwide and the number one cause of cancer deaths. Exposure to cigarette smoke, the primary risk factor in lung cancer, reduces epithelial barrier integrity and increases susceptibility to infections. Herein, we hypothesize that somatic mutations together with cigarette smoke generate a dysbiotic microbiota that is associated with lung carcinogenesis. Using lung tissue from 33 controls and 143 cancer cases, we conduct 16S ribosomal RNA (rRNA) bacterial gene sequencing, with RNA-sequencing data from lung cancer cases in The Cancer Genome Atlas serving as the validation cohort. Results: Overall, we demonstrate a lower alpha diversity in normal lung as compared to non-tumor adjacent or tumor tissue. In squamous cell carcinoma specifically, a separate group of taxa are identified, in which Acidovorax is enriched in smokers. Acidovorax temporans is identified within tumor sections by fluorescent in situ hybridization and confirmed by two separate 16S rRNA strategies. Further, these taxa, including Acidovorax, exhibit higher abundance among the subset of squamous cell carcinoma cases with TP53 mutations, an association not seen in adenocarcinomas. Conclusions: The results of this comprehensive study show both microbiome-gene and microbiome-exposure interactions in squamous cell carcinoma lung cancer tissue. Specifically, tumors harboring TP53 mutations, which can impair epithelial function, have a unique bacterial consortium that is higher in relative abundance in smoking-associated tumors of this type. Given the significant need for clinical diagnostic tools in lung cancer, this study may provide novel biomarkers for early detection.","tags":null,"title":"Interaction between the microbiome and TP53 in human lung cancer","type":"publication"},{"authors":["Khyam Paneru","R. Noah Padgett","Hanfeng Chen"],"categories":null,"content":"","date":1525150800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525150800,"objectID":"40101b9bf5f38d321cf5fa69aa612828","permalink":"/publication/zinormal-boot/","publishdate":"2018-05-01T00:00:00-05:00","relpermalink":"/publication/zinormal-boot/","section":"publication","summary":"A mixture model was adopted from the maximum pseudo-likelihood approach under complex sampling designs to estimate the mean of zero-inflated population. To overcome the complexity and assumptions of asymptotic distribution, the maximum pseudo- likelihood function was used, but a bootstrapping procedure was proposed as an alternative. Bootstrap confidence intervals consistently capture the true means of zero-inflated populations of the simulation studies.","tags":["methods","simulation"],"title":"Estimation of zero-inflated population mean: A bootstrapping approach","type":"publication"},{"authors":["David M. Rehfeld","R. Noah Padgett"],"categories":null,"content":"","date":1516600800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516600800,"objectID":"cdcbfb3adffbd3adac306b71a548d601","permalink":"/publication/casl-2-test-review/","publishdate":"2018-01-22T00:00:00-06:00","relpermalink":"/publication/casl-2-test-review/","section":"publication","summary":"This article presents a review of the Comprehensive Assessment of Spoken Language–Second Edition (CASL-2), in which reliability, utility, and validity are analyzed and discussed. Some limited recommendations for practice are made based on a review of the information provided by the publisher for clinicians.","tags":["Test Review"],"title":"Test Review: Comprehensive Assessment of Spoken Language–Second Edition","type":"publication"},{"authors":[],"categories":null,"content":" Academic makes it easy to create a beautiful website for free using Markdown. Customize anything on your site with widgets, themes, and language packs.\nFollow our easy step by step guide to learn how to build your own free website with Academic. Check out the personal demo or the business demo of what you\u0026rsquo;ll get in less than 10 minutes.\n View the documentation Ask a question Request a feature or report a bug Updating? View the Update Guide and Release Notes Support development of Academic:  Donate a coffee Become a backer on Patreon Decorate your laptop or journal with an Academic sticker Wear the T-shirt   \nKey features:\n Easily manage various content including homepage, blog posts, publications, talks, and projects Extensible via color themes and widgets/plugins Write in Markdown for easy formatting and code highlighting, with LaTeX for mathematical expressions Social/academic network linking, Google Analytics, and Disqus comments Responsive and mobile friendly Simple and refreshing one page design Multilingual and easy to customize  Color Themes Academic is available in different color themes and font themes.\n         Ecosystem  Academic Admin: An admin tool to import publications from BibTeX or import assets for an offline site Academic Scripts: Scripts to help migrate content to new versions of Academic  Install You can choose from one of the following four methods to install:\n one-click install using your web browser (recommended) install on your computer using Git with the Command Prompt/Terminal app install on your computer by downloading the ZIP files install on your computer with RStudio  Quick install using your web browser  Install Academic with Netlify  Netlify will provide you with a customizable URL to access your new site  On GitHub, go to your newly created academic-kickstart repository and edit config.toml to personalize your site. Shortly after saving the file, your site will automatically update Read the Quick Start Guide to learn how to add Markdown content. For inspiration, refer to the Markdown content which powers the Demo  Install with Git Prerequisites:\n Download and install Git Download and install Hugo   Fork the Academic Kickstart repository and clone your fork with Git:\ngit clone https://github.com/sourcethemes/academic-kickstart.git My_Website  Note that if you forked Academic Kickstart, the above command should be edited to clone your fork, i.e. replace sourcethemes with your GitHub username.\n Initialize the theme:\ncd My_Website git submodule update --init --recursive   Install with ZIP  Download and extract Academic Kickstart Download and extract the Academic theme to the themes/academic/ folder from the above step  Install with RStudio View the guide to installing Academic with RStudio\nQuick start  If you installed on your computer, view your new website by running the following command:\nhugo server  Now visit localhost:1313 and your new Academic powered website will appear. Otherwise, if using Netlify, they will provide you with your URL.\n Read the Quick Start Guide to learn how to add Markdown content, customize your site, and deploy it. For inspiration, refer to the Markdown content which powers the Demo\n Build your site by running the hugo command. Then host it for free using Github Pages or Netlify (refer to the first installation method). Alternatively, copy the generated public/ directory (by FTP, Rsync, etc.) to your production web server (such as a university\u0026rsquo;s hosting service).\n  Updating Feel free to star the project on Github to help keep track of updates and check out the release notes prior to updating your site.\nBefore updating the framework, it is recommended to make a backup of your entire website directory (or at least your themes/academic directory) and record your current version number.\nBy default, Academic is installed as a Git submodule which can be updated by running the following command:\ngit submodule update --remote --merge  Check out the update guide for full instructions and alternative methods.\nFeedback \u0026amp; Contributing Please use the issue tracker to let me know about any bugs or feature requests, or alternatively make a pull request.\nFor support, head over to the Hugo discussion forum.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461128400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515823200,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"/post/getting-started/","publishdate":"2016-04-20T00:00:00-05:00","relpermalink":"/post/getting-started/","section":"post","summary":"Create a beautifully simple website or blog in under 10 minutes.","tags":["Academic"],"title":"Academic: the website designer for Hugo","type":"post"},{"authors":null,"categories":["R"],"content":"\rR Markdown\rThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars)\r## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00\rfit \u0026lt;- lm(dist ~ speed, data = cars)\rfit\r## ## Call:\r## lm(formula = dist ~ speed, data = cars)\r## ## Coefficients:\r## (Intercept) speed ## -17.579 3.932\r\rIncluding Plots\rYou can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1))\rpie(\rc(280, 60, 20),\rc(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;),\rcol = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;),\rinit.angle = -50, border = NA\r)\r\rFigure 1: A fancy pie chart.\r\r\r","date":1437703994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1437703994,"objectID":"10065deaa3098b0da91b78b48d0efc71","permalink":"/post/2015-07-23-r-rmarkdown/","publishdate":"2015-07-23T21:13:14-05:00","relpermalink":"/post/2015-07-23-r-rmarkdown/","section":"post","summary":"R Markdown\rThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars)\r## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.","tags":["R Markdown","plot","regression"],"title":"Hello R Markdown","type":"post"}]